
\section{Improved Work Stealing Implementation}
\label{sec:rts_work_stealing2}

\status{The discussion of the benchmarks is new,
other parts have had a round of reviewing already.}

In this section we take the opportunity to improve on our work stealing
implementation from section \ref{sec:rts_work_stealing}.
While we made these improvements,
we also found it useful to change how idle engines behave.
Although these too changes are conceptually distinct,
they were made together and their implementations are interlinked.
Therefore we will present and benchmark them together as one set of changes.

\plan{Describe problems with associating stacks with contexts}
Sparks were originally stored on context local deques.
This has the following significant problems.

\begin{description}

\item[There is a dynamic number of spark deques.]

The number of contexts changes during a program's execution,
therefore the number of spark deques also changes.
This means that we must have code to manage this changing number of
deques.
This code makes the runtime system more complicated than necessary,
both when stealing a spark and when creating or destroying a context.

\item[Locking is required.]

The management of contexts must be thread safe so that the set of
spark deques is not corrupted.
We store spark deques in a global array protected by a lock.
It may be possible to replace the array with a lock free data structure.
However it is better to remove the need for thread safety by using a
constant set of deques.

\item[A large number of contexts makes work stealing slower.]

In prior sections
we have shown that the number of contexts in use can often be very high,
much higher than the number of Mercury engines.
If there are $N$ engines and $M$ contexts,
then there can be at most $N$ contexts running and
at least $M-N$ contexts suspended (blocked and waiting to run).
A context can become suspended in one of two ways:
by blocking on a future's value in a call to \wait,
or by blocking on an incomplete conjunct in a call to \joinandcontinue.
We attempt to minimise the former case (see chapter \ref{chap:overlap})
and the latter case cannot occur
if the context has a spark on its local spark deque (it would run the
spark rather than block).
Therefore the large majority of the suspended contexts will not have
sparks on their deques,
and the probability of selecting a deque at random with a spark on its
stack is low.
Furthermore the value of $M$ can be very high in pathological cases.
If an engine does not successfully steal a spark from a deque,
it will continue by trying to steal a spark from a different deque.
An engine can exhaust all its attempts, even when there is work
available:
a spark may be placed on a deque after the engine has already attempted
to steal from it.
Upon exhausting all its attempts or all the deques,
the engine will sleep before making another round of attempts
(see \trystealspark in algorithm \ref{alg:try_steal_spark_initial} on
page \pageref{alg:try_steal_spark_initial}).
Each round of attempts (regardless of success or failure) has a
complexity of $O(M)$.

\end{description}

\plan{We associate stacks with engines}
The approach we have taken to solving these problems is based on associating
spark deques with engines rather than with contexts.
A running Mercury program has a constant number of engines,
and therefore the number of spark deques will not vary.
This allows us to remove the code used to resize the deque array.
This makes context creation and destruction much simpler.
It also removes the need for the lock protecting the global array of spark
deques.
We can also remove the locking code in \trystealspark
(whose original version was shown in algorithm
\ref{alg:try_steal_spark_initial})
which was used to
ensure that the array is not changed while a thief is trying to steal a
spark.
The cost of work stealing also becomes linear in the number of engines
rather than the number of contexts.

\begin{algorithm}[tbp]
\begin{algorithmic}[1]
\Procedure{MR\_try\_steal\_spark}{}
  \If{$\left(
          \begin{array}{l}
          (current\_context \neq \text{NULL})\, \vee \\
          (MR\_num\_outstanding\_contexts < MR\_max\_contexts) \\
          \end{array}
          \right)$}
    \For{$attempt = 0$ to $MR\_num\_engines - 1$}
      \State $victim\_index \gets
          (MR\_engine.victim\_counter + attempt) \bmod MR\_num\_engines$
      \If{$victim\_index = engine\_id$}
          \Continue
      \EndIf
      \If{MR\_steal\_spark($MR\_spark\_deques$[$victim\_index$],
          \text{\code{\&}}$spark$)}
        \State $MR\_engine.victim\_counter \gets victim\_index$
        \State MR\_prepare\_engine\_for\_spark($spark$, NULL)
        \State \Return $spark.resume$
      \EndIf
    \EndFor
  \EndIf
  \State \Return NULL
\EndProcedure
\end{algorithmic}
\caption{MR\_try\_steal\_spark}
\label{alg:try_steal_spark_revised}
\end{algorithm}

\begin{algorithm}[tbp]
\begin{algorithmic}
\Procedure{MR\_prepare\_engine\_for\_spark}{$spark$, $join\_label$}
    \If{$current\_context = \text{NULL}$}
        \State $ctxt \gets$ MR\_get\_free\_context()
        \If{$ctxt = \text{NULL}$}
            \State $ctxt \gets$ MR\_create\_context()
        \EndIf
        \State MR\_load\_context($ctxt$)
    \EndIf
    \State $MR\_parent\_sp \gets spark.parent\_sp$
    \State $ctxt.thread\_local\_mutables \gets
      spark.thread\_local\_mutables$
\EndProcedure
\end{algorithmic}
\caption{MR\_prepare\_engine\_for\_spark()}
\label{alg:prepare_engine_for_spark}
\end{algorithm}

\plan{Show new \trystealspark}
We have made some other changes to \trystealspark,
whose new code is shown in algorithm \ref{alg:try_steal_spark_revised}.
Previously \trystealspark required its caller to check that stealing a spark
would not exceed the context limit;
this check has been moved into \trystealspark (line two).
Another change is that \trystealspark
will now call \prepareengineforspark (line nine)
which will load a context into the engine if it does not already have one
(algorithm \ref{alg:prepare_engine_for_spark}).
Finally \trystealspark will return the address of the spark's code;
the caller will jump to this address.
If \trystealspark could not find a spark it will return NULL,
in this case its caller will look for other work.

In the previous version,
the lock was also used to protect the victim counter;
it ensured that round-robin selection of the victim was maintained.
Now each engine independently performs its own round-robin selection of the
victim using a new field \code{victim\_counter} in the engine structure.
If a spark is found, \trystealspark will save the current victim index to
this field, as this deque may contain more sparks which it can try to
steal later.
We have not evaluated if this policy is an improvement.
However when compared with the improvement due to removing the lock and
potentially large number of spark deques,
any difference in performance due to the selection of victims will be
negligible.
The two other changes are minor:
we have removed the configurable limit of the number of work stealing
attempts per round,
and the test for null array slots;
both are now unnecessary.

\begin{algorithm}[tbp]
\begin{algorithmic}[1]
    \Procedure{MR\_idle}{}
        \State $eng\_data \gets MR\_engine\_sleep\_data$[$engine\_id$]
        \State $eng\_data.state \gets$ MR\_IDLE 

        \State \State // Try to find a runnable context.
        \State MR\_acquire\_lock($MR\_runqueue\_lock$)
        \State $ctxt \gets$ MR\_get\_runnable\_context()
        \State MR\_release\_lock($MR\_runqueue\_lock$)
        \If{$ctxt \neq$ NULL}
            \If{$current\_context \neq$ NULL}
                \State MR\_release\_context($current\_context$)
            \EndIf
            \State MR\_load\_context($ctxt$)
            \State $resume \gets ctxt.resume$
            \State $ctxt.resume \gets$ NULL
            \Goto $resume$
        \EndIf
       
        \State \State // Try to run a spark from our local spark deque.
        \State $result \gets$ MR\_pop\_spark($this\_engine.spark\_deque$)
        \If{$result \neq$ NULL}
            \State MR\_prepare\_engine\_for\_spark($spark$, $join\_label$)
            \Goto $spark.resume$
        \EndIf

        \State
        \If{$\neg$ MR\_compare\_and\_swap(\&$eng\_data.state$, MR\_IDLE,
            MR\_STEALING)}
            \State Handle the notification that we have received
        \EndIf
        \State // Try to steal a spark
        \State $code\_ptr \gets$ MR\_try\_steal\_spark()
        \If{$code\_ptr \neq$ NULL}
            \Goto $code\_ptr$
        \EndIf
        \Goto MR\_sleep
    \EndProcedure
\end{algorithmic}
\caption{New \idle code}
\label{alg:idle_entry_point}
\end{algorithm}

We have also made changes to the code that idle engines execute.
An idle engine with a context that is free (it can be used for any spark) or
without a context will call \idle to acquire new work.
\idle has changed significantly;
the new version is shown in algorithm \ref{alg:idle_entry_point}.
\idle always tries to find a runnable context before trying to find a local
spark (lines 5--15).
A context that has already been created usually represents work that is
\emph{to the left of}
any work that is still a spark (section \ref{sec:rts_work_stealing}).
Executing $G_1$ of a parallel conjunction $G_1 \& G_2$ can signal futures
that $G_2$ may otherwise block on or create sparks,
making more parallelism available.
Also if the context does not block, then when it finishes its work,
its memory can be used to execute a spark from the engine's local spark queue.
Checking for runnable contexts first also helps to avoid the creation of new
contexts before existing contexts' computations are finished.
This partially avoids deadlocks that can occur when the context limit is
exceeded:
executing a waiting context rather than attempting to create a new context
for a spark does not increase the number of contexts that exist.

\idle tries to run a context by  attempting to take a context from the
context runqueue while holding the context runqueue lock (lines 6--8).
If it finds a context,
it first checks for and unloads the engine's current context,
and then loads the new context (lines 10--12).
It clears the context's resume field and jups to the field's previous value
(lines 13--15).
The context's resume field must be cleared here as it may later be
synchronised on by \joinandcontinue.
Previously the runqueue's lock protected all of \idle,
this critical section has been made smaller which might be more efficient.

\begin{figure}
\begin{center}
\begin{verbatim}
nested_par(...) ;-
    (
        p(..., X),
    &
        (
            q(X, ...)
        & 
            r(..., Y)
        ),
        s(...)
    &
        t(Y, ...)
    ).
\end{verbatim}
\end{center}
\caption{Nested dependent parallelism.}
\label{fig:nested_dep_par}
\end{figure}

If \idle cannot find a context to resume then it
will attempt to run a spark from its local spark deque (lines 17--21).
If there is a spark to execute and the engine does not have a context,
a new one needs to be created,
this is handled by \prepareengineforspark above.
We considered only creating a context and executing the spark only if the
context limit had not been reached,
however this can create a problem, which we explain using an example.
Consider figure \ref{fig:nested_dep_par} which shows two nested
parallel conjunctions
and
two futures that create dependencies between the parallel
computations.
\code{X} creates a dependency between \code{p} and \code{q},
and \code{Y} creates a dependency between \code{r} and \code{t}.
When an engine, called E1,
executes this procedure it pushes the spark for the second and third
conjuncts of the outer conjunction onto its local spark deque.
E1 then begins the execution of \code{p}.
A second engine, called E2,
steals the spark from E1's spark deque and immediately creates two new
sparks, the first is for \code{t} and the second for \code{r}.
It pushes the sparks onto its deque,
therefore the spark for \code{t} is on the bottom (cold end).
E2 then begins the execution of \code{q}.
A third engine, E3,
wakes up and steals the spark for \code{t} spark from E2.
Next,
E2 attempts to read the value of \code{X} in \code{q} but it has not yet
been produced,
therefore E2 suspends its context and calls \idle, it cannot find a context
to execute and therefore checks for and finds the local spark for \code{r}.
E2 needs a new context to execute \code{r}, if the context limit is exceeded
it E2 cannot do any work.
And if E2 does not execute \code{r} then E3 cannot complete the execution of
\code{t} and free up its context.
The system will not deadlock, as the execution of \code{p} by E1 can still
continue, and will eventually allow the execution of \code{q} to continue.
However, we still wish to avoid such situations,
therefore we do not check the context limit before running local contexts.
Without this check,
E2 is able to create a context and execute \code{r},
this can never create more than a handful of extra contexts:
while the limit is exceeded engines will not steal work and will therefore
will execute sparks in a left-to-right order;
dependencies and barriers cannot cause more contexts to become blocked.

The two choices we have made,
executing local sparks without checking the context limit and
attempting to resume contexts before attempting to execute sparks,
work together to prevent deadlocks that could be caused by
the context limit and dependencies.
The context limit is a work-around to prevent some kinds of worst case
behaviour and should not normally affect other algorithmic choices.
These algorithmic choices are valid and desirable even if we did not need or
use the context limit as they eep the number of contexts low.
This is good for performance as contexts consume significant amounts of
memory and the garbage collector spends time scanning that memory.

If an engine executing \idle cannot find a local spark then it will attempt
to steal a spark from another engine.
It starts this by setting its state using a compare and swap,
this informs other engines that the engine is now trying to steal work and
allows the engine to receive any notifications.
Notifications and engine states are described below.
If the engine did not receive any notifications it continues by using the
\trystealspark function above.
\trystealspark is a C function allowing us to call it from multiple places
in the runtime system.
It returns the address of the spark's entry point to its caller which jumps
to the entry point.
In this case the caller is \idle,
which is C code that uses the Mercury calling convention,
this convention passes all its arguments and results in the Mercury abstract
machine registers, including the return address.
Additionally \idle does not call any other Mercury procedures and allow them
to return control to it.
Therefore, it does not need to create a frame on any of the C or Mercury stacks:
any local C variables are stored on the C stack frame shared between all
Mercury procedures (and must be saved by the caller).
\idle has another difference with other Mercury procedures:
it never returns control to its caller.
Instead it continues execution by jumping to the code address of the next
thing to execute,
such as the resume address of a context,
or the entry point of a spark.
If its callee, \trystealspark, were to make this jump rather than
returning and then letting \idle jump,
then this would leak the stack frame created for \trystealspark.
Therefore, \trystealspark must return the address of the next thing to
execute and allow \idle to jump to this code or jump to \sleep.

\plan{Why we need notifications, their purpose}.
Engines that go to sleep either need to wake occasionally to \emph{poll} for
work,
or need to be notified when new work becomes available.
One of the problems with the previous work stealing implementation is
that it required engines to poll for work.
Polling reduces the complexity of the system however
sleeping engines will often wake up needlessly to find that there is no
work available.
When work does become available engines are often slow to react,
since without a notification, they must wait for their polling timer to
expire.
Therefore our new idle code allows engines to be notified when 
sparks are created or contexts become available to execute.

\begin{figure}
\begin{verbatim}
struct MR_engine_sleep_sync {
    sem_t                               sleep_sem;
    lock                                lock;
    volatile unsigned                   state;
    volatile unsigned                   action;
    union MR_engine_wake_action_data    action_data;
};

union MR_engine_wake_action_data {
    MR_EngineId     worksteal_engine;
    MR_Context      *context;
};
\end{verbatim}
\caption{\enginesleepsync structure}
\label{fig:engine_sleep_sync}
\end{figure}

\paul{Normal state transitions and the \enginesleepsync structure.}
Notifications are handled using the \enginesleepsync structure shown in
figure \ref{fig:engine_sleep_sync}.
A global array allows engines to access one-another's \enginesleepsync
structures.
Each structure contains the following fields:
its owner's state,
an action it should perform,
the action's optional data,
a semaphore and a lock.
The \idle code above manipulates the state field in its structure,
which is pointed to by \code{engine\_data}.

\begin{description}
    \item[\code{MR\_WORKING}:] The engine has work to do and is doing it.

    \item[\code{MR\_LOOKING\_FOR\_WORK}:] The engine is looking for work in
        the form of a runnable context or local spark.  It may have work to
        do in the form of a local spark.

    \item[\code{MR\_STEALING}:] The engine is currently trying to steal work.

    \item[\code{MR\_SLEEPING}:] The engine is sleeping; it could not find any
        work and is now waiting for a notification.

    \item[\code{MR\_BUSY}:] The engine state is busy, this is a special value
        used for synchronisation.

    \item[\code{MR\_NOTIFIED}:] The engine has been notifed of some work to
        do but has not started the work yet.
\end{description}

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{pics/engine_state.eps}
\end{center}
\caption{Engine states and transitions}
\label{fig:engine_states}
\end{figure}

\paul{I need colour printing or to fix the prose here}
\noindent
The states are shown in figure \ref{fig:engine_states} as oval-shaped nodes
in the graph except for \code{MR\_NOTIFIED},
which is a single value that represents all the notification types,
which are the rectangular nodes in the graph.
The oval node with a dashed outline is illistrative only,
it exists in the graph to make it easier to visualise how workstealing may
fail.
An engine can move between any of the states connected by a black or blue
edge,
blue denotes a transition that is done with a compare and swap on the
\code{state} field of the \enginesleepsync structure.
The edges whose lines are thicker, are \emph{busier},
these are the transitions that we think occur most often.

\plan{Notification transitions}
When an engine creates a spark or makes a context runnable
it can notify a recipient using the recipient's
\enginesleepsync structure.
The sender will update the \code{action}, \code{action\_data} and
\code{state} fields.
This is modeled as a state transition of the recipient,
such transitions are shown with red and green edges in the graph.
Red edges denote transitions from the sleeping state:
the sender aquires the \code{lock} in the
\enginesleepsync stucture while updating the structures contents and
signaling the sleep semaphore. 
Green edges denote transitions from other states:
the sender uses a compare and swap to write the \code{MR\_BUSY} value to the
\code{state} field,
it then updates the \code{action} and \code{action\_data} fields
and finally writes \code{MR\_NOTIFIED} to the \code{state} field.
A sender using this later method must not use the semaphore.
Using compare and swap with the state field allows the recipient to also
use compare and swap, and in some cases simply writing to the field,
when changing states.
This allows us to make the busy transitions more efficient.
These are the transitions from 
\code{MR\_LOOKING\_FOR\_WORK} to
\code{MR\_STEALING} and 
\code{MR\_WORKING}.
Note that in the diagram all but one of the transitions from
\code{MR\_LOOKING\_FOR\_WORK} and \code{MR\_STEALING}
are protected.
The exceptions are transitions to \code{MR\_WORKING},
this is acceptable because we allow some notifications to be ignored if the
recipient has found some other work.
The other transition and state not listed above occurs when the system shuts
down,
this is another exception as an engine could not possibly find work if the
runtime were shutting down\footnote{
    Mercury supports exceptions and it is conceivable that an exception may
    make the system abort.
    Supporting exceptions in the context of parallel Mercury is discussed in
    as further work (section \ref{sec:further_work}).
}

\plan{Behaviour of \idle WRT notifications.}
We can see two transitions in \idle above,
the transition from \code{MR\_WORKING} to \code{MR\_LOOKING\_FOR\_WORK},
and the transition from 

The recipient must therefore use compare and swap when it writes to the
state field,
this can be seen in \idle above.
Some messages must not be ignored, such as a message that points to a
context that an engine must execute;
other messages may be safely ignored, such as those saying that a recipient
should check the context run queue or attempt to steal work.
These ignoreable messages may be send to a recipient in the \code{MR\_IDLE} or
\code{MR\_STEALING} states,
although we do not send spark notifications to engines in the
\code{MR\_IDLE} state.
All messages may be sent to engines in the \code{MR\_SLEEPING} state.

An engine executing \idle above will be in the \code{MR\_IDLE} state while
checking the context run queue and its local spark deque.
If it does not find work from either of these places then the compare and
swap on line 23 in figure \ref{alg:idle} will check that it has not been
advised of any work before it attempts to steal work.
The main purpose of this particular state change and compare and swap
is to create a well defined (and synchronised) moment when an engine
is considered idle and may be notified about new sparks.


force an engine to re-check the context run queue as a context might
have been placed on the queue after the engine checked the queue.
This 


whereby 

An idle engine is an engine that is currently looking for work,
if it cannot find any work then the compare and swap operation 

\begin{algorithm}
\begin{algorithmic}
    \Procedure{MR\_sleep}{}
        \State $eng\_data \gets MR\_engine\_sleep\_data$[$engine\_id$]
        \If{$\neg$ MR\_compare\_and\_swap(\&$eng\_data.state$, MR\_STEALING,
            MR\_SLEEPING)}
            \State Handle the notification that we have received
        \EndIf
        \State $sem\_wait(eng\_data.sleep_sem)$
        \Switch{$eng\_data.action$}
          \Case{ACTION\_SHUTDOWN}
            \State $\cdots$
          \EndCase
          \Case{ACTION\_STEAL\_SPARK}
            \State $victim\_id \gets
                eng\_data.action\_data.worksteal\_engine$
            \Repeat
                \State $result \gets$
                    MR\_steal\_spark($MR\_spark\_deques$[$victim\_id$],
                        \text{\code{\&}}$spark$)
            \Until{$result \neq$ MR\_BUSY}
            \If{$result =$ SUCCESS}
                \State $eng\_data.state \gets$ MR\_WORKING
                \State $MR\_engine.victim\_counter \gets victim\_id$
                \State MR\_prepare\_engine\_for\_spark($spark$, NULL)
                \Goto $spark.resume$
            \Else \Comment{$result =$ FAILURE}
                \State $MR\_engine.victim\_counter \gets victim\_id + 1$
                \Goto MR\_idle
            \EndIf
          \EndCase
          \Case{ACTION\_RUN\_CONTEXT}
            \State $eng\_data.state \gets$ MR\_WORKING
            \State $ctxt \gets eng\_data.action\_data.context$
            \State MR\_load\_context($ctxt$)
            \Goto $ctxt.resume\_label$
          \EndCase
          \Case{ACTION\_FIND\_CONTEXT}
            \Goto MR\_idle
          \EndCase
        \EndSwitch
    \EndProcedure
\end{algorithmic}
\caption{The \sleep code}
\end{algorithm}

\paul{Currently here, fixing due to RTS changes}

\sleep is another Mercury procedure written in C.

Both the sender and recipient will use the state field for synchronisation  


The engine owning the structure waits on \code{sleep\_sem} in order to
sleep,
and \code{lock} is used by other engines to ensure mutual exclusion for
their accesses to the \enginesleepsync structure.
There are three other fields in each structure:
\code{state} is used by the engine that owns the structure to communicate
its state to other engines,
while \code{action} and \code{action\_data} are used by other threads when
communicating to this engine.

Upon jumping into \idle,
an engine sets its state to \code{MR\_SLEEPING} and waits on \code{sleep\_sem}.
If another engine posts to \code{sleep\_sem} then the engine is woken up,
and retrieves the value of the \code{action} field.
The engine switches on this value to determine what to do,
such as:
shut down, 
execute a context, or
steal and execute a spark.
If it is instructed to run a context,
it will be passed the context directly
using the \code{action\_data} field.
Similarly, if it instructed to steal a spark,
then \code{action\_data} will tell
it whose spark stack it should begin its round-robin search from.
If thread stealing fails,
the engine will check the global context queue for
a runnable context;
if that fails,
then \idle will loop and put the engine back to sleep.
If no action is specified
(the value of the \code{action} field is \code{ACTION\_NOT\_SPECIFIED}),
then \sleep will try to run a context from the global context queue before
attempting to steal a spark.

\begin{algorithm}[tbp]
\begin{algorithmic}
\Procedure{MR\_wake\_engine}{$engine\_id, action, action\_data, states$}
    \State MR\_acquire\_lock($MR\_engine\_sleep\_data$[$engine\_id$]$.lock$)
    \If{$eng\_data.state \in states$}
        \State $MR\_engine\_sleep\_data$[$engine\_id$]$.action \gets action$
        \State $MR\_engine\_sleep\_data$[$engine\_id$]$.action\_data \gets action\_data$
        \State $MR\_engine\_sleep\_data$[$engine\_id$]$.state \gets$ WOKEN
        \State MR\_sem\_post($MR\_engine\_sleep\_data$[$engine\_id$]$.sleep\_sem$)
        \State $result \gets$ true
    \Else
        \State $result \gets$ false
    \EndIf
    \State MR\_release\_lock($MR\_engine\_sleep\_data$[$engine\_id$]$.lock$)
    \State \Return $result$
\EndProcedure
\end{algorithmic}
\caption{\wakeengine}
\label{alg:wake_engine}
\end{algorithm}

Engines may be woken with a call to \wakeengine,
whose code is shown in algorithm \ref{alg:wake_engine}.
\wakeengine takes as parameters the id of the engine to awaken,
the action and its data that the engine should perform upon awakening,
and the set of states from which we may awaken the engine from (usually
\code{MR\_SLEEPING}).
The implementation of \wakeengine is mostly straightforward,
one important detail is that we must only wake the engine if it has not
been woken already, this is done by checking the value of
\code{eng\_data.state} after acquiring the lock,
and setting the new state while holding the lock.
The action and action data must also be written to the
\enginesleepsync structure before posting to the target engine's sleep
semaphore.
The parameter \code{states} is a bit field, with each state represented by
its own bit.
This is flexible, it allows us to message an engine regardless of its
current state.
For example,
we shut down an engine by calling \wakeengine with the shut down action and
all bits set in the \code{states} bit field.
Compared to the previous system which used a lock and a condition variable,
this system allows us to wake engines selectively.
This is useful in the case when a context must be executed on a particular
engine because that engine's C stack contains a frame that made a call to
some Mercury code and now must be returned into.
In the future, this feature could be used to schedule computations on
\emph{nearby} processors,
processors that may share a second or third level cache with the current
processor.

\begin{algorithm}[tbp]
\begin{algorithmic}[1]
\Procedure{MR\_join\_and\_continue}{$ST, ContLabel$}
  \State $finished \gets$ MR\_atomic\_dec\_and\_is\_zero($ST.num\_outstanding$)
  \If{$finished$}
    \If{$ST.orig\_context \neq current\_context$}
      \While{$ST.orig\_context.resume\_label \neq ContLabel$}
        \State CPU\_relax
      \EndWhile
      \State MR\_release\_context($current\_context$)
      \State $current\_context \gets$ NULL
      \State MR\_load\_context($ST.parent$)
    \EndIf
    \Goto{$ContLabel$}
  \Else
    \If{$ST.orig\_context = current\_context$}
      \State $MR\_r1 \gets ContLabel$
      \Goto{MR\_idle\_orig\_context}
    \Else
      \Goto{MR\_idle}
    \EndIf
  \EndIf
\EndProcedure
\end{algorithmic}
\caption{\joinandcontinue}
\label{alg:join_and_continue_ws2}
\end{algorithm}

\paul{Move later}
A context is incompatible if it contains stack frames of a computation that
is not a parent of the computation the spark represents.
We test this using three conditions:
first, is that there is a current context, a non-existent context cannot
be incompatible;
second, resume\_label is non-NULL if a current context represents an
existing computation;
third, if the current context does have a computation, then that computation
may be incompatible if the spark's conjunction's original context is not the
current context.

Moving spark deques from contexts to engines made it necessary to change
\idle.
Similarly we must also change the \joinandcontinue barrier.
The new version of \joinandcontinue is shown in algorithm
\ref{alg:join_and_continue_ws2}.
The most significant change is on line 15,
\joinandcontinue no longer attempts to run a spark from the local deque,
it relies on \idle to do this.
The second change is that \joinandcontinue no longer suspends the original
context, it executes \idle while holding the original context.
This would invalidate \idle's precondition that if an engine has a context,
that the context is otherwise free to execute \emph{any} spark,
but an original context may only execute sparks that were created by the
parallel conjunction that it is currently executing.
The new \idle code allows us to create any number of entry points,
we have created the \idleorigcontext entry point which expects to be called
with such a context.
However, since we do not set the context's \code{resume\_label} field until
the engine has given up the context,
we must pass the value of the continuation label to \idleorigcontext.
We do this by using \code{MR\_r1}, the first Mercury abstract machine
register;
this is the normal calling convention for Mercury procedures.

The other change is an optimisation on line 10.
In the old version of this algorithm the engine would schedule the original
context,
by placing it on the run queue,
and then call \idle,
which would most likely take the same context off the run queue and then
execute it.
We are able to take advantage of the knowledge that the original context is
now runnable and the engine's current context is finished,
Therefore, we can release the engine's current context and load the original
context without calling \idle or using the run queue.
This is more efficient as it uses the runqueue less and,
in turn uses the run queue's lock less.

% \begin{algorithm}[tbp]
% \begin{algorithmic}
%     \Procedure{MR\_idle\_orig\_context}{}
%         \State $join\_label \gets MR\_r1$
%         \State $code\_ptr \gets$ MR\_try\_run\_local\_spark($join\_label$)
%         \If{$code\_ptr \neq$ NULL}
%             \Goto $code\_ptr$
%         \EndIf
%         \State MR\_save\_context($ctxt$);
%         \State $current\_context.resume\_label \gets join\_label$
%         \State $current\_context \gets$ NULL
%         \Goto MR\_idle
%     \EndProcedure
% \end{algorithmic}
% \caption{New entry point to the idle loop for dirty contexts.}
% \label{alg:idle_orig_context}
% \end{algorithm}
% 
% The new entry point \idleorigcontext is shown in algorithm
% \ref{alg:idle_orig_context}.
% This entry point expects to be called from \joinandcontinue while the engine
% is holding a context that is needed to complete a parallel conjunction.
% Using \tryrunlocalspark (algorithm \ref{alg:try_run_local_spark},
% it tries to run a spark from the engine's local spark deque first.
% When we introduced \tryrunlocalspark above,
% we said that it checked whether the spark at the top of the spark deque was
% compatible with the current context;
% when \tryrunlocalspark is called from \idleorigcontext this test can fail
% because the spark may be related to a different parallel conjunction than
% the one that the engine's current context has been executing.
% When this happens \tryrunlocalspark will put the spark back onto the spark
% deque and return \NULL.
% Below we describe why it is best to put the spark back on the deque.
% It will also return \NULL when there is no spark on the deque.
% If non-\NULL was returned then \idleorigcontext will jump to the code
% address that \tryrunlocalspark returned,
% otherwise it will save its current context,
% set the context's \code{resume\_label} field
% and jump to \idle which attempts to find work as described above,
% which involves looking for work with \tryruncontext, \tryrunlocalspark, and
% \trystealspark.
% If the earlier call to \tryrunlocalspark put an incompatible spark back onto
% the spark deque,
% and the call to \tryruncontext did not find a runnable context then
% the next call to \tryrunlocalspark
% will retrieve the spark from the top of the deque and be able to execute it,
% provided that the spark was not stolen in the meantime.
% The spark will now be compatible because the engine released its context
% before jumping to \idle.
% This is one of two simple ways to handle context-spark incompatibility:
% the other is to suspend the current context at the moment when we find that
% a spark is incompatible.
% Both solutions require a context switch,
% but the solution that we have chosen will also attempt to execute a
% suspended context before attempting to execute the spark using a new context
% (we described above that it is best to execute a suspended context rather
% than a spark).
% 
% We could add further entry points in the future if we found them useful,
% and they do not have to jump to \idle when they finish.
% For example,
% it may be useful to create a special case for when an engine is known to
% have a empty context;
% this case could check for work in a different order and if it does not
% find any work it could jump to \sleep the way \idle currently does.
% 
% \input{tab_work_stealing_revised2}
% 
% \plan{Benchmark}
% Table \ref{tab:work_stealing_revised} shows a comparison of the previous
% work stealing implementation with this one.
% There are several row groups with two rows each.
% Each row group gives the results for a single program and configuration,
% while the rows in each group give the results for the previous work stealing
% system and the current one.
% The first row group provides the results for the
% right recursive mandelbrot program.
% We compiled this with the conjunct reordering transformation from section
% \ref{sec:rts_reorder} disabled,
% otherwise it would have had the same results as the left recursive version
% of the same program,
% which is shown in the second grow group.
% It is still important to test right recursion such as in this example,
% as the conjunct reordering transformation cannot reorder dependent parallel
% conjunctions so right recursion can still occur.
% 
% \begin{figure}
% \begin{verbatim}
% :- func fibs(int) = int.
% 
% fibs(N) = F :-
%     ( N < 2 ->
%         F = 1
%     ;
%         (
%             F1 = fibs(N-1)
%         &
%             F2 = fibs(N-2)
%         ),
%         F = F1 + F2
%     ).
% \end{verbatim}
% \caption{\fibs without granularity control}
% \label{fig:fibs}
% \end{figure}
% 
% The following five row groups provide results for a new program named fibs.
% Fibs calculates the value of the 43\textsuperscript{rd} Fibonacci number.
% Fibs uses the naive definition of a Fibonacci number:  The
% $N$\textsuperscript{th} Fibonacci number is the sum of the previous two
% Fibonacci numbers unless $N$ is 0 or 1, in which case the result is 1.
% This calculation is shown as the function \fibs\footnote{
%     The arity of a Mercury function is written as \code{N+1}.
%     The \code{+1} refers the return value while \code{N} refers to the
%     function's input arguments.}
% in figure \ref{fig:fibs},
% this version of \fibs is used in the ``without GC'' (seventh) row group of table
% \ref{tab:work_stealing_revised}.
% but this is not the one used by the fibs program as we will now explain.
% This results in a program with poor performance,
% however it is a suitable micro-benchmark for testing work stealing.
% The recursive branch executes two recursive calls in parallel with one
% another and then sums their results,
% the cost of the arithmetic and if-then-else at each recursion level is very
% small compared to the parallel execution overheads.
% This results in \emph{embarrassing parallelism},
% a case where the there is more parallel work available than the machine can
% handle to such an excess that the overheads of parallel execution from
% dominate the execution time
% making the program slower than its sequential version;
% this can be seen in the table.
% We use this deliberately to allow us to measure the differences in the
% overheads due to work stealing.
% 
% \begin{figure}
% \begin{verbatim}
% :- func fibs_gc(int, int) = int.
% 
% fibsgc(N, Depth) = F :-
%     ( N < 2 ->
%         F = 1
%     ;
%         ( Depth > 0 ->
%             (
%                 F1 = fibs_gc(N-1, Depth-1)
%             &
%                 F2 = fibs_gc(N-2, Depth-1)
%             )
%         ;
%             F1 = fibs_seq(N-1),
%             F2 = fibs_seq(N-2)
%         ),
%         F = F1 + F2
%     ).
% \end{verbatim}
% \caption{\fibsgc, \fibs using granularity control}
% \label{fig:fibsgc}
% \end{figure}
% 
% To avoid this we can use \emph{granularity control} to create fewer larger
% parallel tasks (more coarse-grained parallelism).
% This introduces a new function \fibsgc, which takes an extra argument,
% \Depth.
% \fibsgc is shown in figure \ref{fig:fibsgc}.
% \Depth is the number of recursion levels near the top of the call graph in
% which parallel execution should be used.
% \fibsgc now contains a conditional in place of the original parallel
% conjunction,
% depending on the value of depth we either call a parallel conjunction of two
% calls to \fibsgc,
% or a sequential conjunction of two calls to \fibsseq, a sequential version of
% \fibs.
% \fibsgc decrements \Depth before making the recursive calls to track how
% close the current level is to the level at which it should switch to
% sequential execution.
% The third through sixth row groups in table
% \ref{tab:work_stealing_revised} use \fibsgc, with varying values for
% \Depth.
% This is just one type of granularity control.
% Granularity control is any method used to avoid
% embarrassing parallelism by turning a computation with 
% many small parallel tasks into one with fewer larger parallel tasks.
% 
% \plan{Evaluation}
% Fibs has better parallel performance when \Depth's initial value is
% smaller,
% demonstrating how granularity control effects performance.
% In most tests,
% the mandelbrot tests and the coarse grained fibs tests,
% there is very little difference between the performance of the old and new
% work stealing systems.
% However,
% in the fine grained fibs tests we can clearly see that the new work stealing
% system has much higher overheads.
% This is not what we expected but it makes sense.
% The new work stealing system no longer uses a timeout to wake up sleeping
% engines, allowing them to check for work.
% As an alternative it allows the creation of a spark to notify a sleeping
% engine.
% We think that this contributes to part of the slow down in performance,
% but probably not all of it.
% Further optimisation and tweaking could improve the situation in the future.
% 
% There are some other interesting trends in the results.
% Most obviously,
% as expected and explained above,
% parallel fibs program with fine granularity
% (high values for \Depth or disabled granularity control),
% perform more slowly than those with coarse granularity.
% There
% We expected that sequential version versions of fibs without granularity
% control to be faster than with it,
% as granularity control will introduce its own overheads, this is only
% somewhat true.
% We also expected that the overheads of granularity control would affect the
% sequential programs differently,
% that smaller values of \Depth would result in faster execution times,
% as less time is spent in the part of the call graph that includes the
% granularity control computation.
% This may be caused by some reason why the granularity controlled version of
% sequential \fibs is faster.
% Such a reason might be because the code generated for this procedure is
% faster as it aligns labels differently or interacts with the processor's
% branch prediction hardware more favourably.
% This type of effect can be common with micro-benchmarks,
% which is one reason why their use is often discouraged.
% 
% In the left recursive mandelbrot tests the revised work stealing
% implementation is slightly slower than the original implementation.
% This could be due to the same reasons that make the revised implementation
% slower in the finer grained fibs tests.
% Compare this to the right recursive mandelbrot tests:
% in the tests with three and four Mercury engines the revised implementation
% is slightly faster.
% The left recursive mandelbrot program generates all its
% sparks at once and the right recursive one does not.
% The revised work stealing system may be slightly faster because it
% notifies an engine when there is a spark to execute in parallel,
% while in the original version, a sleeping engine polls the spark deques
% every two milliseconds.
% However, the more likely explanation is that the search through all the
% deques takes longer in the original version as the number of deques is equal
% to the number of contexts,
% and as we saw earlier in this chapter right recursion can create a large
% number of contexts.
% This is especially true with larger numbers of engines;
% fewer sparks are executed on the engine on which they were created where
% they can re-use their parent context.
% 
% \plan{Further potential work}
% \plan{Steal half}
% We have noticed that in many workloads such as a left recursive parallel
% loop one engine creates all the sparks.
% In a situation like this,
% work stealing becomes more common than local work execution.
% One processor would execute work locally while $P - 1$ processors act as
% thieves,
% and the original engine's work queue can become a bottleneck.
% To avoid this behaviour,
% a thief could steal more than one spark from a victim and put all but one of
% those sparks on its own deque.
% Each time this happens work becomes distributed among more of the engines,
% which improves the probability that a given victim has work on its deque.
% The numbers of sparks on each engine's deque should stabilise,
% and then work stealing will become less frequent as each engine will prefer to
% take work from the hot end of its own deque.
% Executing more work from one's own deque, and decreasing the amount of stealing
% should both improve the efficiency of the program as a whole.
% 
% We are currently using the deque data structure and algorithms described by
% \citet{Chase_2005_wsdeque} (CL Deque).
% \citet{hendler:2002:stealhalf} (HS Deque) describes a similar data structure
% and set of algorithms that allow half the work to be stolen in a single lock
% free operation.
% These data structures are similar:
% they are both based on \citet{arora:1998:work-stealing}.
% The HS Deque differs from 
% \citet{arora:1998:work-stealing}
% by stealing half the work from a victim's deque,
% whilst the CL Deque differs by structuring each deque as
% dynamically resizable circular buffer.
% We have chosen to use the CL Deque because it is resizable,
% a feature that we identified as important (section \ref{sec:rts_work_stealing}).
% Furthermore,
% if we ever adopt a steal half deque (or any work stealing deque that allows
% us to steal more than one item in a single operation) we will have to
% consider spark execution order carefully.
% Therefore,
% these ideas may be interesting for further work.
% 
% \plan{memory hierarchy awareness}
% Another potential improvement is in the selection of a thief's victim.
% A thief may wish to prefer victims that are nearby in terms of memory
% topology, so that communication of the data relevant to the spark is
% cheaper.
% This can also be used when selecting which engine to wake up in order to
% pass work to it.
% 
