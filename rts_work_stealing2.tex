
\section{Improved Work Stealing Implementation}
\label{sec:rts_work_stealing2}

\status{
    A few things to fix up before this section is ready, but it is mostly
    done now.
}

In this section we take the opportunity to improve on our work stealing
implementation from section \ref{sec:rts_work_stealing}.
While we made these improvements,
we also found it useful to change how idle engines behave.
Although these too changes are conceptually distinct,
they were made together and their implementations are interlinked.
Therefore we will present and benchmark them together as one set of changes.

\plan{Describe problems with associating stacks with contexts}
Sparks were originally stored on context local deques.
This has the following significant problems.

\begin{description}

\item[There is a dynamic number of spark deques.]

The number of contexts changes during a program's execution,
therefore the number of spark deques also changes.
This means that we must have code to manage this changing number of
deques.
This code makes the runtime system more complicated than necessary,
both when stealing a spark and when creating or destroying a context.

\item[Locking is required.]

The management of contexts must be thread safe so that the set of
spark deques is not corrupted.
We store spark deques in a global array protected by a lock.
It may be possible to replace the array with a lock free data structure.
However it is better to remove the need for thread safety by using a
constant set of deques.

\item[A large number of contexts makes work stealing slower.]

In prior sections
we have shown that the number of contexts in use can often be very high,
much higher than the number of Mercury engines.
If there are $N$ engines and $M$ contexts,
then there can be at most $N$ contexts running and
at least $M-N$ contexts suspended (blocked and waiting to run).
A context can become suspended in one of two ways:
by blocking on a future's value in a call to \wait,
or by blocking on an incomplete conjunct in a call to \joinandcontinue.
We attempt to minimise the former case (see chapter \ref{chap:overlap})
and the latter case cannot occur
if the context has a spark on its local spark deque (it would run the
spark rather than block).
Therefore the large majority of the suspended contexts will not have
sparks on their deques,
and the probability of selecting a deque at random with a spark on its
stack is low.
Furthermore the value of $M$ can be very high in pathological cases.
If an engine does not successfully steal a spark from a deque,
it will continue by trying to steal a spark from a different deque.
An engine can exhaust all its attempts, even when there is work
available:
a spark may be placed on a deque after the engine has already attempted
to steal from it.
Upon exhausting all its attempts or all the deques,
the engine will sleep before making another round of attempts
(see \trystealspark in algorithm \ref{alg:try_steal_spark_initial} on
page \pageref{alg:try_steal_spark_initial}).
Each round of attempts (regardless of success or failure) has a
complexity of $O(M)$.

\end{description}

\plan{We associate stacks with engines}
The approach we have taken to solving these problems is based on associating
spark deques with engines rather than with contexts.
A running Mercury program has a constant number of engines,
and therefore the number of spark deques will not vary.
This allows us to remove the code used to resize the deque array.
This makes context creation and destruction much simpler.
It also removes the need for the lock protecting the global array of spark
deques.
We can also remove the locking code in \trystealspark
(whose original version was shown in algorithm
\ref{alg:try_steal_spark_initial})
which was used to
ensure that the array is not changed while a thief is trying to steal a
spark.
The cost of work stealing also becomes linear in the number of engines
rather than the number of contexts.

\begin{algorithm}[tbp]
\begin{algorithmic}[1]
\Procedure{MR\_try\_steal\_spark}{}
  \If{$\left(
          \begin{array}{l}
          (current\_context \neq \text{NULL})\, \vee \\
          (MR\_num\_outstanding\_contexts < MR\_max\_contexts) \\
          \end{array}
          \right)$}
    \For{$attempt = 0$ to $MR\_num\_engines - 1$}
      \State $victim\_index \gets
          (MR\_engine.victim\_counter + attempt) \bmod MR\_num\_engines$
      \If{$victim\_index = engine\_id$}
          \Continue
      \EndIf
      \If{MR\_steal\_spark($MR\_spark\_deques$[$victim\_index$],
          \text{\code{\&}}$spark$)}
        \State $MR\_engine.victim\_counter \gets victim\_index$
        \State MR\_prepare\_engine\_for\_spark($spark$)
        \State \Return $spark.resume$
      \EndIf
    \EndFor
  \EndIf
  \State \Return NULL
\EndProcedure
\end{algorithmic}
\caption{MR\_try\_steal\_spark}
\label{alg:try_steal_spark_revised}
\end{algorithm}

\begin{algorithm}[tbp]
\begin{algorithmic}
\Procedure{MR\_prepare\_engine\_for\_spark}{$spark$, $join\_label$}
    \If{$current\_context = \text{NULL}$}
        \State $ctxt \gets$ MR\_get\_free\_context()
        \If{$ctxt = \text{NULL}$}
            \State $ctxt \gets$ MR\_create\_context()
        \EndIf
        \State MR\_load\_context($ctxt$)
    \EndIf
    \State $MR\_parent\_sp \gets spark.parent\_sp$
    \State $ctxt.thread\_local\_mutables \gets
      spark.thread\_local\_mutables$
\EndProcedure
\end{algorithmic}
\caption{MR\_prepare\_engine\_for\_spark()}
\label{alg:prepare_engine_for_spark}
\end{algorithm}

\plan{Show new \trystealspark}
We have made some other changes to \trystealspark,
whose new code is shown in algorithm \ref{alg:try_steal_spark_revised}.
Previously \trystealspark required its caller to check that stealing a spark
would not exceed the context limit;
this check has been moved into \trystealspark (line two).
Another change is that \trystealspark
will now call \prepareengineforspark (line nine)
which will load a context into the engine if it does not already have one
(algorithm \ref{alg:prepare_engine_for_spark}).
Finally \trystealspark will return the address of the spark's code;
the caller will jump to this address.
If \trystealspark could not find a spark it will return NULL,
in this case its caller will look for other work.

In the previous version,
the lock was also used to protect the victim counter;
it ensured that round-robin selection of the victim was maintained.
Now each engine independently performs its own round-robin selection of the
victim using a new field \code{victim\_counter} in the engine structure.
If a spark is found, \trystealspark will save the current victim index to
this field, as this deque may contain more sparks which it can try to
steal later.
We have not evaluated if this policy is an improvement.
However when compared with the improvement due to removing the lock and
potentially large number of spark deques,
any difference in performance due to the selection of victims will be
negligible.
The two other changes are minor:
we have removed the configurable limit of the number of work stealing
attempts per round,
and the test for null array slots;
both are now unnecessary.

\begin{algorithm}[tbp]
\begin{algorithmic}[1]
    \Procedure{MR\_idle}{}
        \State $eng\_data \gets MR\_engine\_sleep\_data$[$engine\_id$]
        \State $eng\_data.state \gets$ MR\_LOOKING\_FOR\_WORK

        \State \State // Try to find a runnable context.
        \State MR\_acquire\_lock($MR\_runqueue\_lock$)
        \State $ctxt \gets$ MR\_get\_runnable\_context()
        \State MR\_release\_lock($MR\_runqueue\_lock$)
        \If{$ctxt \neq$ NULL}
            \State $eng\_data.state \gets$ MR\_WORKING
            \If{$current\_context \neq$ NULL}
                \State MR\_release\_context($current\_context$)
            \EndIf
            \State MR\_load\_context($ctxt$)
            \State $resume \gets ctxt.resume$
            \State $ctxt.resume \gets$ NULL
            \Goto $resume$
        \EndIf
       
        \State \State // Try to run a spark from our local spark deque.
        \State $spark \gets$ MR\_pop\_spark($this\_engine.spark\_deque$)
        \If{$spark \neq$ NULL}
            \State $eng\_data.state \gets$ MR\_WORKING
            \State MR\_prepare\_engine\_for\_spark($spark$, $join\_label$)
            \Goto $spark.resume$
        \EndIf

        \State
        \If{$\neg\left(
            \begin{array}{l}
                \text{MR\_compare\_and\_swap(\&}eng\_data.state\text{,} \\
                \qquad \text{MR\_LOOKING\_FOR\_WORK, MR\_STEALING)} \\
            \end{array}
            \right)$}
            \State Handle the notification that we have received
        \EndIf
        \State // Try to steal a spark
        \State $code\_ptr \gets$ MR\_try\_steal\_spark()
        \If{$code\_ptr \neq$ NULL}
            \State $eng\_data.state \gets$ MR\_WORKING
            \Goto $code\_ptr$
        \EndIf
        \Goto MR\_sleep
    \EndProcedure
\end{algorithmic}
\caption{New \idle code}
\label{alg:idle}
\end{algorithm}

We have also made changes to the code that idle engines execute.
An idle engine with a context that is free (it can be used for any spark) or
without a context will call \idle to acquire new work.
\idle has changed significantly;
the new version is shown in algorithm \ref{alg:idle}.
This algorithm users a structure pointed to by $eng\_data$ to receive
notifications,
this is discussed later.
For now we will discuss how \idle looks for work in lieu of any notification.

\idle looks tries to find a runnable context before trying to find a local
spark (lines 5--16).
A context that has already been created usually represents work that is
\emph{to the left of}
any work that is still a spark (section \ref{sec:rts_work_stealing}).
Executing $G_1$ of a parallel conjunction $G_1 \& G_2$ can signal futures
that $G_2$ may otherwise block on or create sparks,
making more parallelism available.
Also if the context does not block, then when it finishes its work,
its memory can be used to execute a spark from the engine's local spark queue.
Checking for runnable contexts first also helps to avoid the creation of new
contexts before existing contexts' computations are finished.
This partially avoids deadlocks that can occur when the context limit is
exceeded:
executing a waiting context rather than attempting to create a new context
for a spark does not increase the number of contexts that exist.

\idle tries to run a context by attempting to take a context from the
context run queue while holding the context run queue lock (lines 6--8).
If it finds a context,
it first checks for and unloads the engine's current context,
and then loads the new context (lines 11--13).
It clears the context's resume field and jumps to the field's previous value
(lines 14--16).
The context's resume field must be cleared here as it may later be
synchronised on by \joinandcontinue.
Previously the run queue's lock protected all of \idle,
this critical section has been made smaller which might be more efficient.

\begin{figure}
\begin{center}
\begin{verbatim}
nested_par(...) ;-
    (
        p(..., X),
    &
        (
            q(X, ...)
        & 
            r(..., Y)
        ),
        s(...)
    &
        t(Y, ...)
    ).
\end{verbatim}
\end{center}
\caption{Nested dependent parallelism.}
\label{fig:nested_dep_par}
\end{figure}

If \idle cannot find a context to resume then it
will attempt to run a spark from its local spark deque (lines 18--23).
If there is a spark to execute and the engine does not have a context,
a new one needs to be created,
this is handled by \prepareengineforspark above.
We considered only creating a context and executing the spark only if the
context limit had not been reached,
however this can create a problem, which we explain using an example.
Consider figure \ref{fig:nested_dep_par} which shows two nested
parallel conjunctions
and
two futures that create dependencies between the parallel
computations.
\code{X} creates a dependency between \code{p} and \code{q},
and \code{Y} creates a dependency between \code{r} and \code{t}.
When an engine, called E1,
executes this procedure it pushes the spark for the second and third
conjuncts of the outer conjunction onto its local spark deque.
E1 then begins the execution of \code{p}.
A second engine, called E2,
steals the spark from E1's spark deque and immediately creates two new
sparks, the first is for \code{t} and the second for \code{r}.
It pushes the sparks onto its deque,
therefore the spark for \code{t} is on the bottom (cold end).
E2 then begins the execution of \code{q}.
A third engine, E3,
wakes up and steals the spark for \code{t} spark from E2.
Next,
E2 attempts to read the value of \code{X} in \code{q} but it has not yet
been produced,
therefore E2 suspends its context and calls \idle, it cannot find a context
to execute and therefore checks for and finds the local spark for \code{r}.
E2 needs a new context to execute \code{r}, if the context limit is exceeded
it E2 cannot do any work.
And if E2 does not execute \code{r} then E3 cannot complete the execution of
\code{t} and free up its context.
The system will not deadlock, as the execution of \code{p} by E1 can still
continue, and will eventually allow the execution of \code{q} to continue.
However, we still wish to avoid such situations,
therefore we do not check the context limit before running local contexts.
Without this check,
E2 is able to create a context and execute \code{r},
this can never create more than a handful of extra contexts:
while the limit is exceeded engines will not steal work and will therefore
will execute sparks in a left-to-right order;
dependencies and barriers cannot cause more contexts to become blocked.

The two choices we have made,
executing local sparks without checking the context limit and
attempting to resume contexts before attempting to execute sparks,
work together to prevent deadlocks that could be caused by
the context limit and dependencies.
The context limit is a work-around to prevent some kinds of worst case
behaviour and should not normally affect other algorithmic choices.
These algorithmic choices are valid and desirable,
even if we did not need or use the context limit as they keep the number of
contexts low.
This is good for performance as contexts consume significant amounts of
memory and the garbage collector spends time scanning that memory.

If an engine executing \idle cannot find a local spark then it will
check for any notifications (lines 25--26).
If it has not been notified then it will try to steal a spark from another
engine using \trystealspark (above) (lines 27--31).
\trystealspark is a C function allowing us to call it from multiple places
in the runtime system.
It returns the address of the spark's entry point to its caller which jumps
to the entry point.
In this case the caller is \idle,
which is C code that uses the Mercury calling convention,
this convention passes all its arguments and results in the Mercury abstract
machine registers, including the return address.
Additionally \idle does not call any other Mercury procedures and allow them
to return control to it.
Therefore, it does not need to create a frame on any of the C or Mercury stacks:
any local C variables are stored on the C stack frame shared between all
Mercury procedures (and must be saved by the caller).
\idle has another difference with other Mercury procedures:
it never returns control to its caller.
Instead it continues execution by jumping to the code address of the next
thing to execute,
such as the resume address of a context,
or the entry point of a spark.
If its callee, \trystealspark, were to make this jump rather than
returning and then letting \idle jump,
then this would leak the stack frame created for \trystealspark.
Therefore, \trystealspark must return the address of the next thing to
execute and allow \idle to jump to this code or jump to \sleep.

\plan{Why we need notifications, their purpose}.
Engines that go to sleep either need to wake occasionally to \emph{poll} for
work,
or need to be notified when new work becomes available.
One of the problems with the previous work stealing implementation is
that it required engines to poll for work.
Polling reduces the complexity of the system however
sleeping engines will often wake up needlessly to find that there is no
work available.
When work does become available engines are often slow to react,
since without a notification, they must wait for their polling timer to
expire.
Therefore our new idle code allows engines to be notified when 
sparks are created or contexts become available to execute.

\begin{figure}
\begin{verbatim}
struct MR_engine_sleep_sync {
    volatile unsigned                   state;
    volatile unsigned                   action;
    union MR_engine_wake_action_data    action_data;
    sem_t                               sleep_sem;
    lock                                lock;
};

union MR_engine_wake_action_data {
    MR_EngineId     worksteal_engine;
    MR_Context      *context;
};
\end{verbatim}
\caption{\enginesleepsync structure}
\label{fig:engine_sleep_sync}
\end{figure}

\plan{Normal state transitions and the \enginesleepsync structure.}
Notifications are handled using the \enginesleepsync structure shown in
figure \ref{fig:engine_sleep_sync}.
A global array allows engines to access one-another's \enginesleepsync
structures.
Each structure contains the following fields:
\code{state} represents the engine's current state,
it is used by both the owner and other engines to read and write the state
of the engine;
\code{action} and \code{action\_data} are a closure describing
the next action that the engine should perform;
\code{sleep\_sem} is a semaphore that the owner will wait on in order to
sleep and other engines will signal if they want the engine to wake up;
\code{lock} is used to prevent multiple engines trying to wake the same
engine at once.
The \idle code above manipulates the state field in its structure,
which is pointed to by \code{engine\_data}.

\begin{description}
    \item[\code{MR\_WORKING}:] The engine has work to do and is doing it.

    \item[\code{MR\_LOOKING\_FOR\_WORK}:] The engine is looking for work in
        the form of a runnable context or local spark.  It may have work to
        do in the form of a local spark.

    \item[\code{MR\_STEALING}:] The engine is currently trying to steal work.

    \item[\code{MR\_SLEEPING}:] The engine is sleeping; it could not find any
        work and is now waiting for a notification.

    \item[\code{MR\_BUSY}:] The engine state is busy, this is a special value
        used for synchronisation.

    \item[\code{MR\_NOTIFIED}:] The engine has been notified of some work to
        do but has not started the work yet.
\end{description}

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{pics/engine_state.eps}
\end{center}
\caption{Engine states and transitions}
\label{fig:engine_states}
\end{figure}

\noindent
\paul{I need colour printing or to fix the prose here}
The states are shown in figure \ref{fig:engine_states} as oval-shaped nodes
in the graph except for \code{MR\_NOTIFIED},
which is a single value that represents all the notification types,
which are the rectangular nodes in the graph.
The oval node with a dashed outline is illustrative only,
it exists in the graph to make it easier to visualise how work stealing may
fail.
An engine can move itself between any of the states connected by a black or
blue edge,
blue denotes a transition that is done with a compare and swap on the
\code{state} field of the \enginesleepsync structure,
other transitions are made with an assignment.
The edges whose lines are thicker, are \emph{busier},
these are the transitions that we think occur most often.

\plan{Notification transitions}
When an engine creates a spark or makes a context runnable
it can notify a recipient using the recipient's
\enginesleepsync structure.
The sender will update the action closure and
\code{state} fields.
This is modelled as a state transition of the recipient,
such transitions are shown with red and green edges in the graph.
Red edges denote transitions from the sleeping state:
the sender acquires the \code{lock} in the
\enginesleepsync structure while updating the structures contents and
signalling the sleep semaphore. 
Green edges denote transitions from other states:
the sender uses a compare and swap to write the \code{MR\_BUSY} value to the
\code{state} field,
it then updates the action closure
and finally writes \code{MR\_NOTIFIED} to the \code{state} field.
A sender using this later method must not use the semaphore.
Using compare and swap with the state field allows the recipient to also
use compare and swap, and in some cases simply writing to the field,
when changing states.
This allows us to make the busy transitions more efficient.
These are the transitions from 
\code{MR\_LOOKING\_FOR\_WORK} to
\code{MR\_STEALING} and 
\code{MR\_WORKING}.
Note that in the diagram all but one of the transitions from
\code{MR\_LOOKING\_FOR\_WORK} and \code{MR\_STEALING}
are protected.
The exceptions are transitions to \code{MR\_WORKING},
this is acceptable because we allow some notifications to be ignored if the
recipient has found some other work.
The other transition and state not listed above occurs when the system shuts
down,
this is another exception as an engine could not possibly find work if the
runtime were shutting down\footnote{
    Mercury supports exceptions and it is conceivable that an exception may
    make the system abort.
    Supporting exceptions in the context of parallel Mercury is discussed in
    as further work (section \ref{sec:further_work}).
}

There are several different actions that a sender can specify when notifying
another engine,
these are:

\begin{description}

    \item[\code{ACTION\_CONTEXT}:]
    A context has become runnable and is pointed to by the
    \code{action\_data} field.
    We attach the context to the message so that we do not have to place it
    in the context run queue and then retrieve it again when we know that
    this engine is capable of executing the context.
    However this notification cannot be ignored as that would cause the
    context to be leaked.
    Therefore we only ever send \code{ACTION\_CONTEXT} messages to engines in
    the \code{MR\_SLEEPING} state,
    the only state that does not have an owner-initiated transition from 
    it.

    \item[\code{ACTION\_RUNQUEUE\_CHECK}:]
    A context has become runnable and was placed on the context run queue.
    Because we cannot use the \code{ACTION\_CONTEXT} message with
    non-sleeping engines,
    we have created this message to be used with non-sleeping engines.
    When a context can only be executed by a single engine,
    because that engine has a frame on its C stack that the context needs,
    then a deadlock would occur if the context were placed in the context run
    queue after the engine had checked the run queue,
    but before the engine had gone to sleep (and allowed the message above).
    By adding this message we can notify the engine that it should check the
    run queue before going to sleep.

    \item[\code{ACTION\_WORKSTEAL}:]
    A spark has been placed on the spark deque indicated by the
    \code{action\_data} field.
    This message is used to notify an engine of a new spark that it might be
    able to steal and execute.
    Informing the thief which spark deque it should check is an
    optimisation.

    \item[\code{ACTION\_SHUTDOWN}:]
    The runtime system is shutting down, this engine should exit.

\end{description}

\noindent
\plan{Behaviour of \idle WRT notifications.}
The code for \idle in algorithm \ref{alg:idle} shows several of these
transitions.
Including a the transition to the \code{MR\_STEALING} state on line 25.
If this compare and swap fails it indicates that the current engine should 
wait for \code{state} to become a value other than \code{MR\_BUSY} and then,
depending on the action closure,
either re-check the run queue or exit.
We have not shown the code that interprets and acts on the action closure.

When an engine creates a spark it should attempt to notify other engines of
the new spark.
This was not needed in the previous version of the runtime system as the
system used polling rather than notification.
However, looking for and notifying an idle engine on every spark creation
would be prohibitively expensive.
Therefore we maintain a counter of the number of engines in an \emph{idle}
state.
When this counter is zero, an engine creating a spark will not attempt to
notify any other engine.
Idle states are \code{MR\_STEALING} and \code{MR\_SLEEPING}
and are shown in figure \ref{fig:engine_states} as shaded ovals.
Only engines in these states are sent the \code{ACTION\_WORKSTEAL} message.
We considered including the \code{MR\_LOOKING\_FOR\_WORK} state as an idle
state and allowing engines in this state to receive the
\code{ACTION\_WORKSTEAL} message.
However, we think that most engines that execute \idle will find a runnable
context or a spark from their own deque and therefore trying to steal work
would be counter productive (and such a message would often be ignored).
Therefore sending such a message would be \emph{too} speculative
(and is still a little speculative when sent to an engine in the
\code{MR\_STEALING} state).

\begin{algorithm}
\begin{algorithmic}[1]
    \Procedure{MR\_sleep}{}
        \State $eng\_data \gets MR\_engine\_sleep\_data$[$engine\_id$]
        \If{$\neg$ MR\_compare\_and\_swap(\&$eng\_data.state$, MR\_STEALING,
            MR\_SLEEPING)}
            \State Handle the notification that we have received
        \EndIf
        \State $sem\_wait(eng\_data.sleep_sem)$
        \Switch{$eng\_data.action$}
          \Case{ACTION\_SHUTDOWN}
            \State $\cdots$
          \EndCase
          \Case{ACTION\_WORKSTEAL}
            \State $victim\_id \gets
                eng\_data.action\_data.worksteal\_engine$
            \Repeat
                \State $result \gets$
                    MR\_steal\_spark($MR\_spark\_deques$[$victim\_id$],
                        \text{\code{\&}}$spark$)
            \Until{$result \neq$ BUSY}
            \If{$result =$ SUCCESS}
                \State $eng\_data.state \gets$ MR\_WORKING
                \State $MR\_engine.victim\_counter \gets victim\_id$
                \State MR\_prepare\_engine\_for\_spark($spark$, NULL)
                \Goto $spark.resume$
            \Else \Comment{$result =$ FAILURE}
                \State $MR\_engine.victim\_counter \gets victim\_id + 1$
                \Goto MR\_idle
            \EndIf
          \EndCase
          \Case{ACTION\_CONTEXT}
            \State $eng\_data.state \gets$ MR\_WORKING
            \State $ctxt \gets eng\_data.action\_data.context$
            \State MR\_load\_context($ctxt$)
            \Goto $ctxt.resume\_label$
          \EndCase
          \Case{ACTION\_RUNQUEUE\_CHECK}
            \Goto MR\_idle
          \EndCase
        \EndSwitch
    \EndProcedure
\end{algorithmic}
\caption{The \sleep code}
\label{alg:sleep}
\end{algorithm}

An engine that executes \idle and cannot find any work will jump to the
\sleep procedure,
which is shown in algorithm \ref{alg:sleep}.
\sleep is another Mercury procedure written in C,
it starts by attempting to put the engine into the sleeping state,
checking for any notifications using a compare and swap as we saw in \idle.
If there are no notifications it will wait on the sleep semaphore in the
\enginesleepsync structure.
When another engine signals \code{sleep\_sem} then the engine is woken up.
The woken engine will retrieve and act on the action closure.
We have shown the full switch/case statement for interpreting the values of
\code{action} and \code{action\_data} in \sleep.
The implied switch/case statement in \idle is similar,
except that it does not need to cover some actions.

When an engine receives a \code{ACTION\_WORKSTEAL} message it executes the
code at lines 10--21,
note the loop on lines 11--13.
As we said in section \ref{sec:rts_work_stealing},
a data race when attempting to steal a spark will result in
\steal returning an error.
In \trystealspark this error is ignored and the algorithm moves to the next
deque.
However,
when an engine has received a message telling it that there is a spark on
this deque the engine will use this loop to retry when a collision occurs.
It stops once it has either been successful or the queue is empty.
We made this decision because it is possible that many sparks may be placed
on the deque at the same time,
and even though our engine lost a race to another thief, there may be more
sparks available.
If the engine fails to steal a spark from this deque,
the engine will jump to \idle where it will check the run queue and then
recheck all the spark deques.
The other actions performed when receiving a message are simple to
understand from the figure and algorithm.

\begin{algorithm}[tbp]
\begin{algorithmic}[1]
\Procedure{MR\_join\_and\_continue}{$ST, ContLabel$}
  \State $finished \gets$ MR\_atomic\_dec\_and\_is\_zero($ST.num\_outstanding$)
  \If{$finished$}
    \If{$ST.orig\_context \neq current\_context$}
      \While{$ST.orig\_context.resume\_label \neq ContLabel$}
        \State CPU\_busy\_loop\_hint
      \EndWhile
      \State MR\_release\_context($current\_context$)
      \State $current\_context \gets$ NULL
      \State MR\_load\_context($ST.parent$)
    \EndIf
    \Goto{$ContLabel$}
  \Else
    \State $spark \gets$ MR\_pop\_spark($this\_engine.spark\_deque$)
    \If{$spark \neq$ NULL}
        \If{$ST.orig\_context = current\_context \wedge
               spark.sync\_term \neq ST$}
            %\left(
            %    \begin{array}{l}
            %        ST.orig\_context = current\_context \wedge \\
            %        spark.sync\_term \neq ST \\
            %    \end{array}
            %    \right)$}
            \State MR\_save\_context($current\_context$)
            \State $current\_context.resume\_label \gets ContLabel$
            \State $current\_context \gets$ NULL
            \If{nonempty($MR\_context\_runqueue$)}
                \State MR\_push\_spark($this\_engine.spark\_deque$, $spark$)
                \Goto MR\_idle
            \EndIf
        \EndIf
        \State MR\_prepare\_engine\_for\_spark($spark$)
        \Goto $spark.resume$
    \Else
        \If{$ST.orig\_context = current\_context$}
            \State MR\_save\_context($current\_context$)
            \State $current\_context.resume\_label \gets ContLabel$
            \State $current\_context \gets$ NULL
        \EndIf
        \Goto{MR\_idle}
    \EndIf
  \EndIf
\EndProcedure
\end{algorithmic}
\caption{\joinandcontinue}
\label{alg:join_and_continue_ws2}
\end{algorithm}

We have also had to make two changes to \joinandcontinue,
the new version is shown in algorithm \ref{alg:join_and_continue_ws2}.
The first change is an optimisation on lines 8--10,
when the parallel conjunction's original context is suspended
and the engine's current context executes the last conjunct in the parallel
conjunction,
then the previous version of the algorithm would schedule the original
context by placing it on the run queue and then execute \idle as its current
context had finished.
Since we know that the current context is finished and the original one is
runnable we now resume the original context's execution directly.

The second change is on lines 15--21,
the new if statement checks whether the context that the engine holds is
compatible with the spark it has just popped from the deque.
A context is incompatible if it contains stack frames of a computation that
is not a parent of the computation the spark represents.
This means that the context is the original context for the parallel
conjunction it has been working on,
but the spark is not part of this parallel conjunction.
When this happens we must change the context
(we cannot choose a different spark without stealing and we try to minimise
stealing),
therefore we suspend the current context (lines 16--18).
Since a context switch is mandatory, then we know that it is better to
switch to a context that is already runnable,
so if the run queue is non-empty we put the spark back on the deque and jump
to \idle (lines 19--21);
otherwise we execute the spark.

\input{tab_work_stealing_revised2}

\plan{Benchmark}
Table \ref{tab:work_stealing_revised} shows a comparison of the previous
work stealing implementation with this one.
There are several row groups with two rows each.
Each row group gives the results for a single program and configuration,
while the rows in each group give the results for the previous work stealing
system and the current one.

\begin{figure}
\begin{verbatim}
:- func fibs(int) = int.

fibs(N) = F :-
    ( N < 2 ->
        F = 1
    ;
        (
            F1 = fibs(N-1)
        &
            F2 = fibs(N-2)
        ),
        F = F1 + F2
    ).
\end{verbatim}
\caption{\fibs without granularity control}
\label{fig:fibs}
\end{figure}

The first row group provides the results for the
right recursive mandelbrot program.
We compiled this with the conjunct reordering transformation from section
\ref{sec:rts_reorder} disabled,
otherwise it would have had the same results as the left recursive version
of the same program,
which is shown in the second grow group.
It is still important to test right recursion such as in this example,
as the conjunct reordering transformation cannot reorder dependent parallel
conjunctions so right recursion can still occur.
The following five row groups provide results for a new program named fibs.
Fibs calculates the value of the 43\textsuperscript{rd} Fibonacci number
using the naive algorithm of summing the previous two
Fibonacci numbers where these are defined recursively.
This calculation is shown as the function \fibs in figure \ref{fig:fibs}.
    (The arity of a Mercury function is written as \code{N+1}.
    The \code{+1} refers the return value while \code{N} refers to the
    function's input arguments.)
The naive fibs program is an excellent micro-benchmark for work stealing.
It creates many small tasks whose overheads dominate the program's execution
time which allows us easily measure the overheads of work stealing.
Results for this version of \fibs (figure \ref{fig:fibs})
are shown in the ``without GC'' (seventh) row group of table
\ref{tab:work_stealing_revised}.

%making the program slower than its sequential version;
%this can be seen in the table.

\begin{figure}
\begin{verbatim}
:- func fibs_gc(int, int) = int.

fibsgc(N, Depth) = F :-
    ( N < 2 ->
        F = 1
    ;
        ( Depth > 0 ->
            (
                F1 = fibs_gc(N-1, Depth-1)
            &
                F2 = fibs_gc(N-2, Depth-1)
            )
        ;
            F1 = fibs_seq(N-1),
            F2 = fibs_seq(N-2)
        ),
        F = F1 + F2
    ).
\end{verbatim}
\caption{\fibsgc, \fibs using granularity control}
\label{fig:fibsgc}
\end{figure}

To avoid embarrassing parallelism and create a point for comparison,
we have also shown results for fibs using \emph{granularity control} (GC) to
create fewer but larger parallel tasks
(parallelism with courser grains).
We can achieve this by introducing a new function \fibsgc,
which is shown in figure \ref{fig:fibsgc}.
\fibsgc takes an additional argument named \Depth which is
the number of recursion levels near the top of the call graph in
which parallel execution should be used,
below this depth sequential execution is used.
The third through sixth row groups in table
\ref{tab:work_stealing_revised} use \fibsgc, with varying initial values for
\Depth.
%This is just one type of granularity control.
%Granularity control is any method used to avoid
%embarrassing parallelism by turning a computation with 
%many small parallel tasks into one with fewer larger parallel tasks.

\plan{Evaluation}
Fibs has better parallel performance when \Depth's initial value is
smaller,
demonstrating how granularity control improves performance,
(row groups three to six in the table).
In parallelised divide and conquer code such as Fibs the context limit does
not affect performance as badly as it does with right
single recursive code.
The original context for each parallel conjunction is not blocked for as
long as it is with right recursive code,
because in the divide and conquer case both conjuncts have roughly
equivalent costs.
Recall that since introducing our initial work stealing implementation in
section \ref{sec:rts_work_stealing} left recursion has not been effected by
the context limit at all.
In most tests,
the mandelbrot tests and the coarse grained fibs tests,
there is very little difference between the performance of the old and new
work stealing systems.
However,
in the fine grained fibs tests we can clearly see that the new work stealing
system has much higher overheads.
This is not what we expected but it makes sense.
The new work stealing system no longer uses a timeout to wake up sleeping
engines, allowing them to check for work.
Instead when an engine creates a spark must notify a sleeping engine (if an
engine is sleeping).
We think that the additional overhead of notification contributes to part of
the slow down,
but probably not all of it.
Even if notifications are not required because all the engines are busy,
the runtime system still needs to decide if it should make a notification.
Whereas in a polling system polling is only used when there is an idle
engine, which is rare in this benchmark.
Further optimisation and tweaking could improve the situation in the future.

\paul{TODO: switching between notification and polling}

There are some other interesting trends in the results.
Most obviously,
as expected and explained above,
parallel fibs program with fine granularity
(high values for \Depth or disabled granularity control),
run more slowly than those with coarse granularity.
There we expected that sequential version of fibs without
granularity control to be faster than with it,
as granularity control will introduce its own overheads, this is only
somewhat true.
We also expected that the overheads of granularity control would affect the
sequential programs differently,
that smaller values of \Depth would result in faster execution times,
as less time is spent in the part of the call graph that includes the
granularity control computation.
However, when the sequential execution times for fibs with a \Depth of 40
are faster than those with a \Depth of 10,
they are also faster than those without granularity control.
We are not sure of the cause,
one hypothesis is that of the two procedures used in the granularity control
code (\fibsseq and \fibsgc) that \fibsgc is faster due to
some strange interaction of the processor's branch predictor and the code's
alignment.
This type of effect can be common with micro-benchmarks,
which is one reason why their use is often discouraged.
This could be tested by running several tests with randomised code layout
\citep{curtsinger:2012:stabelizer}.

In the left recursive mandelbrot tests,
the revised work stealing implementation is slightly slower than the
original implementation.
This could be due to the same reasons that make the revised implementation
slower in the finer grained fibs tests.
Compare this to the right recursive mandelbrot tests:
in the tests with three and four Mercury engines the revised implementation
is slightly faster.
The left recursive mandelbrot program generates all its
sparks at once and the right recursive one does not.
The revised work stealing system may be slightly faster because it
notifies an engine when there is a spark to execute in parallel,
while in the original version, a sleeping engine polls the spark deques
every two milliseconds.
However, the more likely explanation is that the search through all the
deques takes longer in the original version as the number of deques is equal
to the number of contexts,
and as we saw earlier in this chapter right recursion can create a large
number of contexts.
This is especially true with larger numbers of engines;
fewer sparks are executed on the engine on which they were created where
they can re-use their parent context.

\plan{Further potential work}
\plan{Steal half}
We have noticed that in many workloads such as a left recursive parallel
loop,
one engine creates all the sparks.
In situations like this,
work stealing is more common than executing sparks from an engine's own
deque.
One processor would execute its own sparks while $P - 1$ processors act as
thieves,
and the original engine's work queue can become a bottleneck.
To avoid this behaviour,
a thief could steal more than one spark from a victim and put all of
the sparks on its own deque except one, which it executes immediately.
With each stealing operation, 
sparks becomes distributed among more of the engines,
which improves the probability that a given victim has work on its own deque.
Work stealing will become less frequent as engines try to execute their own
sparks before stealing other's,
this should improve the efficiency of the program as a whole.

We are currently using the deque data structure and algorithms described by
\citet{Chase_2005_wsdeque} (C.L.\ Deque).
It is a circular resizable array whose operations
(section \ref{sec:rts_work_stealing})
allow us to steal one item at a time.
It is based on \citet{arora:1998:work-stealing}.
We have considered using the deque described by
\citet{hendler:2002:stealhalf} (H.S.\ Deque),
this is also a deque using a circular array 
based on \citet{arora:1998:work-stealing}.
It allows a thief to steal up to half the items on a victim's deque
in a single operation.
However, the H.S.\ Deque does not automatically resize,
a feature that we identified as important (section \ref{sec:rts_work_stealing}).
We should investigate using a deque similar to the H.S.\ Deque with resizing
support in the future.
At that time we will also have to carefully consider spark execution order.

\plan{memory hierarchy awareness}
Another potential improvement is in the selection of a thief's victim.
A thief may wish to prefer victims that are nearby in terms of memory
topology, so that communication of the data relevant to the spark is
cheaper.
Likewise, selecting an engine to wake up when creating a spark could also
consider memory topology.

