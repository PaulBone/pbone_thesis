
\status{Currently acting on feedback}

In this section we take the opportunity to improve on our work stealing
implementation from Section \ref{sec:rts_work_stealing}.
While we made these improvements,
we also found it useful to change how idle engines behave.
Although these too changes are conceptually distinct,
they were made together and their implementations are interlinked.
Therefore we will present and benchmark them together as one set of changes.

\plan{Describe problems with associating stacks with contexts}
Sparks were originally stored on context local deques,
which has several significant problems.

\textbf{There is a dynamic number of spark deques.}
The number of contexts changes during a programs execution,
therefore the number of spark deques also changes.
This means that we must have code to manage this changing number of
deques.
This code makes the runtime system more complicated than necessary,
both when stealing a spark and when creating or destroying a context.

\textbf{Locking is required.}
The management of contexts must be thread safe so that the set of
spark deques is not corrupted.
We store spark deques in a global array protected by a lock.
It may be possible to replace the array with a lock free data structure.
However it is better to remove the need for thread safety by using a
constant set of deques.

\textbf{A large number of contexts makes work stealing slower.}
In prior sections
we have shown that the number of contexts in use can often be very high,
much higher than the number of Mercury engines.
If there are $N$ engines and $M$ contexts,
then there can be at most $N$ contexts running and
at least $M-N$ contexts suspended (blocked and waiting to run).
A context can become suspended in one of two ways:
by blocking on a future's value in a call to \wait,
or by blocking on an incomplete conjunct in a call to \joinandcontinue.
We attempt to minimise the former case (see Chapter \ref{chap:overlap})
and the latter case cannot occur
if the context has a spark on its local spark deque (it would run the
spark rather than block).
Therefore the large majority of the suspended contexts will not have
sparks on their deques,
and the probability of selecting a deque at random with a spark on its
stack is only a little bit higher than $M \choose N$ at best and
$M \choose 1$ at worst.
Furthermore the value of $M$ can be very high in pathological cases.
If an engine does not successfully steal a spark from a deque,
it will continue by trying to steal a spark from a different deque.
An engine can exhaust all its attempts, even when there is work
available:
a spark may be placed on a deque after the engine has already attempted
to steal from it.
Upon exhausting all its attempts or all the deques,
the engine will sleep before making another round of attempts
(see \trystealspark in Algorithm \ref{alg:try_steal_spark_initial} on 
page \pageref{alg:try_steal_spark_initial}).
Each round of attempts (regardless of success or failure) has a
complexity of $O(M)$.

\plan{We associate stacks with engines}
The approach we have taken to solving these problems is based on associating
spark deques with engines rather than with contexts.
A running Mercury program has a constant number of engines,
and therefore the number of spark deques will not vary.
This allows us to remove the code used to resize the deque array,
the array is filled in when the engines are created during startup,
after which it is never modified.
This makes context creation and destruction much simpler.
It also removes the need for the lock protecting the global array of spark
deques.
We can also remove the locking code in \trystealspark
(whose original version was shown in Algorithm
\ref{alg:try_steal_spark_initial})
which was used to
ensure that the array is not changed while a thief is trying to steal a
spark.
The cost of work stealing also becomes linear in the number of engines
rather than the number of contexts.

\begin{algorithm}[tbp]
\begin{algorithmic}[1]
\Procedure{MR\_try\_steal\_spark}{$spark\_ptr$}
  \State $result \gets false$
  \For{$attempt = 0$ to $MR\_num\_engines - 1$}
    \State $victim\_index \gets
        (MR\_engine.victim\_counter + attempt) \bmod MR\_num\_engines$
    \State $result \gets$
        MR\_steal\_spark($MR\_spark\_deques$[$victim\_index$], $spark\_ptr$)
    \If{$result$}
      \State \Break
    \EndIf
  \EndFor
  \State $MR\_engine.victim\_counter \gets
    MR\_engine.victim\_counter + attempt$
  \State \Return $result$
\EndProcedure
\end{algorithmic}
\caption{try\_steal\_spark}
\label{alg:try_steal_spark_revised}
\end{algorithm}

\plan{Show new \trystealspark}
We have made some other changes to \trystealspark,
whose new code is shown in Algorithm \ref{alg:try_steal_spark_revised}.
The lock was also used to protect the victim counter,
ensuring that round robin selection of the victim was maintained.
Now each engine independently performs its own round robin selection of the
victim using a new field \code{victim\_counter} in the engine structure.
We have not evaluated if this policy is an improvment.
However when compared with the improvement due to removing the lock and
potentially large numver of spark deques,
any difference in performance due to the selection of victims will be
negligible.
The two other changes are minor:
we have removed the configurable limit of the number of work stealing
attempts per round,
and the test for null array slots has been removed;
both are now unnecessary.

\plan{Show how this is safe.}
Associating the spark deques with engines can change the order in which
sparks are executed,
so we must ensure that the system is still deadlock free.
In the previous version,
a context that is blocked on a call to \wait may contain sparks
which can only be accessed by a thief.
In the new version,
those sparks are now associated with the engine that the context was running
on,
and therefore that engine has the opportunity to run those sparks by
retrieving them from the hot end of its own deque.
If the engine attempts to execute one of these sparks it will need a
context,
and the creation of a new context may exceed the context limit even though
the spark is being executed locally.
This is just one instance in which an engine \emph{without a context}
may try to run a spark from its own deque.
In the previous version a context would always be available because sparks
were associated with contexts.

\begin{algorithm}[tbp]
\begin{algorithmic}
    \Procedure{MR\_idle}{}
        \State $code\_ptr \gets$ MR\_try\_run\_context()
        \If{$code\_ptr$}
            \Goto $code\_ptr$
        \EndIf
        \State $code\_ptr \gets$ MR\_try\_run\_local\_spark($NULL$)
        \If{$code\_ptr$}
            \Goto $code\_ptr$
        \EndIf
        \State $code\_ptr \get$ MR\_try\_steal\_spark()
        \If{$code\_ptr$}
            \Goto $code\_ptr$
        \EndIf
        \Goto MR\_sleep
    \EndProcedure
\end{algorithmic}
\caption{New \idle code}
\label{alg:idle_entry_point}
\end{algorithm}

\begin{algorithm}
\begin{algorithmic}
\Procedure{MR\_try\_run\_context}{}
    \State MR\_acquire\_lock($MR\_runqueue\_lock$)
    \State $ctxt \gets$ MR\_get\_runnable\_context()
    \State MR\_release\_lock($MR\_runqueue\_lock$)
    \If{$ctxt \neq$ NULL}
        \If{$current\_context \neq$ NULL}
            \State MR\_release\_context($current\_context$)
        \EndIf
        \State MR\_load\_context($ctxt$)
        \State $resume \gets ctxt.resume$
        \State $ctxt.resume \gets$ NULL
        \State \Return $resume$
    \Else
        \State \Return NULL
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\begin{algorithmic}
\Procedure{MR\_try\_run\_local\_spark}{$join\_label$}
    \State $result \gets$ MR\_pop\_spark($this\_engine.spark\_deque$)
    \If{$result$}
        \State MR\_prepare\_engine\_for\_spark($spark$, $join\_label$)
        \State \Return $spark.resume$
    \Else
        \State \Return NULL
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\begin{algorithmic}
\Procedure{MR\_try\_steal\_spark}{$join\_label$}
    \If{$\left(
            \begin{array}{l}
            (current\_context \neq \text{NULL})\, \vee \\
            (MR\_num\_outstanding\_contexts < MR\_max\_contexts) \\
            \end{array}
            \right)$}
        \If{MR\_try\_steal\_spark($spark$)}
            \State MR\_prepare\_engine\_for\_spark($spark$, NULL)
            \State \Return $spark.resume$
        \EndIf
    \EndIf
    \State \Return NULL
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\begin{algorithmic}
\Procedure{MR\_prepare\_engine\_for\_spark}{$spark$, $join\_label$}
    \If{$\left(
        \begin{array}{l}
            (current\_context \neq \text{NULL}) \wedge \\
            (join\_label \neq \text{NULL}) \wedge \\
            (spark.sync\_term.orig\_context \neq current\_context)
        \end{array}\right)$}
        \State MR\_save\_context($current\_context$)
        \State $current\_context.resume \gets join\_label$
        \State $current\_context \gets$ NULL
    \EndIf
    \If{$current\_context = \text{NULL}$}
        \State $ctxt \gets$ MR\_get\_free\_context()
        \If{$ctxt = \text{NUL}$}
            \State $ctxt \gets$ MR\_create\_context()
        \EndIf
        \State MR\_load\_context($ctxt$)
    \EndIf
    \State $MR\_parent\_sp \gets spark.parent\_sp$
    \State $ctxt.thread\_local\_mutables \gets
      spark.thread\_local\_mutables$
    \State \Return $spark.resume$
\EndProcedure
\end{algorithmic}
\end{algorithm}

An idle engine without a context will call \idle to acquire new work.
\idle has changed significantly;
the new version is shown in Algorithm \ref{alg:idle_entry_point}.
Many of its details shown in previous sections have been moved into C
functions,
these include:
\tryruncontext which tries to execute a suspended but runnable context;
\tryrunlocalspark which tries to run a spark from the top of the engine's
spark deque, possibly creating a new context for the spark;
and 
\trystealspark which was shown above in Algorithm
\ref{alg:try_steal_spark_revised}.
Because running a local spark may allocate a context and exceed the context
limit,
the engine must execute any suspended but runnable context instead.
Doing so is always permitted despite the context limit.
Furthermore,
a context whose execution has already begin may make more parallel work
available creating sparks or signalling futures.
If the engine is able to run the context until the completion of its work
(until the contexts executes the \joinandcontinue barrier at the end of a
parallel conjunct)
then this may make the context available to execute a spark from the
engine's local spark queue.
Therefore we always try to run a context (using \tryruncontext)
before attempting to run a local spark (using \tryrunlocalspark).
Of course there will be cases when \tryruncontext fails and an engine
without a context then attempts to run a local spark.
To facilitate this, the context limit is not checked in \tryrunlocalspark
(it is still used in \trystealspark).

\idle is a hard coded Mercury procedure and it does not return control to
its caller,
therefore it continues execution by jumping to the code address of the next
thing to execute.
This can either be the resume address of a context,
the entry point of a spark,
or the Mercury procedure \sleep.
However
the C functions are used to find the next context or spark to execute,
if successful these C functions prepare the engine to execute the spark or
context and then return the address that the engine should execute.
they \emph{must} return rather than execute the spark or context directly so
that the C stack pointer has the same value that it did upon entering \idle.
\idle uses a \emph{trampoline}\footnote{
    Trampoline is an overloaded term in computer science.
    The reader may or may not agree with our use of this term.
    }
macro to jump to the returned code
address or fall through to the next instruction if the function returned
false.
If the idle engine cannot find a spark or context to execute then it jumps
to \sleep.

\begin{algorithm}[tbp]
\begin{minipage}{\textwidth}
\begin{verbatim}
struct MR_engine_sleep_sync {
    sem_t                               sleep_sem;
    lock                                lock;
    volatile unsigned                   state;
    volatile unsigned                   action;
    union MR_engine_wake_action_data    action_data;
};

union MR_engine_wake_action_data {      
    MR_EngineId     worksteal_engine;
    MR_Context      *context;
};
\end{verbatim}

\begin{algorithmic}
    \Procedure{MR\_sleep}{}
        \Loop
            \State $eng\_data \gets MR\_engine\_sleep\_data$[$engine\_id$]
            \State $eng\_data.state \gets$ SLEEPING
            \State $sem\_wait(eng\_data.sleep_sem)$
            \Switch{$eng\_data.action$}
              \Case{ACTION\_SHUTDOWN}
                \State $\cdots$
              \EndCase
              \Case{ACTION\_RUN\_CONTEXT}
                \State $ctxt \gets eng\_data.action\_data.context$
                \State MR\_load\_context($ctxt$)
                \Goto $ctxt.resume\_label$
              \EndCase
              \Case{ACTION\_STEAL\_SPARK}
                \State $MR\_engine.victim\_counter \gets
                    eng\_data.action\_data.worksteal\_engine$ 
                \State $jump\_target \gets$ MR\_try\_steal\_spark()
                \If{$jump\_target \neq$ NULL}
                    \Goto $jump\_target$
                \EndIf
                \State $jump\_target \gets$ MR\_try\_run\_context()
                \If{$jump\_target \neq$ NULL}
                    \Goto $jump\_target$
                \EndIf
                \State \Break
              \EndCase
              \Case{ACTION\_NONE}
                \State $jump\_target \gets$ MR\_try\_run\_context()
                \If{$jump\_target \neq$ NULL}
                    \Goto $jump\_target$
                \EndIf
                \State $jump\_target \gets$ MR\_try\_steal\_spark()
                \If{$jump\_target \neq$ NULL}
                    \Goto $jump\_target$
                \EndIf
                \State \Break
              \EndCase
            \EndSwitch
        \EndLoop
    \EndProcedure
\end{algorithmic}

\end{minipage}
\caption{The \sleep code}
\end{algorithm}

\sleep is another hand written Mercury procedure.
It uses an
\enginesleepsync structure to manage the state of each engine.
These structures are organised into a global array indexed by engine ids,
making them accessible to other engines.
The structure contains a semaphore and a lock;
the engine owning the structure waits on \code{sleep\_sem} in order to
sleep,
and \code{lock} is used by other engines to control their mutual
access to the \enginesleepsync structure.
There are three other fields in the structure:
\code{state} is used by the engine that owns the structure to communicate
its state to other engines,
\code{action} and \code{action\_data} are used by other threads when
communicating to this engine.

Upon jumping into \idle an engine sets its state to \code{SLEEPING} and
waits on \code{sleep\_sem}.
If another engine posts to \code{sleep\_sem} then the engine is woken up,
and retrieves the value of the \code{action} field.
This field can be used to tell the engine what to do upon wakening.
It may be instructed to shutdown, to execute a context or to steal a spark.
If it is instructed to run a context it can be passed the context directly
using the \code{action\_data} field,
similarly, if it instructed to steal a spark \code{action\_data} will tell
it whose spark stack it should begin its round robin search from.
If thread stealing fails the engine will check the global contest queue for
a runnable context,
if that fails then \idle will loop and putting the engine back to sleep.
If no action is specified
(the value of the \code{action} field is \code{ACTION\_NONE})
them \sleep will try to run a context from the global context queue before
attempting to steal a spark.

\begin{algorithm}[tbp]
\begin{algorithmic}
\Procedure{MR\_wake\_engine}{$engine\_id, action, action\_data, states$}
    \State $eng\_data \gets MR\_engine\_sleep\_data$[$engine\_id$]
    \State MR\_acquire\_lock($eng\_data.lock$)
    \If{$eng\_data.state \in states$}
        \State $eng\_data.action \gets action$
        \State $eng\_data.action\_data \gets action\_data$
        \State $eng\_data.state \gets$ WOKEN
        \State MR\_sem\_post($eng\_data.sleep\_sem$)
        \State $result \gets$ true
    \Else
        \State $result \gets$ false
    \EndIf
    \State MR\_release\_lock($eng\_data.lock$)
    \Return $result$
\EndProcedure
\end{algorithmic}
\caption{\wakeengine}
\label{alg:wake_engine}
\end{algorithm}

Engines may be woken with a call to \wakeengine whose code is shown in
Algorithm \ref{alg:wake_engine}.
\wakeengine takes id of the engine to awaken,
the action and its data that the engine should perform upon awakening,
and the set of states that we may awaken the engine from (usually
\code{SLEEPING}).
The implementation of \wakeengine is mostly straightforward,
the important details are that we must only wake the engine if it has not
been woken already, this is done by checking the value of
\code{eng\_data.state} after acquiring the lock,
and setting the new state while holding the lock.
The action and action data must also be written to the
\enginesleepsync structure before posting to the engine's sleep
semaphore.
The parameter \code{states} is a bit field, with each state represented by
its own bit.
This is flexible, it allows us to message an engine regardless of its
current state.
For example,
we shutdown an engine by calling \wakeengine with the shutdown action a and
all bits set in the \code{states} bit field.
Compared to the previous system in which a lock and condition variable were
used,
this system allows us to wake engines selectively.
This is useful in the case when a context must be executed on a particular
engine because that engine's C stack contains a frame that made a call to
some Mercury code and now must be returned into.
In the future this feature could be used to schedule computations on
\emph{nearby} processors,
processors that may share a second or third level cache with the current
processor.

\begin{algorithm}[tbp]
\begin{algorithmic}[1]
\Procedure{MR\_join\_and\_continue}{$ST, ContLabel$}
  \State $finished \gets$ MR\_atomic\_dec\_and\_is\_zero($ST.num\_outstanding$)
  \If{$finished$}
    \If{$ST.orig\_context = current\_context$}
      \Goto{$ContLabel$}
    \Else
      \While{$ST.orig\_context.resume\_label \neq ContLabel$}
        \State CPU\_relax
      \EndWhile
      \State MR\_release\_context($current\_context$)
      \State $current\_context \gets$ NULL
      \State MR\_load\_context($ST.parent$)
      \Goto{$ContLabel$}
    \EndIf
  \Else
    \If{$ST.orig\_context = current\_context$}
      \State $MR\_r1 \gets ContLabel$
      \Goto{MR\_idle\_orig\_context}
    \Else
      \Goto{MR\_idle}
    \EndIf
  \EndIf
\EndProcedure
\end{algorithmic}
\caption{\joinandcontinue}
\label{alg:join_and_continue_ws2}
\end{algorithm}

By moving spark deques from contexts to engines we had to change \idle.
We have also had to change the \joinandcontinue barrier.
The new version of \joinandcontinue is shown in Algorithm
\ref{alg:join_and_continue_ws2}.
The most significant change is on line 15,
\joinandcontinue no longer attempts to run a spark from the local deque,
it relies on \idle to do this.
The second change is that \joinandcontinue no longer suspends the original
context, it executes \idle while holding the original context.
This would invalidate \idle's precondition that if an engine has a context,
that that context is otherwise free to execute \emph{any} spark,
but an original context may only execute sparks that were created by the
parallel conjunction that it is currently executing.
The new \idle code allows us to create any number of entry points,
we have created the \idleorigcontext entry point which expects to be called
with such a context.
However, since we do not set the contest's \code{resume\_label} field until
the engine has given up the context,
we must pass the value of the continuation label into \idleorigcontext.
This is done using \code{MR\_r1}, the first Mercury abstract machine
register;
this is the normal calling convention used by compiled Mercury procedures.

The other change is an optimisation on line 10.
if the parallel conjunction has been finished and this engine is not
executing the original context,
then instead of placing the original context on the context run queue as the
previous version did,
the new version unloads its current context and resumes execution of the
original context,
This has three benefits,
firstly, this does not use the run queue's lock,
making it more efficient.
Secondly, in the old algorithm, the engine would schedule the context and
call \idle,
in which it would check the context run queue and was likely to run the same
context anyway.
Thirdly, by deliberately running the context on the CPU that most
recently completed the parallel conjunction,
we may be able to take advantage of relevant data being hot in the CPU's
cache.

\begin{algorithm}[tbp]
\begin{algorithmic}
    \Procedure{MR\_idle\_orig\_context}{}
        \State $join\_label \gets MR\_r1$
        \State $jump\_target \gets$ MR\_try\_run\_local\_spark($join\_label$)
        \If{$jump\_target \neq$ NULL}
            \Goto $jump\_target$
        \EndIf
        \State MR\_save\_context($ctxt$);
        \State $current\_context.resume\_label \gets join\_label$
        \State $current\_context \gets$ NULL
        \State $jump\_target \gets$ MR\_try\_run\_context()
        \If{$jump\_target \neq$ NULL}
            \Goto $jump\_target$
        \EndIf
        \State $jump\_target \gets$ MR\_try\_run\_local\_spark(NULL)
        \If{$jump\_target \neq$ NULL}
            \Goto $jump\_target$
        \EndIf
        \State $jump\_target \gets$ MR\_try\_steal\_spark()
        \If{$jump\_target \neq$ NULL}
            \Goto $jump\_target$
        \EndIf
        \Goto MR\_sleep
    \EndProcedure
\end{algorithmic}
\caption{New entry point to the idle loop for dirty contexts.}
\label{alg:idle_orig_context}
\end{algorithm}

The new entry point \idleorigcontext is shown in Algorithm
\ref{alg:idle_orig_context}.
This entry point expects to be called from \joinandcontinue while the engine
is holding a context that is needed to complete a parallel conjunction.
It tries to run a spark from the engine's local spark deque first,
the \tryrunlocalspark will check if the spark at the top of the spark deque
is compatible with the current context,
if it is not it will put the spark back onto the spark deque and return
\NULL, it also returns \NULL if there is no spark on the deque.
\idleorigcontext uses the trampoline macro from Algorithm
\ref{alg:idle_entry_point} to jump to the spark's entry point
\tryrunlocalspark found a valid spark.
Next \idleorigcontext saves the context that the engine was using and then
attempts to resume a suspended context,
failing that it retries running a local spark,
this is done because there may have been a spark on the engine's spark deque
that was not compatible, but now that the engine has no context, any spark
is compatible.
If this also fails then
\idleorigcontext will attempt to steal a spark from another engine and run
it,
failing that the engine will jump to \sleep.
We could add further entry points in the future if we found them useful,
for example,
it may be useful to distinguish between cases in which an engine already has a
context or does not have a context.

\input{tab_work_stealing_revised}

\plan{Benchmark}
\paul{XXX: There's something wrong with the prior left recursive results,
the thread safe sequential time is slower than it should be thus we get stupidly high speedups}
\paul{Also, when comparing absolute times the new work stealing code is
slower, maybe due to the order we check for work in \idle in?}

\plan{Evaluation}

\plan{Further potential work}
\plan{Steal half}
Our work stealing implementation could be improved and tuned further.
We have noticed that in many workloads such as a left recursive parallel
loop one engine creates all the sparks, all the parallelism comes from one
place.
In a situation like this work stealing becomes more common than local work
execution,
one processor would execute work locally while $P - 1$ processors act as
thieves, and a single work queue can again become a bottleneck.
To avoid this behaviour
a steal-half implementation (such as \citet{hendler:2002:stealhalf})
should quickly distributed work evenly between the processors,
reducing the overall number of work stealing operations.
\plan{memory hierarchy awareness}
Another potential improvement is in the selection of a thief's victim.
A thief may wish to prefer victims that are nearby in terms of memory
topology, so that communication of the data relevant to the spark is
cheaper.
This can also be used when selecting which engine to wake up in order to
pass work to it.
We discuss memory hierarchy awareness in more detail in the next section.

