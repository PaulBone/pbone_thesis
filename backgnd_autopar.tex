
\status{
    This section is complete.
}

% Declaration of work done that contributed to a previous award,
Some of the work described in this section contributed towards my honours
research project,
which contributed towards 
% Note that my degree is not '... with Honours', it is just 'Honours', despite
% what my degree actually says.  Basically, I have been erroneously awarded an
% extra bachelor's degree.
the Degree of Bachelor of Computer Science Honours.
I have improved these contributions during my Ph.D.\
candidature.
The following were part of my honours project:
the coverage profiling transformation (Section~\ref{sec:backgnd_coverage}),
the related variable use analysis (Section~\ref{sec:backgnd_var_use_analysis})
and the feedback framework (Section~\ref{sec:feedback}).
Other parts of this section include:
The Mercury deep profiler \citep{conway:2001:mercury-deep}
(Section~\ref{sec:backgnd_deep}) and
\citet*{tannier:2007:parallel_mercury}'s work on automatic parallelism
(Section~\ref{sec:backgnd_priorautopar}).

Most compiler optimisations work on the representation of the program in
the compiler's memory alone.
For most optimisations this is enough.
However,
automatic parallelisation is sensitive to variations in the runtime cost of
parallelised tasks.
This sensitivity increases when dependent parallelisation is used.
For example,
a search operation on a small list is cheap, compared with the same operation on
a large list.
It may not be useful to parallelise the search on the small list against some
other computation,
but it will usually be useful to parallelise the search on the large list
against another computation.
It is important not to create too much parallelism:
The hardware is limited in how many parallel tasks it can execute,
any more and the overheads of parallel execution will slow the program down.
Therefore, it is not just sub-optimal to parallelise the search of the small list,
but detrimental.
The only way we can know the actual cost of most pieces of code
is by understanding their typical inputs,
or measuring their runtime cost while operating on typical inputs.
Therefore,
profiling data should be used in auto-parallelisation;
it allows us to predict runtime costs for computations whose
runtime is dependent on their inputs.

To use profiling data for optimisations,
the usual workflow for compiling software must change.
The programmer compiles the source code with profiling enabled and runs the
resulting program on representative input.
As the program terminates,
it writes its profiling data to disk.
The profiling data includes a bytecode representation of the program,
similar to the compiler's representation.
The analysis tool reads this file,
and writes out a description of how to parallelise the program as a feedback
file.
The programmer then re-compiles their program,
this time with auto-parallelism enabled.
During compilation,
the compiler reads the feedback file and introduces parallelism into the
resulting program at the places indicated by the feedback file.
Figure~\ref{fig:prof_fb} shows this workflow.

\picfigure{prof_fb}{Profiler feedback loop}

\paul{Consider discussing these criticisms in the introduction.}
A common criticism of profile directed optimisation is that the programmer will
have to compile the program twice,
and run it at least once to generate profiling data.
``If I have to run the program to optimise it, then the program has
already done its job,
therefore there is no point continuing with the optimisation?''
The answer to this is that a program's lifetime is far more than a
single execution.
A program will usually be used many times, and by many people.
Each time the optimisation will have a benefit,
this benefit will pay off the cost of the feedback directed optimisation.
We expect that programmers will use feedback-directed optimisations when the
they are building a release candidate of their program,
after they tested the software and fixed any bugs.

Another criticism is that if the program is profiled with one set of input and
used with another,
then the profile will not necessarily represent the actual use
of the program,
and that the optimisation may not be as good as it should be.
Parallelisation decisions are binary,
either `do not parallelise' to `parallelise'.
Therefore,
they will only change as the costs of computations cross some threshold.
Unless alternative input data causes a computation's runtime to cross such a
threshold,
then the parallelisation based on the original input data is 
as equally valid as the parallelisation based on the alternative input data.
Most variations in input data will not  
effect parallelisation enough to cause a significant difference in performance.

Even in a case where automatic parallelisation does not produce optimal
results,
near optimal results are good enough.
Automatic parallelisation will always be cheaper than manual
parallelisation,
it requires much less time and does not require a company hire a
parallelisation expert.
Therefore, automatic parallelisation will be preferred in all but the most
extreme situations.

\subsection{The deep profiler}
\label{sec:backgnd_deep}

Typical Mercury programs make heavy use of code reuse,
especially through parametric polymorphism and higher order calls.
This is also true for other declarative languages.
For example, while a C program may have
separate tables for different kinds of entities,
for whose access functions
the profiler would gather separate performance data,
most Mercury programs would use
the same polymorphic code to handle all those tables,
making the task of disentangling the characteristics of the different tables
infeasibly hard.

\citep{conway:2001:mercury-deep}
solved this problem for Mercury by introducing deep profiling.
Mercury's deep profiler gathers much more information about the context of
each measurement than traditional profilers such as \code{gprof} \citep{gprof} do.
When it records the event of a call,
a memory allocation or a profiling clock interrupt,
it records with it the chain of ancestor calls,
all the way from the current call to the entry point of the program
(\code{main/2}).
To make this tractable,
recursive and mutually recursive calls,
known as \emph{strongly connected components} (SCCs),
must be folded into a single memory structure.
Therefore, the call graph of the program is a tree (it has no cycles)
and SCCs are represented as single nodes in this tree.

Deep profiling allows the profiler to find and present to the user
not just information such as the total number of calls to a procedure
and the average cost of a call,
or even information such as the total number of calls to a procedure
from a particular call site and the average cost of a call from that call
site.
It will provide information such as the total number of calls to a procedure
\emph{from a particular call site
when invoked from a particular chain of ancestor SCCs}
and the average cost of a call \emph{in that context}.
We call such a context for profiling data an \emph{ancestor context}.
For example, it could tell that
procedure $h$ called procedure $i$ ten times
when $h$'s chain of ancestors was $main \calls f \calls h$,
while $h$ called $i$ only seven times
when $h$'s chain of ancestors was $main \calls g \calls h$,
the calls from $h$ to $i$ took on average twice as long
from the $main \calls g \calls h$ context as from $main \calls f \calls h$,
so that despite making fewer calls to $i$,
$main \calls g \calls h \calls i$ took more time than $main \calls f \calls h \calls i$.
This is shown in Figure~\ref{fig:call_tree5}.

\picfigure{call_tree5}{Example call graph}

It can be difficult to locate a single goal within the body of a procedure.
This is made more difficult as nested unifications are flattened into super
homogeneous form
(Section~\ref{superhomogeneous} on page~\pageref{superhomogeneous}),
meaning that what might be a single line of code in a source file
can be a conjunction of dozens of goals.
Therefore the profiler uses \emph{goal paths} to uniquely identify a sub
goal within a procedure's body.
A goal path is a list of goal path steps, each step describes which
sub-goal to recurse into when traversing a goal structure from its
root (the procedure as a whole)
to either its leaves (atomic goals) or some compound goal.
The goal path ``c3;d2'' refers to the second disjunct ``d2'' within the
third conjunct ``c3''.
Goal paths where first introduced to support debugging in Mercury \citep{mdb}.
The deep profiler was written after the debugger and was able to re-use them.

We use four structures during profiling.

\begin{description}

    \item[\PD]
    represents a procedure within the program's call tree.
    There can be multiple \PD structures for any given procedure:
    the same procedure will usually be called from a number of different
    contexts in the program.

    \item[\CSD]
    represents a call site in the program's call tree.
    As with \PD, there may be multiple \CSD structures for any given
    call site.
    Profiling data for the call site \& context is stored within its \CSD
    structure.
    A \PD structure has an array of pointers to \CSD structures representing
    the calls used during the execution of the procedure.
    A \CSD structure has a pointer to the \PD structure of its callee.
    These links represent the call graph of the program.

    Higher order and method calls are tracked by the \PD structure of the
    caller,
    the array of pointers is actually an array of arrays of pointers,
    multiple items in the second array represent different higher order
    values at the same call site.
    This means that a number of \CSD structures can represent a single
    higher-order call site.
    This is actually rare:
    A call site and ancestor context are usually used with a single higher
    order value.
    Other higher order values are associated with other ancestor contexts
    and are therefore represented by other parts of the call graph.

    \item[\PS]
    represents the static data about a procedure
    This data includes the procedure's name, arity, source file and line
    number.
    Because multiple \PD structures may exist for the same procedure,
    this structure is used to factor out common information.
    Each \PD structure has an pointer to its \PS structure.
    Each procedure in the program has a single \PS structure.
    Each \PS structure has an array of pointers to the \CSS structures
    of the calls its procedure makes.

    \item[\CSS]
    represents static data for a call site.
    This includes the procedure the call site is found in, the line number,
    the goal path and a pointer to the \PS structure of the callee (if known
    statically).
    As with \PS structures,
    \CSS structures reduce the memory usage of \CSD structures by
    factoring out common information.
    Each call site in the program has a single \CSS structure.

\end{description}

\noindent
The profiling data is stored on disk using the same four structures that
where used while capturing the data.
When the profiling tool reads in this data,
it will build the call graph and generate the list of SCCs
(which \citet*{conway:2001:mercury-deep} call \emph{cliques})
each SCC is used to create a \Clique data structure.
It also constructs several indexes,
these indexes together with the {\Clique}s make traversing the program's
profile (the other structures) more efficient.
This data is used both by
the interactive user interface,
and automated tools.

% Callseqs
Profilers have traditionally measured time
by sampling the program counter at clock interrupts.
Unfortunately, even on modern machines,
the usual portable infrastructure for clock interrupts
(\emph{e.g}., SIGPROF on Unix)
supports only one frequency for such interrupts,
which is usually 60 or 100Hz.
This frequency is far too low for the kind of detailed measurements
the Mercury deep profiler wants to make.
For typical program runs of few seconds,
it results in almost all calls having a recorded time of zero,
with the calls recording a nonzero time
(signifying a profiling interrupt during their execution)
being selected almost at random.

We have therefore implemented a finer-grained measure of time
that turned out to be very useful
even though it is inherently approximate.
This measure is \emph{call sequence counts} (CSCs):
the profiled program basically behaves
as if the occurrence of a call signified
the occurrence of a new kind of profiling interrupt.
In imperative programs, this would be a horrible measure,
since calls to different functions often have hugely different runtimes.
However, in declarative languages like Mercury there are no explicit loops;
what a programmer would do with a loop in an imperative language
must be done by a recursive call.
This means that the only thing that the program can execute between two calls
is a sequence of primitive operations such as unifications and arithmetic.
For any given program,
there is a strict upper bound on the maximum length of such sequences,
and the distribution of the length of such sequences
is very strongly biased towards very short sequences
of half-a-dozen to a dozen operations.
In practice, we have found that
the fluctuations in the lengths of different sequences
can be ignored for any measurement
that covers any significant number of call sequence counts,
say more than a hundred.
The only drawback that we have found is that on 32 bit platforms,
its usability is limited to short program runs (a few seconds)
by the wraparound of the global CSC counter;
This has not been a concern for a number of years.
On 64 bit platforms, the problem would only occur
on a profiling run that lasted for years.

\subsection{Prior auto-parallelism work}
\label{sec:backgnd_priorautopar}

\citet{tannier:2007:parallel_mercury} describes a prior attempt at automatic
parallelism in Mercury.
In his approach, 
an analysis tool gathered parallelisation feedback data in the form of
a list of the procedures with the highest costs in the program.
Ancestor context independent data was used,
meaning that all uses of a procedure were considered when ranking that
procedure within the list.
The user was able to choose between using the mean or median
execution time of a procedure, measured in CSCs.
Only procedures whose cost was greater than a configurable threshold were
included in the feedback data.
The analysis did not use a representation of the program,
as at that time the profiler did not support that capability.

The compiler used the feedback data to make parallelisation decisions.
Each procedure being compiled by the compiler was searched for calls that
appeared in the list of top procedures and calls to these were parallelised
against similar calls if they where independent or had fewer shared
variables than another configurable limit.

\label{honours_autopar}
For my honours project I attempted automatic parallelisation of Mercury
\citep{bone:2008:hons}.
My work differed from Tannier's in a number of ways:
The first differences is that at the time of my work,
the deep profiler was able to access a representation of the program
(my work was the motivation for this feature).
This allowed my analysis to use both profiling data and the representation
of the program at the same time,
making it possible to measure to the times at which variables are produced
and consumed within parallel conjuncts.
The analysis uses this to calculate how much parallelism is available,
for conjunctions with a single shared variable.
Consider the timeline shown in Figure~\ref{fig:overlap4},
In the sequential case the computations \code{p} and \code{q} are shown
side-by-side.
Below this, \code{q} is parallelised against \code{p}.
The future \code{A} is communicated from \code{p} to \code{q},
\code{q} will execute the \wait operation for \code{A}
before \code{p} executed \signal, therefore \wait will suspend \code{q}.
Later, \code{p} executes \signal, producing \code{A}
and scheduling \code{q} for execution.
The formula used to compute the execution times in the sequential and parallel
cases, and the speedup due to parallelism is:

\picfigure{overlap4}{Overlap of \code{p} and \code{q}}

\begin{eqnarray*}
T_{Sequential} & = & T_p + T_q \\
T_{DependentQ} & = & max(T_{BeforeProduceP}, T_{BeforeConsumeQ}) +
T_{AfterConsumeQ} \\
T_{Parallel} & = & max(T_p, T_{DependentQ}) + T_{Overheads} \\
Speedup & = & \frac{T_{Sequential}}{T_{Parallel}}
%\label{eqn:time_deppar}
\end{eqnarray*}

This describes the speedup owing to parallelisation of two conjuncts,
\code{p}
and \code{q}.
whose execution times are $T_p$ and $T_q$.
In the parallel case \code{q}'s execution time is $T_{DependentQ}$ since it
accounts for the \code{q}'s dependency on \code{A}.
Except for this small difference,
the calculation of parallel execution time and speedup are the commonly
used formulas for many parallel execution cost models.
We will greatly expand and generalise this calculation in
Chapter~\ref{chap:overlap}.

This cost model requires information about when a shared variable is
produced by the first conjunct and when it is consumed by the second.
To provide this information,
we introduced a variable use time analysis
(Section~\ref{sec:backgnd_var_use_analysis}),
that depends upon information about how often different goals are executed,
called \emph{coverage}.
The deep profiler provides coverage information for most goals,
however we need coverage information for all goals
and therefore we introduced coverage profiling.
% Removed reference since this is the very next section.

\subsection{Coverage profiling}
\label{sec:backgnd_coverage}

\paul{Explain how the algorithms work in tandem, this seems to be a point
that readers have trouble with.}

Code \emph{coverage} is a term often used in the context of debugging and
testing:
it refers to which parts of a program get executed and which do not.
It is used to determine the completeness of a test-suite.
We use the term code coverage,
which we often abbreviate to ``coverage'',
to describe how \emph{often} a given piece of code is executed.
Thus, we have extended the concept of coverage from a binary ``has been
executed'' or ``has not been executed'' to a more informative
``has been executed $N$ times''.

The traditional Prolog box
model~\citep{box-model} describes each predicate
as having four ports through which control flows.
Control flows into a predicate through either the \emph{call} or
\emph{redo} ports,
control flows out of a predicate through either the \emph{exit} or
\emph{fail} ports.
These describe:
    a call being made to the predicate,
    re-entry to the predicate after it has exited to check for more
        solutions,
    the predicate exiting with a solution,
    or the predicate failing because there are no more solutions.
Mercury adds an extra port,
known as the \emph{exception} port:
when a predicate throws an exception it returns control via the exception
port.
Provided that execution has finished,
control must have left a predicate the same number of times that it entered
it.

\begin{equation*}
Calls + Redos = Exits + Fails + Exceptions
\end{equation*}

\noindent
Mercury adds additional invariants for many determinism types.
For example, deterministic code may not redo or fail,
therefore these port counts will be zero.
For each call and its ancestor context the
deep profiler will track the number of times each port is used,
Port counts provide code coverage information for predicate and call site
entry and exit.

Code coverage of many goals can be inferred.
In the simple case of a conjunction containing deterministic conjuncts,
each conjunct will have the same coverage as the one before it provided
that the earlier goal does not throw an exception.
There are two types of exception,
those that are caught and those that terminate the program.
When an exception terminates the program, we are not interested in that
execution's profile,
when an exception is caught and the program recovers we are interested in
coverage, however this is rare.
A deterministic conjunction containing a single call
can have complete coverage information inferred for all of its conjuncts,
provided that we are willing to loose a little accuracy if an exception is
thrown.
Similarly,
the coverage before an \emph{if-then-else} can be inferred as the sum
of the coverage at the beginning of the \emph{then} and \emph{else}
branches,
assuming that the condition has at most one solution.
The same is true for the ends of these branches and the end of the
if-then-else.
The coverage at the beginning of the then part is also equal to the
number of times the \emph{condition} of the if-then-else succeeds.
The coverage of a switch is the sum of the branch's coverage plus the number
of times the switch may fail.
Many switches are known to be complete, and therefore they cannot fail.
This can allow us to infer the missing coverage of a single branch provided
that we know the coverage of all the other branches plus the coverage of the
switch.
Or to infer the coverage of the switch provided that we know the coverage of
all the branches.

Based on these observations,
we have implemented a coverage inference algorithm in the deep profiler.
The coverage inference algorithm makes a single forward pass
tracking the coverage before the current goal if known,
when it reaches a call,
it can read the port counts of the call site to set the coverage 
before and after the current goal.
When it encounters a deterministic goal,
it can set that goal's coverage to the coverage before the current goal.
The inference algorithm will also infer the coverage of the then and else
parts of an if-then-else, as described above;
as well as the coverage of the last branch of a complete switch,
as described above.
% A good reader might realize that we have only discussed a forward pass,
% more complicated passes are possible but the reader will have to take
% in on faith that this will be discussed later in this section.

Unfortunately, port counts alone will not provide complete coverage
information.
For example, a conjunction containing a sequence of \dsemidet
unifications will have incomplete coverage information:
we cannot know how often each individual unification fails.
We introduced coverage profiling as part of Mercury's deep profiler to
add extra instrumentation to gather coverage data where it would
otherwise be unknown.
The coverage inference algorithm will look for a coverage point whenever
it cannot infer the coverage from earlier goals.

We want to reduce the number of coverage points added to a procedure while
still allowing inference of complete coverage information.
This is because instrumentation,
such as a coverage point,
can slow the program down and sometimes distort the profile,
\footnote{
    Automatic parallelism analysis is not affected by profile distortion
    since it uses call sequence counts to measure time.
    Nevertheless,
    avoiding profile distortion is good as
    programmers may continue to use the profiler's other
    time measurements.}
therefore preventing superfluous instrumentation from being added is
desirable.
We do this in the coverage profiling transformation.
The coverage profiling transformation makes the exact same traversal through
the program as the coverage inference algorithm.
Instead of tracking the coverage before the current goal
the coverage profiling transformation tracks a boolean that indicates
whether or not coverage would be known before the current goal based on the
goals before it.
When this boolean becomes false,
the transformation will introduce a coverage point at that place in the code
and set the value to true.
This has the effect of placing a coverage point at exactly the location at
which the coverage inference algorithm would attempt to look up coverage
from a coverage point.
The coverage profiling transformation will set the value of its boolean
to false after a goal with a determinism other than
\ddet or \dccmulti and which does not provide coverage data of its own.
It also sets the boolean to false at the beginning of a branch whose
coverage cannot be inferred.

\begin{figure}
\paul{XXX: Should I center things in figures such as this?}
\begin{tabular}{l}
\code{map(P, Xs0, Ys) :-} \\
\code{~~~~(} \\
\code{~~~~~~~~}\instr{coverage\_point(ProcStatic, 0);} \\
\code{~~~~~~~~Xs0 = [],} \\
\code{~~~~~~~~Ys = []} \\
\code{~~~~;} \\
\code{~~~~~~~~Xs0 = [X $|$ Xs],} \\
\code{~~~~~~~~P(X, Y),} \\
\code{~~~~~~~~map(P, Xs, Ys0),} \\
\code{~~~~~~~~Ys = [Y $|$ Ys0]} \\
\code{~~~~).} \\
\end{tabular}
\caption{Coverage annotated \code{map/3}.}
\label{fig:map_coverage}
\end{figure}

Figure~\ref{fig:map_coverage} shows \code{map/3} annotated with a
coverage point.
The forward pass of the coverage inference algorithm cannot infer
coverage at the beginning of this switch arm,
and therefore the coverage profiling transformation in the deep profiler
introduced a coverage point at that position.
It is true that the coverage point could be removed and the port
counts of the recursive code in this example could be used,
but doing so would require multiple passes
for both algorithms,
making them slower.
Coverage profiling adds a small cost to profiling
(6\% slower than normal deep profiling);
it is not a high priority to optimise this further.

During my honours project I added three new fields to the \PS structure,
which we introduced in Section~\ref{sec:backgnd_deep}.
These fields are:
a constant integer representing the number of coverage points in this
procedure;
an array of constant structures,
each describing the static details of a coverage point;
and an array of integers representing the number of times each coverage
point has been executed.
The coverage point instrumentation code refers to the procedure's \PS
structure and increments the coverage point's counter at the appropriate index.
The static coverage data,
which includes the goal path of the coverage point and the type of the
coverage point (branch or post-goal),
is written out with the profile.
We were concerned that a procedure might require dozens of coverage points,
each one consuming memory,
therefore to avoid the risk of consuming large amounts of memory we choose
to associate coverage points with \PS structures rather than the more
numerous \PD structures.
Therefore coverage data is not associated with ancestor contexts in the way
that other profiling data is.
The auto-parallelisation work described in my honours report
did not expect to use ancestor context specific coverage information.

The coverage point in Figure~\ref{fig:map_coverage}
has two arguments,
the first points to the \PS containing this procedure's coverage points,
the second is the index within this procedure's coverage point arrays
for this coverage point.
Although the instrumentation code appears to be a call,
it is implemented as a C preprocessor macro as it is very small and should
be inlined into the compiled procedure.


\subsection{Variable use time analysis}
\label{sec:backgnd_var_use_analysis}

The automatic parallelism analysis used in my honours report considers
conjunctions with at most two conjuncts and at most one shared variable.
To predict how much parallelism is available,
it needs to know when that variable is produced by the first conjunct,
and when it is first consumed by the second.
These times are calculated in units of call sequence counts.

During compilation of dependent conjunctions,
the compiler creates a future to replace any shared variable.
The compiler will attempt to push that future into the goal that
produces its value,
so that,
the \signal operation is after, and as close as possible
to, the unification that binds the variable.

\begin{algorithm}
\[
\begin{array}{l @{}l}
\prodtime V\,G                            &=
    \begin{cases}
        \timef G         & \text{if } G \text{ is not executed or
                            \derroneous} \\
        \prodtimep V\,G  & \text{if } G \text{ is \ddet or \dccmulti} \\
        \canthappen      & \text{otherwise}
    \end{cases} \\
\end{array}
\]
\[
\begin{array}{l @{}l @{}l}
\prodtimep V\,& \pat{X = Y}                        &= 0 \\
\prodtimep V\,& \pat{X = f(\ldots)}                &= 0 \\
\prodtimep V\,& \pat{p(X_1,\,\ldots,\,X_n)}        &=
    \callprodtime V\,p\,[X_1,\,\ldots,\,X_n] \\
%    \prodtime V\,(\operatorname{body} p) \\
\prodtimep V\,& \pat{X_{n+1} = \,f(X_1,\,\ldots,\,X_n)}  &=
    \callprodtime V\,f\,[X_1,\,\ldots,\,X_n,\,X_{n+1}] \\
%    \prodtime V\,(\operatorname{body} f) \\
\prodtimep V\,& \pat{X_0(X_1,\,\ldots,\,X_n)}      &= \timeofcall \\
\prodtimep V\,& \pat{m(X_1,\,\ldots,\,X_n)}        &= \timeofcall \\
\prodtimep V\,& \pat{foreign(\ldots)}              &= \timeofcall + 1 \\
\prodtimep V\,& \pat{G_1,\,G_2,\,\ldots,\,G_n}     &=
   \begin{cases}
       \prodtime V\,G_1
            & \text{if } G_1 \text{ binds } V \\
       \left(
       \begin{array}{l}
           \timef G_1 + \\ \prodtime V\,\pat{G_2,\,\ldots,\,G_n}
       \end{array}
       \right.
            & \text{otherwise}
   \end{cases} \\
\prodtimep V\,& \pat{G_1\,\&\,\ldots\,\&\,G_n}      &= \canthappen \\
\prodtimep V\,& \pat{G_1\,;\,\ldots\,;\,G_n}        &= \canthappen \\
\prodtimep V\,& 
    \left\llbracket
    \begin{array}{l}
        \hbox{switch}\,X\,(f_1:\,G_1\,; \\
        \quad\ldots\,,f_n:\,G_n)
    \end{array}
    \right\rrbracket &=
    \sum\limits_{1 \leq i \leq n}\,Pr_{G_i} \times \prodtime\,V\,G_i \\
\prodtimep V\,& 
    \left\llbracket
    \begin{array}{ll}
        \hbox{if}\,Cond\,&\hbox{then}\,Then \\
                         &\hbox{else}\,Else)
    \end{array}
    \right\rrbracket &= \timef Cond +
    \left(
        \begin{array}{l}
            Pr_{Then} \times \prodtime V\,Then+ \\
            Pr_{Else} \times \prodtime V\,Else
        \end{array}
    \right) \\
%    \Avg_{G \in \{Then,\,Else\}} \prodtime V\,G \\
\prodtimep V\,& \pat{not\,G}                       &= \canthappen \\
\prodtimep V\,& \pat{some\,[X_1,\ldots,X_n]\,G}    &= \prodtime V\,G \\
%\prodtimep V\,& \pat{all\,[X_1,\ldots,X_n]\,G}     &= \prodtime V\,G \\
\prodtimep V\,& \pat{promise\_pure\,G}             &= \prodtime V\,G \\
\prodtimep V\,& \pat{promise\_semipure\,G}         &= \prodtime V\,G \\
\end{array}
\]
\[
\begin{array}{l @{}l}
\callprodtime V\,P\,Args        \,&=
    \begin{cases}
        \prodtime Params\,B &
            \text{if } P \text{ is in the current module} \\
        \timef B &
            \text{otherwise}
    \end{cases} \\
    & \begin{array}{l l @{}l}
    \text{where} & Params  &= \operatorname{arg\_to\_param}\, V\, Args \\
                 & B       &= \operatorname{body}\, P \\
    \end{array}
\end{array}
\]
\[
\begin{array}{l @{}l}
\timef G    \,&= \text{The duration of } G\text{'s execution.} \\
\end{array}
\]
\caption{Variable production time analysis}
\label{alg:var_prod_time}
\end{algorithm}

The algorithm for computing the expected production time
of a shared variable looks at the form of the conjunct that produces it,
this is shown in Algorithm~\ref{alg:var_prod_time}.
This algorithm is only invoked on goals that are either \ddet or \dccmulti,
and that produce the variable $V$.
For each goal type,
it calculates at what point within the goal's execution
the future for $V$ will signalled.
As noted above,
a future cannot be pushed into certain goal types,
such as higher order calls, method calls,
foreign code or calls that cross module boundaries.
Therefore, a conservative answer is used in these cases,
the conservative answer is `at the end of $G$'s execution'.
This is conservative not just because it is typical,
but because it will avoid creating parallelisations that are not likely
to speed up the program.
The case for foreign calls does not measure a call sequence count for the
foreign call itself, so we add one.
Goals such as unifications are trivial, and always have zero cost,
therefore they must produce their variable after zero call sequence counts
of execution.
Parallel conjunctions are converted into sequential conjunctions before
profiling,
they will not appear in the algorithm's input.
A negation cannot produce a variable and therefore it cannot produce $V$,
the algorithm does not need to handle negations.
The algorithm is only invoked on goals that are \ddet or \dccmulti,
and the only disjunction that can be \ddet or \dccmulti is one that does not
produce outputs,
and therefore cannot produce $V$,
the algorithm does not need to handle disjunctions either.

The algorithm handles conjunctions by determining which conjunct
$G_k$ in the conjunction $G_1,~\ldots,~G_k,~\ldots,G_n$ produces the
variable.
The production time of the variable in the conjunction is its production
time in $G_k$ plus the sum of the durations of $G_1,~\ldots,~G_{k-1}$.

We handle switches by invoking the algorithm recursively on each switch
arm,
and computing a weighted average of the results,
with the weights being the arms' entry counts as determined by coverage
inference.
We handle if-then-elses similarly.
we need the weighted average of the two possible cases:
the variable being generated by the then arm versus the else branch.
(It cannot be generated by the condition:
variables generated by the condition are visible only from the then branch.)
% TODO: implement this behaviour
To find either number,
we invoke the algorithm on either the then or else branch,
depending on which is applicable,
and add this to the time taken by the condition.
Note that this is the time taken by the condition on average
(whether or not it fails),
we do not have access to timing information dependent on a goal's success or
failure.
This is not a concern, most condition goals are inexpensive,
therefore their runtimes do not vary significantly.

Using the weighted average for switches and if-then-elses is meaningful because
the Mercury mode system dictates
that if one branch of a switch or if-then-else generates a variable,
then they \emph{all} must do so.
The sole exception is branches that are guaranteed to abort the program,
whose determinism is erroneous.
We use a weight of zero for erroneous branches 
because optimising them does not make sense.
Coverage profiling was introduced to provide the weights of switch and
if-then-else arms used by this algorithm.

If the goal is a quantification,
then the inner goal must be \ddet,
in which case we invoke the algorithm recursively on it.
If the inner goal were not \ddet,
then the outer quantification goal could be \ddet
only if the inner goal did not bind any variables visible from the outside.
Therefore, if the analysis is invoked on a quantification,
we already know that this is because the quantification produces the
variable, and therefore the inner goal must be \ddet.

\begin{algorithm}
\[
\begin{array}{l @{}l @{}l}
\constime V\,& G                            &=
    \begin{cases}
        0                & \text{if } G \text{ is not executed or
                            \derroneous} \\
        \constimep V\,G  & \text{if } G \text{ is \ddet, \dsemidet,
            \dfailure, \dccmulti or \dccnondet} \\
        \canthappen           & \text{otherwise}
    \end{cases} \\
\end{array}
\]
\[
\begin{array}{l @{}l @{}l}
\constimep V\,& \pat{X = Y}                        &= 0 \\
\constimep V\,& \pat{X = f(\ldots)}                &= 0 \\
\constimep V\,& \pat{p(X_1,\,\ldots,\,X_n)}        &=
    \callconstime V\,p\,[X_1,\,\ldots,\,X_n] \\
\constimep V\,& \pat{X_{n+1} = \,f(X_1,\,\ldots,\,X_n)}  &=
    \callconstime V\,f\,[X_1,\,\ldots,\,X_n,\,X_{n+1}] \\
\constimep V\,& \pat{X_0(X_1,\,\ldots,\,X_n)}      &= 0 \\
\constimep V\,& \pat{m(X_1,\,\ldots,\,X_n)}        &= 0 \\
\constimep V\,& \pat{foreign(\ldots)}              &= 0 \\
\constimep V\,& \pat{G_1,\,G_2,\,\ldots,\,G_n}     &=
   \begin{cases}
       \constime V\,G_1
            & \text{if } G_1 \text{ consumes } V \\
       \left(
       \begin{array}{l}
           \timef G_1 + \\ \constime V\,\pat{G_2,\,\ldots,\,G_n}
       \end{array}
       \right.
            & \text{otherwise}
   \end{cases} \\
\constimep V\,& \pat{G_1\,\&\,\ldots\,\&\,G_n}     &= \canthappen \\
\constimep V\,& \pat{G_1\,;\,\ldots\,;\,G_n}       &= 
    \begin{cases}
        \constime V\,G_1
            & \text{if } G_1 \text{ consumes } V \\
        \left(
        \begin{array}{l}
            \timef G_1 + \\ \constime V\,\pat{G_2,\,\ldots,\,G_n}
        \end{array}
        \right.
            & \text{otherwise}
    \end{cases} \\
\constimep V\,& 
    \left\llbracket
    \begin{array}{l}
        \hbox{switch}\,X\,(f_1:\,G_1\,; \\
        \quad\ldots\,,f_n:\,G_n)
    \end{array}
    \right\rrbracket &=
    \sum\limits_{1 \leq i \leq n}\,Pr_{G_i} \times \constime\,V\,G_i \\
\constimep V\,& 
    \left\llbracket
    \begin{array}{ll}
        \hbox{if}\,Cond\,&\hbox{then}\,Then \\
                         &\hbox{else}\,Else)
    \end{array}
    \right\rrbracket &= \iteconstime V\,Cond\,Then\,Else \\
\constimep V\,& \pat{not\,G}                       &= \constime V\,G \\
\constimep V\,& \pat{some\,[X_1,\ldots,X_n]\,G}    &= \constime V\,G \\
%\constimep V\,& \pat{all\,[X_1,\ldots,X_n]\,G}     &= \constime V\,G \\
\constimep V\,& \pat{promise\_pure\,G}             &= \constime V\,G \\
\constimep V\,& \pat{promise\_semipure\,G}         &= \constime V\,G \\
\end{array}
\]
\[
\begin{array}{l @{}l}
    \begin{split}
        \iteconstime V\,Cond\,\\
        Then\,Else
    \end{split} &=
\begin{cases}
    \constime V\,Cond   & \text{\parbox{1in}{if $Cond$ \\ consumes $V$}} \\
    \begin{split}
        \timef Cond +
            Pr_{Then} \times \constime V\,Then + \\
            Pr_{Else} \times \constime V\,Else
    \end{split}
        & otherwise \\
\end{cases} \\
\end{array}
\]
\[
\begin{array}{l @{}l}
\callconstime V\,P\,Args        \,&=
    \begin{cases}
        \constime Params\,B &
            \text{if } P \text{ is in the current module} \\
        0 &
            \text{otherwise}
    \end{cases} \\
    & \begin{array}{l l @{}l}
    \text{where} & Params   &= \operatorname{arg\_to\_param}\, V\, Args \\
                 & B        &= \operatorname{body}\,P \\
    \end{array}
\end{array}
\]
\[
\begin{array}{l @{}l}
\timef G    \,&= \text{The duration of } G\text{'s execution.} \\
\end{array}
\]
\caption{Variable consumption time analysis}
\label{alg:var_cons_time}
\paul{XXX: Try to place this on the facing page from its reference}
\end{algorithm}

The algorithm we use for computing the time
at which a shared variable is first consumed by the second conjunct
is similar,
it is shown in Algorithm~\ref{alg:var_cons_time}.
The main differences are that
negated goals, conditions and disjunctions are allowed to consume variables,
and some arms of a switch or if-then-else
may consume a variable even if other arms do not.
The code that pushes wait operations into goals can be configured to include
a \wait operation on every execution path so that when the goal completes,
it is guaranteed that a \wait operation has taken place.
The variable use analysis should reflect the compiler's configuration with
respect to how \wait operations are placed but it does not.
It makes the conservative assumption that waits are always placed in switch
arms of switches and if-then-elses that do not consume the variable.
This assumption is a conservative approximation.
This can prevent introducing parallelism that leads to a slow-down.
To make this assumption,
the algorithm checks if a goal consumes a variable, if it does not,
it returns the goal's execution time as the consumption time of the variable.
% XXX: This is not yet implemented.
The other important difference is that when making
any other conservative assumption
such as when the algorithm finds a higher order call,
the consumption time used is zero
(it was the cost of the call when analysing for productions)

For example
suppose the first appearance of the variable (call it $X$)
in a conjunction $G_1, \ldots, G_n$ is in $G_k$, and $G_k$ is a switch.
If $X$ is consumed by some switch arms and not others,
then on some execution paths,
the first consumption of the variable may be in $G_k$ (a),
on some others it may be in $G_{k_1}, \ldots, G_n$ (b),
% XXX: We do not handle c - but we should.
and on some others it may not be consumed at all (c).
For case (a),
we compute the average time of first consumption by the consuming arms,
and then compute the weighted average of these times,
with the weights being the probability of entry into each arm, as before.
For case (b), we compute the probability of entry
into arms which do \emph{not} consume the variable,
and multiply the sum of those probabilities
by the weighted average of those arms' execution time
\emph{plus} the expected consumption time of the variable
in $G_{k+1},~\ldots,~G_n$.
For case (c)
we pretend $X$ is consumed at the very end of the goal,
and then handle it in the same way as (b).
This is because for our overlap calculations,
a goal that does not consume a variable is equivalent to
a goal that consumes it at the end of its execution.

\subsection{Feedback framework}
\label{sec:feedback}

Automatic parallelism is just one use of profiler feedback.
Other optimisations
such as inlining,
branch hints
and type specialisation
might also benefit from profiling feedback.
\paul{I have found some papers but I am yet to read them.}
%  Feedback linking: optimizing object code layout for updates
%  Link-time optimization of ARM binaries
%  Feedback directed optimization in Compaq's compilation tools for Alpha
%  Workshop on Feedback-Directed Optimization

During my honours project I designed and implemented a generic feedback
framework that allows tools to create feedback information for the compiler.
These tools may include the profiler, the compiler itself,
or any tool that links to the feedback library code.
Any Mercury value can be used as feedback information,
making the feedback framework flexible.
New feedback-directed optimisations may require feedback information
from new analyses.
We anticipate that many new feedback information types will be added to
support these optimisations.
We also expect that an optimisation may use more than one type of
feedback information and that more than one optimisation may use the
same feedback information.

% Describe on-disk format,
The on-disc format for the feedback information is very simple:
it contains a header that identifies the file format,
including a version number,
followed by a list of feedback information items,
stored in the format that Mercury uses for reading and writing
terms.
When the feedback library reads the file in
it will check the file format identifier and version number,
thet it will read the list of information items and check that no two
items describe the same type of feedback information.

The API allows developers to open an existing file or create a new one,
query and set information in the in-memory copy of the file,
and write the file back out to disk.
It is straight forward to open the file,
update a specific feedback item in it,
and close it,
leaving the other feedback items in the file unchanged.



% Incorrect background, some of this prose can be used later.
%This approach also used profiling feedback data similar to Tannier's
%work.
%However, it also made use of and some of the advanced
%features of the deep profiler.
%The deep profiler was modified to export a representation of the profiled
%program.
%This representation, along with the profiling data could both be accessed by
%the analysis.
%My approach differed from Tannier's in that the analysis was done outside of the
%compiler in an analysis tool.
%The feedback given to the compiler was the result of this analysis:
%a list of candidate parallel conjunctions that the compiler would attempt to parallelise.
%Each candidate parallel conjunction describes its location in the original
%program and how its conjuncts should be constructed of smaller goals.
%However, at the completion of my honours project the implementation was incomplete
%and no feedback data was actually given to the compiler.
%Figure~\ref{fig:prof_fb} closely describes the workflow for our feedback
%analysis.
%
%% call graph traversal.
%
%The auto-parallelism analysis traverses the call graph looking for parallelism
%opportunities.
%The traversal begins at the root of the call graph,
%namely \code{main/2}.
%Each node in the graph is an SCC, and each edge is a call from a call site to a
%distinct callee.
%Each procedure in an SCC is searched for parallelisation candidates before any
%child-nodes are checked (which are also SCCs).
%The search continues in this fashion until it finds an edge
%representing a call whose cost is too low such that it is not worth parallelising
%anything within the callee, and therefore, in the callee's callees.
%In practice, only a small part of the callgraph is traversed,
%since most parts of a program have a very low runtime cost and are not worth
%parallelising.
%The search will also track the number of processors available for spawned-off work
%at any point in the call graph,
%as parallelisation candidates are found, this number will decrease.
%Once it reaches zero, the search will stop as parallelising anything below this
%point in the call graph will not create any useful parallelism.
%This traversal could only be done with access to both a representation of the
%program being profiled and profiling data of the program.
%Therefore, implementing the analysis in a venerate tool was the obvious choice.
%
%Because each node in the graph is an SCC, and the graph is a tree
%the traversal does not need to worry about following cycles infinitely.
%However,
%determining the cost of recursive code is problematic.
%If a predicate calls itself, we cannot know the cost of the recursive
%call,
%this cost is attributed to the cost of the call of that predicate which
%is already active.
%The automatic parallelism analysis described in \citet*{paul_hons}
%treats recursive calls naively,
%A solution to this is presented in section~\ref{sec:recursive_call_costs}.
