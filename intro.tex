%
% vim: ft=tex ts=4 sw=4 
%
\chapter{Introduction}
\label{chap:intro}

\status{This chapter is ready for review by Zoltan.}

\plan{Why multicore}
In 1965 Moore \citep{moore} predicted that the transistor 
density on processors would double roughly every two years,
and at the same time the manufacturing cost would fall.
This is known as Moore's Law,
it is often misquoted saying that processors' clock speeds would double
roughly every two years.
This is due, in part, to the strong relationship between transistor density and
clock speeds that existed for about 40 years.
Around 2005 it became clear that this relationship would be broken;
processor clock speeds would no longer continue to increase
exponentially \citep{free_lunch}.
%One interesting example of this is \citet{intel-high-k},
%which describes how electrons can quantium tunnel through the thinest parts
%of a transistor's gate creating wasted current and excess heat.
%Changing the transistor's materials improved the situation.

To continue to deliver performance without increasing clock speeds
manufacturers now build processors that can do more work in parallel.
There are a number of ways to achieve this including:
Single Instruction Multiple Data (SIMD) instruction sets,
and super-scalar execution such as pipelining and reordering.
One of the most notable improvements is multicore execution.
Multiple processing cores can be placed in the same package,
and usually on the same die.
Individual cores work separately, communicating through the memory
hierarchy.
Multicore computing is only the latest form of multiprocessor computing.

\plan{SMP \& programming}
Prior forms of multiprocessor computing include
symmetric multi processing (SMP) and non-uniform memory architectures
(NUMA).
SMP includes any collection of processors which share uniform access to
memory,
each processor can access any memory location,
with the same or very similar bandwidth and latency as any other processor or
memory location.
Multicore computing is a specific example of SMP.
SMP systems are also fully cache coherent.
NUMA is defined as any system where processor's memory accesses are
non-uniform.
there may be multiple memory areas, some of which a processor cannot access
or can access only with high latency and or low bandwidth.
NUMA systems do not always support cache coherency.
The benefit of NUMA is that it is easier to build large NUMA systems than
large SMP systems.
The drawback is that it is harder to program.
In this dissertation we are only concerned with SMP systems as they are
vastly more common,
and therefore programmers are usually more interested in programming for SMP
systems.

\plan{We need parallelism}
For a long time the software industry enjoyed the exponential growth of
clock speeds.
Programmers did not need to spend any effort in order to use the extra
clock-speed of a newer model processor.
Now that manufacturers add performance by adding parallel processing
features rather than increasing clock speeds,
there is extra pressure on the software industry.
Multicore computing is one of the most powerful features,
however it is arguably the most difficult to use feature;
which is why we choose to concentrate our effort on making it easier to use.

Normally programmers must parallelise their software into multiple
threads of execution which execute in parallel with one another.
This is very difficult in imperative languages as the programmer is
responsible for coordinating the threads.
Few programmers have the skills necessary to accomplish this,
and those that do, still make expensive mistakes as
threaded programming is inherently error prone. 
Bugs such as data corruption, dead locks and race conditions
can be extremely tedious to find and fix.
These bugs and their risks increase the costs of software development.
Software companies who want their software to out-perform their competitors
will usually take on the risks and inherent costs of multicore programming.
We will explain these problems in Section~\ref{sec:intro_concurrency}.

In contrast to parallelism in imperative languages,
it is trivial to express parallelism in pure declarative languages.
However a significant challenge remains,
the programmer must find the parts of their program where parallel execution
is profitable and annotate those parts of their program.
Making a task available to another CPU
may take thousands of instructions,
so spawning off a task that takes only a hundred instructions is clearly a
loss.
Even spawning off a task of a few thousand instructions is not a win;
it should only be done for computations
that take long enough to benefit from parallelism.
\paul{XXX: Still cannot find this citation}
Programmers find these costs very difficult to estimate,
making it difficult for them to determine if parallelisation of
any particular computation is worthwhile.

Even when the programmer finds a computation worth parallelising,
how it is parallelised becomes important.
It is easy to create more parallel tasks than the machine can handle.
While the amount of parallelism the machine can exploit
cannot increase beyond the number of CPUs,
the overheads of parallel execution continue to increase.
This often cripples the performance of such programs.
For example a ray-tracer that creates an image
1,000$\times$1,000 pixels in size has 1,000,000 independent computations
available for parallelisation.
Parallelising all of these on a four processor machine creates too
many overheads, more than would be necessary to parallelise this program
optimally.
This is known as an \emph{embarrassingly parallel} workload.
Programs with this problem often perform \emph{slower} than their sequential
equivalents.
Situations like this can be solved using \emph{granularity control}:
by creating fewer larger (more coarsely-grained) tasks most of the extra
overheads can be avoided.
In the raytracer example, this can be achieved by parallelising the
rendering of whole rows against one another rather than individual pixels.
On top of the programmer's understanding of their program's computations'
costs,
they must also avoid situations with embarrassing parallelism,
in more complicated examples this means keeping track of how many processors
are available at any given point in the program.
When computations are dependent the situation becomes even more complex.

Automatic parallelism promises to remove these problems.
Software companies will not need to hire experts,
programmers will not have to painstakingly find and fix race conditions.
When programmers are using parallelism with pure declarative languages they
will not need to tediously introduce parallelism.
Better yet,
programmers will be able to take advantage of the extra cores on newer
processors without additional effort ---
just as they did when clock speed improvements arrived exponentially.
Furthermore, 
as a program is modified its performance characteristics will change,
it would need to be \emph{re-parallelised}.
% an automatic parallelisation system will be able to trivially
% \emph{re-parallelise} the program.
Re-parallelising a program manually will be a tedious waste of time,
automatic parallelisation can simply be re-executed, saving programmers a
lot of time.

Unfortunately automatic parallelisation technology is yet to be developed to the
point where it is generally useable.
In this dissertation we have solved several critical issues with automatic
parallelism.
We are looking forward to a future where programmers think about
parallelism no more than they currently think about traditional compiler
optimisations;
we believe that our work is a step in the right direction.
Our work is targeted towards Mercury \citep{mercury_jlp},
a purely declarative logic programming language.
Mercury provides a good framework for automatic parallelisation:
Mercury already supports explicit parallelism of dependent conjunctions,
as well as powerful profiling tools which generate data for our profile
feedback analyses.
We believe that our work could be adapted for other systems.

% As processors have changed over the years,
% high level languages have remained the same;
% the same programming paradigms have been applicable to all processors.
% This is no longer true for multicore processors,~%
% \footnote{We show in this thesis that multicore processors do not need to be
% programmed any differently than single-core processors.}
% programmers are expected to supply each processor with its own instruction
% stream.
% This is usually done by programming with \emph{threads}.
% The programmer creates multiple threads of exececution and the operating system
% multiplexes these threads onto processors.
% Threads communicate through shared memory and must synchronise to ensure that
% memory is kept in a consistent state.
% The most common synchronisation method is the use of mutual exclusion locks.
% Locks prevent concurrent access to the same resource by different threads,
% however they pose particular problems: XXX

\input{literature_review}

