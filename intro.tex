%
% vim: ft=tex ts=4 sw=4 
%
\chapter{Introduction}
% XXX
%\pagenumbering{arabic}
%\setcounter{page}{1}
\label{chap:intro}

\status{This chapter is ready for its \textbf{second} review by Zoltan.}

\plan{Why multicore}
In 1965 Moore \citep{moore} predicted that the transistor 
density on processors would grow exponentially with time,
and the manufacturing cost would fall.
The smaller transistors are, the faster they can switch
(given adequate power),
and therefore manufacturers can ship faster processors.
The industry celebrated this trend,
calling it Moore's Law.
However as faster processors require more power,
they create more heat which must be dissipated.
Without novel power saving techniques
(such as \citet{intel-high-k}),
this limits increases of processors' clock speeds.

Around 2005 it became clear that significant improvements in performance
would not come from increased clock speeds but from parallel processing
\citep{free_lunch}.
Manufacturers now build processors that can do more work in parallel.
Multiple processing cores can be placed in the same package,
and usually on the same die.
Individual cores work separately, communicating through the memory
hierarchy.
Multicore computing is only the latest form of multiprocessor computing.

Other methods of improving performance without increasing clock speed have
also been tried.

\begin{itemize}

\item
Modern processors perform super-scalar execution:
processor instructions form a pipeline,
with several instructions at different stages of execution at once,
and by adding extra circuitry, several instructions may be at the same stage
of execution.
However, we have reached the limits of what super-scalar execution can
offer.

\item
Manufacturers have also added Single Instruction Multiple Data (SIMD)
instructions to their processors;
this allows programmers to perform the same operation on multiple pieces of
data.
In practice however, SIMD is useful only in some specific circumstances.

\item
Multicore computing has the potential to be useful in many circumstances,
and does not appear to have limitations that cannot be overcome.
Cache coherency could be a limitation for processors with many cores.
However there are solutions to this such as directory based memory
coherency;
there are also research opportunities such as making compilers responsible
for cache management.

\end{itemize}

%There are a number of ways to achieve this including:
%Single Instruction Multiple Data (SIMD) instruction sets,
%and super-scalar execution such as pipelining and reordering.
%One of the most notable improvements is multicore execution.

\plan{SMP \& programming}
Multicore computing is a special case of multi-processing.
Most multiprocessing systems are symmetric multi-processing (SMP) systems.
An SMP system consists of several homogeneous processors and some memory
connected together.
Usually all processors are equally-distant from any memory location.
Most multicore systems are SMP systems;
they may have more than one CPU each with any number of cores.
Some multi-processing systems use a non-uniform memory architecture (NUMA).
Usually this means that each processor has fast access to some local memory
and slower access to the other processors' memories.
%there may be multiple memory areas, some of which a processor cannot access
%or can access only with high latency and or low bandwidth.
%NUMA systems do not always support cache coherency.
%The benefit of NUMA is that it is easier to build large NUMA systems than
%large SMP systems.
%The drawback is that it is harder to program.
SMP systems are vastly more common, so programmers are usually more
interested in programming for them.
Therefore, In this dissertation we are only concerned with SMP systems.
Our approach will work with NUMA systems, but not optimally.

%\plan{We need parallelism}
%For a long time the software industry enjoyed the exponential growth of
%clock speeds.
%Programmers did not need to spend any effort in order to use the extra
%clock-speed of a newer model processor.
%Now that manufacturers add performance by adding parallel processing
%features rather than increasing clock speeds,
%there is extra pressure on the software industry.
%Multicore computing is one of the most powerful features,
%however it is arguably the most difficult to use feature;
%which is why we choose to concentrate our effort on making it easier to use.

To use a multicore system, or multi-processing in general,
programmers must parallelise their software.
This is normally done by dividing the software into multiple threads of
execution
which execute in parallel with one another.
This is very difficult in imperative languages as the programmer is
responsible for coordinating the threads.
Few programmers have the skills necessary to accomplish this,
and those that do, still make expensive mistakes as
threaded programming is inherently error prone. 
Bugs such as data corruption, deadlocks and race conditions
can be extremely tedious to find and fix.
These bugs increase the costs of software development.
Software companies who want their software to out-perform their competitors
will usually take on the costs of multicore programming.
We will explain the problems with parallelism in imperative languages in
Section~\ref{sec:intro_concurrency}.

In contrast to imperative languages,
it is trivial to express parallelism in pure declarative languages.
expressing this parallelism creates two strongly-related problems.
First,
one must overcome the costs of parallel execution.
For example,
it may take thousands of instructions to make task available for execution on
another processor.
However, if that task only takes hundreds of instructions to execute,
then there is no benefit to executing it in parallel.
Even if the task creates thousands of instructions to execute,
parallel execution is probably not worthwhile.
Most easy-to-exploit parallelism is \emph{fine grained} such as this.
Second,
An abundance of coarse grained parallelism can also be a problem.
Whilst the amount of parallelism the machine can exploit
cannot increase beyond the number of processors;
the more parallel tasks a program creates,
the more parallel execution overheads will have an effect on performance.
In these situations,
there is no benefit in parallelising many of the tasks,
and yet the overheads of their parallel executions will still have an
effect.
This often cripples the performance of such programs.
%For example a ray-tracer that creates an image
%1,000$\times$1,000 pixels in size has 1,000,000 independent computations
%available for parallelisation.
%Parallelising all of these on a four processor machine creates too
%many overheads, more than would be necessary to parallelise this program
%optimally.
This is known as an \emph{embarrassingly parallel} workload.
Programs with either of these problems perform more \emph{slowly} than their
sequential equivalents.
Programmers must therefore find the parts of their program where parallel execution
is profitable and annotate those parts of their program \emph{only},
whilst being careful to avoid embarrassing parallelism.
This means that a programmer must have a strong understanding of their
program's  computations' costs,
how much parallelism is exposed,
and how many processors may be available and any point in the program's
execution.
Programmers are not good at identifying the hotspots in their programs
or understanding the costs of computations,
consequently programmers are not good at manually parallelising programs.

Automatic parallelism aims to make it easy to introduce parallelism.
Software companies will not need to spend as much effort on parallelising
their software.
% Programmers will not have to painstakingly find and fix race conditions.
%When programmers are using parallelism with pure declarative languages they
%will not need to tediously introduce parallelism.
Better yet,
it will be easier for programmers to take advantage of the extra cores on
newer processors.
Furthermore, 
as a parallel program is modified its performance characteristics will
change,
some changes may affect the benefit of the parallelism that has already been
exploited.
In these cases automatic parallelism will make it easy to
\emph{re-parallelise} the program,
saving programmers a lot of time.

\input{literature_review}

\section{General approach}
\label{sec:intro_general_approach}

Unfortunately automatic parallelisation technology is yet to be developed to the
point where it is generally useable.
Our aim is that automatic parallelisation will be easy to use and
will parallelise programs more effectively that most programmers can by
hand.
Most significantly,
automatic parallelism will be very simple to use compared with the
difficulty of manual parallelisation.
Furthermore as programs change,
costs of computations within them will change,
and this may make manual parallelisations (using explicit parallelism) less
effective.
An automatic parallelisation system will therefore make it easier to
maintain programs as the automatic parallelisation analysis can simply be
re-executed to re-parallelise the programs.
We are looking forward to a future where programmers think about
parallelism no more than they currently think about traditional compiler
optimisations.

In this dissertation we have solved several critical issues with automatic
parallelism.
Our work is targeted towards Mercury
We choose to use Mercury because
it already supports explicit parallelism of dependent conjunctions,
and it provides powerful profiling tools which generate data for our profile
feedback analyses.
The parallelisation system described in this dissertation is detailed,
we have paid attention to issues in Mercury's explicit parallelism as well
as automatic parallelism and visualisation of parallel programs' execution
profiles.
We believe that our work could also be adapted for other systems.

