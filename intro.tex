%
% vim: ft=tex ts=4 sw=4 
%

\status{I'm happy with the first three paragraphs.}

Field Effect Transistors (FETs) in particular Metal Oxide Semiconductor FETs
(MOSETs) consist of a channel of silicon between source and drain terminals and
a layer of oxide connected to a gate.
As a potential voltage is applied between the gate and drain creating an
electric field in the oxide
the transistor switches on;
current may flow between the source and drain.
The thiner the layer if oxide the stronger the electric field and the faster
the transistor can switch.
Transistors also switch faster when more voltage is applied
and as the area of the oxide layer increases.
Since it is desirable to make faster and faster processors,
manufacturers are try to make processors smaller and smaller.

In 1965 Moore~\cite{moore} predicted that the transistor 
density would double roughly every two years,
therefore, as transistors became smaller we saw an
exponential growth in the speed of processors.
Until at a 45nm process the width of the oxide
(actually silicon oxide, not metal oxide)
was roughly 1nm thick.
about the size of only five silicon atoms.
At this size quantum tunnelling is problematic and current is leaked from the
gate to the silicon beneath,
wasting power and therefore producing excess heat.
Novel solutions,
such as using hafnium-based compound rather than silicon
oxide~\cite{intel-high-k},
are necessary to manufacture power efficient CPUs on a 45nm process.
Smaller processes are being used, shrinking the area in MOSFETs,
however, the thickness of the oxide layer is not decreasing.
Similarly, as the connections between transistors become thinner,
lower voltages must be used to avoid excess heat.
These two effects mean that transistors cannot run faster as they become
smaller.

To overcome these clock speed limitations manufacturers must build processors
that can do more work concurrently.
There are a number of ways to achieve this including:
SIMD instruction sets,
pipelining,
and executing multiple instructions in a single clock cycle.
One of the most notable of these techniques is multicore execution.
Multiple processing cores can be placed in the same package,
and usually on the same die.
Individual cores work individually, communicating through the memory
hierarchy;
this makes additional cores relatively easy to add to a processor design.
Therefore, this is a popular solution to the physical limitations of
transistors.

\status{The rest of this section is incomplete, I don't even have an
outline}

As processors have changed over the years,
high level languages have remained the same;
the same programming paradigms have been applicable to all processors.
This is no-longer true for multicore processors,~%
\footnote{We show in this thesis that multicore processors do not need to be
programmed any differently than single-core processors.}
programmers are expected to supply each processor with its own instruction
stream.
This is usually done by programming with \emph{threads}.
The programmer creates multiple threads of exececution and the operating system
multiplexes these threads onto processors.
Threads communicate through shared memory and must synchronise to ensure that
memory is kept in a consistent state.
The most common synchronisation method is the use of mutual exclusion locks.
Locks prevent concurrent access to the same resource by different threads,
however they pose particular problems: XXX



However, multicore processors are a significant challenge for programmers.
modern processors have simply been faster.



However, they are difficult to program because the task of scheduling work and
communicating between them is now the responsibility of the programmer.



Automatically parallelising programs is becoming much more desirable.
Parallelisation of programs written in imperative languages is very
difficult.  In contrast, it is trivial to determine if it is safe to
parallelise part of a program written in a pure declarative language.
However, it is difficult to determine if parallelising part of any
program is an optimisation.  Frequently parallelisation can lead to
cases where the overheads of parallel execution outweigh the speedup
that might have been available by parallelising the program.

Mercury --- a purely declarative logic programming language ---
provides a good framework for automatic parallelisation.  Mercury
already supports explicit parallelism of dependent conjunctions, as
well as powerful profiling tools which generate data for my analysis.

When a lot of parallelism is available (in which case the program is said to
be \emph{embarrassingly parallel}), it is easy to parallelise the
program beyond the parallel execution capacity of the computer it is
running on.
While the amount of parallelism the machine can exploit
cannot increase beyond the number of CPUs,
the overheads of parallel execution continue to increase.
This often cripples the performance of such programs.
For example a ray-tracer that creates an image
1,000$\times$1,000 pixels in size has 1,000,000 independent computations
available for parallelisation.
Parallelising all of these on a four processor machine creates too
many overheads, more than would be necessary to parallelise this program
optimally.

In other cases it is difficult for programmers to \emph{judge} whether
a computation is expensive enough to be worth parallelising,
or keep track of how many processors will be available at any given
time during the program's execution.
This makes automatic parallelisation desirable.

I expect that automatic parallelisation will more easily and
effectively parallelise declarative programs than manual
parallelisation.
Furthermore, if a program is modified it's performance characteristics
are also likely to change and it would need to be
\emph{re-parallelised}.
Re-parallelising a program manually will be a tedious waste of time,
automatic parallelisation will save programmers a lot of time in these
situations.

