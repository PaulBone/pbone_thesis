%
% vim: ft=tex ts=4 sw=4 
%
\chapter{Introduction}
\label{chap:intro}

\status{This section is a draft. I will re-read it before I ask for
reviews}
    
\plan{Why multicore}
In 1965 Moore~\citep{moore} predicted that the transistor 
density on processors would double roughly every two years,
and at the same time the manufacturing cost would fall.
This is known as Moore's Law,
it is often misquoted saying that processors' clock speeds would double
roughly every two years.
This is partly due to the strong relationship between transistor density and
clock speeds that existed for about 40 years.
Around 2005 it became clear that this relationship would be broken;
processor clock speeds would no longer continue to increase
exponentially~\cite{free_lunch}.
%One interesting example of this is \citet{intel-high-k},
%which describes how electrons can quantium tunnel through the thinest parts
%of a transistor's gate creating wasted current and excess heat.
%Changing the transistor's materials improved the situation.

To continue to deliver performance without increasing clock speeds
manufacturers now build processors that can do more work in parallel.
There are a number of ways to achieve this including:
Single Instruction Multiple Data (SIMD) instruction sets,
pipelining,
and executing multiple instructions in a single clock cycle.
One of the most notable of these techniques is multicore execution.
Multiple processing cores can be placed in the same package,
and usually on the same die.
Individual cores work separately, communicating through the memory
hierarchy.
Multicore computing is only the latest form of multiprocessor computing.

\plan{We need parallelism}
For a long time the software industry enjoyed the exponential growth of
clock speeds.
Programmers did not need to spend any effort in order to use the extra
clock-speed of a newer model processor.
Now that manufacturers add performance by adding parallel processing
features rather than increasing clock speeds,
there is extra pressure on the software industry.
We will consider one of the most powerful,
and most difficult to use features, multicore execution.

Normally programmers must parallelise their software into multiple
threads of execution which execute in parallel with one another.
This is very difficult in imperative languages as the programmer is
responsible for coordinating the threads.
Few programmers have the skills necessary to accomplish this,
and those that do, still make expensive mistakes as
threaded programming and is inherently error prone. 
Bugs such as data corruption, dead locks and race conditions
can be extremely tedious to find and fix.
We will explain these problems in section \ref{sec:intro_concurrency}.
This increases the costs of software development.
Software companies who want their software to out-perform their competitors
will usually take on the risks and inherent costs of multicore programming.

In contrast to parallelism in imperative languages,
it is trivial to express parallelism in pure declarative languages.
However a significant challenge remains,
the programmer must find the parts of their program where parallel execution
is profitable and annotate those parts of their program.
Making a task available to another CPU
may take thousands of instructions,
so spawning off a task that takes only a hundred instructions is clearly a
loss.
Even spawning off a task of a few thousand instructions is not a win;
it should only be done for computations
that take long enough to benefit from parallelism.
It is often difficult for a programmer to determine if parallelisation of
any particular computation is worthwhile.

Even when the programmer finds a computation worth parallelising,
how it is parallelised becomes important.
It is easy to create more parallel tasks than the machine can handle.
While the amount of parallelism the machine can exploit
cannot increase beyond the number of CPUs,
the overheads of parallel execution continue to increase.
This often cripples the performance of such programs.
For example a ray-tracer that creates an image
1,000$\times$1,000 pixels in size has 1,000,000 independent computations
available for parallelisation.
Parallelising all of these on a four processor machine creates too
many overheads, more than would be necessary to parallelise this program
optimally.
This is known as an \emph{embarrassingly parallel} workload.
Programs with this problem often perform \emph{slower} than their sequential
equivalents.
Situations like this can be solved using \emph{granularity control}:
by creating fewer larger (more coarsely-grained) tasks most of the extra
overheads can be avoided.
In the raytracer example, this can be achieved by parallelising the
rendering of whole rows against one another rather than individual pixels.

All of this requires the programmer to have a strong understanding of the
costs of parallel execution
and the relative costs of their program's computations.
The programmer must also keep track of how many processors are available at
any given point in their program.
Doing all this is infeasible,
and we have not even considered dependent parallelism.

Automatic parallelism promises to remove these problems.
Companies will not need to hire experts,
programmers will not have to painstakingly find and fix race conditions.
When programmers are using parallelism with pure declarative languages they
will not need to tediously introduce parallelism.
Better yet,
programmers will be able to take advantage of the extra cores on newer
processors without additional effort ---
just as they did when clock speed improvements arrived exponentially.
Furthermore, 
as a program is modified its performance characteristics will change,
it would need to be \emph{re-parallelised}.
% an automatic parallelisation system will be able to trivially
% \emph{re-parallelise} the program.
Re-parallelising a program manually will be a tedious waste of time,
automatic parallelisation will save programmers a lot of time in these
situations.

Unfortunately automatic parallelisation technology is yet to be developed to the
point where it is generally useable.
In this dissertation we have solved several critical issues with automatic
parallelism.
We are looking forward to toward a future where programmers think about
parallelism no more than they currently think about traditional compiler
optimisations,
and we think that this work is a step in the right direction.
Our work is targeted towards Mercury~\cite{mercury},
a purely declarative logic programming language.
We believe that our work could be adapted for other systems.
Mercury provides a good framework for automatic parallelisation:
Mercury already supports explicit parallelism of dependent conjunctions,
as well as powerful profiling tools which generate data for our profile
feedback analyses.

In the rest of this chapter we will explore the literature concerning
parallel programming.
We have created four categories in which we group the various parallel
programming systems, including our own.

\begin{description}
    \item[Concurrency] is related to parallelism, however the two concepts
    are orthogonal.
    We will explain this and discuss the systems that use concurrency in
    order to achieve parallelism (\emph{concurrency for parallelism})
    (section \ref{sec:intro_concurrency}).

    \item[Explicit parallelism] is safer and easier to use compared to
    concurrency, however it is less expressive.
    We will explain how explicit parallelism is distinct from and improves
    upon \emph{concurrency for parallelism} in section
    \ref{sec:intro_explicit_par}.

    \item[Implicit parallelism] is a category that includes 
    systems that attempt to make parallel execution the typical form of
    execution.
    We describe a number of systems and explain why the general approach is
    flawed in section \ref{sec:intro_implicit_par}.

    \item[Automatic parallelism] is our last category.
    In these systems sequential execution is the typical form of execution
    and parallel execution is used sparingly.
    Many authors often group these systems with implicit parallelism systems.
    We have chosen to separate them because the authors of the two systems
    approach the same problem from different directions
    (parallel as default vs.\ sequential as default).
    Prior work in automatic parallelism is discussed in section
    \ref{sec:intro_auto_par}.
\end{description}    

\noindent
We explore the literature in this order as each successive category builds
on the work of the previous category.
Our own system is an automatic parallelism system;
it makes use of concepts described in the previous categories.
Therefore
we cannot discuss it meaningfully until we have explored the literature
from giants upon whose shoulders we are standing.

% \plan{SMP \& programming}
% Prior forms of multiprocessor computing include
% symmetric multi processing (SMP) and non-uniform memory architectures
% (NUMA).
% SMP includes any collection of processors which share uniform access to
% memory,
% each processor can access any mamory location,
% with the same or very similar bandwith and latecy as any other processor or
% memory location.
% Multicore computing is included within SMP.
% The definition of SMP also specifies that the machine is fully cache
% coherent.
% NUMA is defined as any system where processor's memory accesses are
% non-uniform.
% there may be multiple memory areas, some of which a processor cannot access
% or can only access with high latency and or low bandwith.
% NUMA systems do not always support cache coherency.
% The benifit of NUMA is that it is easier to build large NUMA systems than
% large SMP systems.
% The drawback is that it is harder to program.
% In this thesis we are only concerned with SMP systems as they are vastly
% more common,
% and therefore programmers are usually more interested in programming for SMP
% systems.
% 
% As processors have changed over the years,
% high level languages have remained the same;
% the same programming paradigms have been applicable to all processors.
% This is no longer true for multicore processors,~%
% \footnote{We show in this thesis that multicore processors do not need to be
% programmed any differently than single-core processors.}
% programmers are expected to supply each processor with its own instruction
% stream.
% This is usually done by programming with \emph{threads}.
% The programmer creates multiple threads of exececution and the operating system
% multiplexes these threads onto processors.
% Threads communicate through shared memory and must synchronise to ensure that
% memory is kept in a consistent state.
% The most common synchronisation method is the use of mutual exclusion locks.
% Locks prevent concurrent access to the same resource by different threads,
% however they pose particular problems: XXX

\input{literature_review}

