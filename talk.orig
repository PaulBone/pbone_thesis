% vim: ts=4 sw=4 expandtab ft=tex

SECTION 1 Automatic Parallelism for Mercury

SUBSECTION Introduction

SLIDE Motivation --- Multicore computing

Computing has traditionally seen a logarithmic increase in CPU clock
speeds.
However, due to phyiscal limitations this trend no-longer continues.

Manufacturers now ship multicore processors to continue to deliver
better-performing processors without increasing clock speeds.

Programmers who want to take advantage of the extra cores on these
processors must write parallel programs.

SLIDE Motivation --- Threaded Programming

Threads are the most common method of parallel programming.
When using threads,
programmers use critical sections to protect shared resources from
concurrent access.

Critical sections are normally protected by locks, but it is easy to make
errors when using locks.

\begin{itemize}

\item Forgetting to use locks can put the program into an inconsistent state,
      corrupt memory and crash the program.

\item Using multiple locks in different orders in different places
      can lead to deadlocks.

\item Critical sections are not composable, nesting critical sections
      may aquire locks in different orders in different places.

\item Misplacing lock operations can lead to critical sections
      that are too wide (causing poor performance)
      or too narrow (causing data corruption and crashes).

\end{itemize}

SLIDE Automatic Parallelism.

An good compiler performs many optimizations on behalf of the
programmer.
Programmers rarely think about:

\begin{itemize}

    \item register allocation,

    \item inlining,

    \item simplification such as constant propagation \& strength
          reduction.

\end{itemize}

We believe that parallelisation is just another optimization,
and it would be best if the compiler handled it for us;
so that, like any other optimization, we wouldn't need to think of it.

SLIDE About Mercury

\begin{itemize}
\item Mercury is a \textbf{pure} logic/functional language
designed to support the creation of large, reliable, efficient programs.

\item It has a syntax similar to Prolog's,
    however the operational semantics are very different.

\item It is strongly typed using a Hindley Milner type system.

\item It also has mode and determinism systems.
      % making many bugs impossible.
\end{itemize}

\begin{verbatim}
:- pred map(pred(T, U), list(T), list(U)).
:- mode map(pred(in, out) is det, in, out) is det.

map(_, [], []).
map(P, [X | Xs], [Y, Ys]) :-
    P(X, Y),
    map(P, Xs, Ys).
\end{verbatim}

SLIDE Effects in Mercury

In Mercury, all effects are explicit,
which helps programmers as well as the compiler.

\begin{verbatim}
main(IO0, IO) :-
    write_string("Hello ", IO0, IO1),
    write_string("world!\n", IO1, IO).
\end{verbatim}

The I/O state represents
the state of the world outside of this process.
Mercury ensures that only one version
is alive at any given time.

This program has three versions of that state:
\begin{description}
\item[\code{IO0}]
represents the state before the program is run
\item[\code{IO1}]
represents the state after printing \verb|Hello |
\item[\code{IO}]
represents the state after printing \verb|world!\n|.
\end{description}

SLIDE Data dependencies

\begin{verbatim}
qsort([], []).
qsort([Pivot | Tail], Sorted) :-
    partition(Pivot, Tail, Bigs0, Smalls0),     %1
    qsort(Bigs0, Bigs),                         %2
    qsort(Smalls0, Smalls),                     %3
    Sorted = Smalls ++ [Pivot | Bigs].          %4
\end{verbatim}

\parbox{0.5\textwidth}{
\epsfbox{dep_graph.eps}
}~\parbox{0.5\textwidth}{
\begin{itemize}
  \item Steps 2 and 3 are independent.
  \item \red{This is easy to prove because
  there are never any \emph{side} effects.}
  \item They may be executed in parallel.
\end{itemize}
}

SLIDE Explicit Parallelism

Mercury allows explicit, deterministic parallelism
via the parallel conjunction operator \red{\&}.

\vspace{1em}

\begin{tabular}{l}
\code{qsort([], []).} \\
\code{qsort([Pivot $|$ Tail], Sorted) :-} \\
\code{~~~~partition(Pivot, Tail, Bigs0, Smalls0), } \\
\code{~~~~(} \\
\code{~~~~~~~~qsort(Bigs0, Bigs) } \\
\code{~~~~\red{\&}} \\
\code{~~~~~~~~qsort(Smalls0, Smalls)} \\
\code{~~~~),} \\
\code{~~~~Sorted = Smalls ++ [Pivot $|$ Bigs].}
\end{tabular}

SLIDE Why make this automatic?

We might expect parallelism to yield a speedup in the quicksort example,
but it does not.

The above parallelization creates $N$ parallel tasks for a list of length $N$.
Most of these tasks are trivial and the overheads of managing them
slow the program down.

Programmers rarely understand the performance of their programs,
even when they think they do.

SUBSECTION Runtime system changes

SLIDE Runtime system changes

Before we can automatically parallelise programs effectively we need to
be able to manually parallelise them effectively.
This meant making several improvements to the runtime system.

The RTS has several objects used in parallel Mercury programs.

\begin{description}

    \item[Engines] represent abstract CPUs, the RTS will create as many
    engines as there are processors in the system, and control each one
    from a POSIX Thread.

    \item[Contexts] represent a computation in progress,
    they contain the stacks for that computation,
    and a copy of the engine's registers when the context is suspended.

    \item[Sparks] are a very small structure representing a computation
    that has not yet been started,
    and therefore has no allocated stack space.

\end{description}

SLIDE Work stealing

Peter Wang introduced sparks and a partial work stealing implementation.

Work stealing reduces contention on a global queue of work by allowing
each context to maintain its own work stack.
Contexts can:

\begin{itemize}

    \item Push a spark onto their own stack.

    \item Pop a spark off their own stack.
    
    \item Steal a spark from the cold end of another's stack.

\end{itemize}

All of these operations are lock free,
the first two operations are wait free and do not use any atomic
operations.
The stealing operation uses an atomic compare-and-swap that may
busy-wait.

Credit: 80\% Peter Wang, 20\% myself, excluding the queue data structure.

SLIDE Dependent Parallelism

Mercury can handle dependencies between parallel conjuncts.
\emph{Shared variables} are produced in one conjunction and consumed in
another.

\begin{tabular}{l}
\code{map\_foldl(\_, \_, [], Acc, Acc).} \\
\code{map\_foldl(M, F, [X $|$ Xs], Acc0, Acc) :-} \\
\code{~~~~(} \\
\code{~~~~~~~~M(X, Y),} \\
\code{~~~~~~~~F(Y, Acc0, \red{Acc1})} \\
\code{~~~~) \&} \\
\code{~~~~map\_foldl(M, F, Xs, \red{Acc1}).} \\
\end{tabular}

\code{\red{Acc1}} will be replaced with a \emph{future},
If the second conjunct attempts to read from the future before the first
conjunct writes the future,
its context will be blocked and resumed once the first conjunct has placed a
value into the future.

SLIDE Right-recursive parallel code

Mode correctness requires that all producers of variables occur before
consumers in conjunctions.

Programmers are encouraged to make their code tail-recursive.
This means that the recursive call is placed lasted in a conjunction so that it
can become a tail call.

A parallel conjunction $G_1~\&~G_2~\&~\ldots~\&~G_N$ will be executed by spawning
off $G_2~\&~\ldots~\&~G_N$, then executing $G_1$ immediatly.
In the common case that the forked-off task is not taken up by another engine
then,
a dependency between the tasks would not require a context switch.

However, if the forked-off task was taken by another engine,
the original context must be suspended until that task completes.
When the last conjunct is a tail call, it often takes far longer to execute than the other conjuncts.
Causing the original context to be blocked for a long time.

SLIDE Decomposing a parallel conjunction

Psudeo compiler output:

\code{case\_label:} \\
\code{~~~~SyncTerm st;} \\
\code{~~~~\red{init\_sync\_term}(\&st);} \\
\code{~~~~\red{spawn\_off}(spawn\_off\_label, \&st);} \\
\code{~~~~M(X, Y);} \\
\code{~~~~F(Y, Acc0, Acc1);} \\
\code{~~~~\red{join\_and\_continue}(resume\_label, \&st);} \\
\code{spawn\_off\_label:} \\
\code{~~~~map\_foldl(M, F, Xs, Acc1, Acc);} \\
\code{~~~~\red{join\_and\_continue}(resume\_label, \&st);} \\
\code{resume\_label:} \\
\code{~~~~return;} \\

SLIDE Execution of right-recursive parallel code

\vspace{-2mm}
Blocking the original context can create a pathological worst-case behaviour:
the same behavior will occur at each level of recursion.

This will cause it to use a number of contexts \textbf{linear} in the depth of
the recursion.

\vspace{-2mm}
\begin{center}
\epsfbox{linear_context_usage.eps}
\end{center}
\vspace{-4mm}

If each context contains 4MB of stack space,
a loop only of 256 iterations will consume 1GB!

SLIDE Workarround --- Reorder conjuncts

The compiler understands recursion including mutual recursion.
Therefore,
it can move conjuncts with recursive calls to the left of those without.
This avoids the problem above.

However, where dependencies exist conjuncts cannot be moved without violating
the program's mode-correctness.
Therefore this solution only works in the rare cases that
parallel conjunctions are independant.

SLIDE Workarround --- The max-contexts limit

To reduce the impact of this,
Peter added a maximum limit on the number of contexts that could be in memory
at the same time.

When the limit is reached no context can be created to handle
the spawned off computation.

This limit trades memory usage for sequential execution.

SLIDE Management of work queues

Work queues where originally owned by contexts,
if there where 100's of contexts active then there where 100's of work queues
to steal work from, most of which are usually empty.
This makes work stealing slower.

Futhermore, as contexts are created and destoryed the number of work queues
changes.
This required a global lock to manage the set of work queues,

Work queues are now owned by engines, which means that there are a small
fixed number of queues.
Greatly simplifying the work stealing code.

More importantly it is faster in the pathalogical right-recursive case.

SLIDE Old engine wakeup code

When a new context is created or an existing one becomes runnable and an engineV
is sleeping,
the engine is woken up.

If the context must be executed on a particular engine
(because foreign code needs to use the C-stack owned by that engine)
then all the engines are woken up so that the correct one can execute the
context.

Engines sleep using a posix condition variable associated with the runqueue lock.
Sleeping engines wakeup periodically to attempt to steal sparks.

SLIDE New engine wakeup code

In the new system each engine has a semaphore,
Engines wait on the semaphore when they are idle.

When a context becomes runnable and an engine is sleeping the runtime system
wakes the enigne by pusting to the semaphore.
This prevents races that could occur in the previous system.

Individual engines can be targeted specifically,
so if a context has only one valid engine,
then only that engine will be woken.

We can also pass contexts directly to engines,
avoiding the runqueue and synchronization in many cases.

When a spark is created a sleeping engine is woken and told which spark queue contains
a spark it can execute.

The hew code is much more responsive.

SUBSECTION Automatic parallelism

SLIDE Our approach

\begin{itemize}
  \item Profile the program to find the expensive parts.
  \item Analyse the program to determine what parts \emph{can} be run in
        parallel.
  \item Select only the parts that can be parallelized \emph{profitably}.
        This may involve trial and error when done by hand.
  % Originally the above item was:
  %\item Determine which parts it is \emph{profitable} run in parallel.
  %      This may involve trial and error when done by hand.
  \item Continue introducing parallel evaluation until the all processors are
        fully utilised or there is no profitable parallelism left.
\end{itemize}

\begin{center}
\epsfbox{prof_fb.eps}
\end{center}

SLIDE Finding parallelization candidates

The deep profiler's call graph is a tree of strongly connected components
(SCCs).
Each SCC is a group of mutually recursive calls.
The automatic parallelism analysis follows the following algorithm:

\begin{itemize}

    \item Recurse depth-first down the call graph from \code{main/2}.

    \item Analyze each procedure of each SCC, identify conjunctions that have
          two or more goals whose cost is greater than a configurable
          threshold.

    \item Stop recursing into children if either:

    \begin{itemize}

        \item the child's cost is below another configurable threshold; or

        \item there is no free processor to exploit any parallelism the child
              may have.

    \end{itemize}

\end{itemize}

SLIDE Recursive calls

The deep profiler records profiling data not just for a call,
for for a call-chain.
The chain is an alternating list of call, call-site, call, call-site entries.

\vspace{-2mm}
\begin{center}
\epsfbox{call_tree.eps}
\end{center}
\vspace{-2mm}

The two calls to $f$ are recorded seperatly as the first one is:
$main~\calls~g~\calls~map~\calls~f$
and the second one is:
$main~\calls~map~\calls~f$

This even works with higher order calls such as the one in map.

SLIDE Recursive calls

Because recursive calls are included in this chain,
multiple calls to the same procedure from a single procedure will have their
profiling data recorded seperatly.

\vspace{-2mm}
\begin{center}
\epsfbox{call_tree2.eps}
\end{center}

If $g$ contains two call sites that both call $f$,
then these also record seperate profiling data for $f$ and its descendants.


SUBSECTION Overlap analysis

SLIDE Calculating How would you parallelize this?

XXX: I've given the solution to this problem above, but I want to
discuss it anyway so I need to re-word it.

%Assume that the call to \code{M} is expensive and the call to \code{F} is
%cheap.

\begin{verbatim}
map_foldl(_, _, [], Acc, Acc).
map_foldl(M, F, [X | Xs], Acc0, Acc) :-
    M(X, Y),
    F(Y, Acc0, Acc1),
    map_foldl(M, F, Xs, Acc1, Acc).
\end{verbatim}

During parallel execution,
a task will block if a variable it needs
is not available when it needs it.

\code{F} needs \code{Y} from \code{M},
and the recursive call needs \code{Acc1} from \code{F}.

Can \code{map\_foldl} be profitably parallelized
despite these dependencies, and if yes, how?

SLIDE Parallelizing \code{map\_foldl}

\code{Y} is produced at the very end of \code{M}
and consumed at the very start of \code{F},
so the execution of these two calls cannot overlap.

\code{Acc1} is produced at the end of \code{F},
but it is \emph{not} consumed at the start of the recursive call,
so some overlap \emph{is} possible.

\begin{verbatim}
map_foldl(_, _, [], Acc, Acc).
map_foldl(M, F, [X | Xs], Acc0, Acc) :-
    (
        M(X, Y),
        F(Y, Acc0, Acc1)
    ) &
    map_foldl(M, F, Xs, Acc1, Acc).
\end{verbatim}

SLIDE \code{map\_foldl} overlap

The recursive call needs \code{Acc1} only when it calls \code{F}.
The calls to \code{M} can be executed in parallel.

\vspace{1em}
\epsfbox{mapfoldl-overlap.eps}

% SLIDE \code{map\_foldl} overlap
% 
% The more expensive \code{M} is relative to \code{F},
% the bigger the overall speedup.
% 
% \vspace{1em}
% \epsfbox{mapfoldl-overlap2.eps}

SLIDE Simple overlap example

\vspace{-1em}

Tasks \code{p} and \code{q} have one shared variable.

We conceptually split each task split into sections,
each section ended by the production or consumption of a shared variable.

$pR$ and $qR$ denote the time that the tasks take to compute any non-shared
variables needed after the conjunction.

\vspace{1em}
\epsfbox{overlap1.eps}

SLIDE Overlap with more than one dependency

We calculate the execution time of \code{q} by iterating over its sections.
In this case, that means iterating over the variables it consumes
in the order that it consumes them.

SLIDE Overlap algorithm --- Loop over variables

\begin{verbatim}
find_conjunct_par_time(Conj, SeqTime,
    inout CurParTime, inout ProdTimeMap):
  ProdConsList := get_sorted_var_uses(Conj)
  CurSeqTime := 0
  forall (Var_j, Time_j) in ProdConsList:
    Duration_j := Time_j - CurSeqTime
    CurSeqTime := CurSeqTime + Duration_j
    if Conj produces Var_j:
      CurParTime := CurParTime + Duration_j + ...
      ProdTimeMap[Var_j] := CurParTime
    else Conj must consume Var_j:
      ParWantTime := CurParTime + Duration_j + ...
      CurParTime := max(ParWantTime, ProdTimeMap[Var_j]) + ...
  DurationRest := SeqTime - CurSeqTime
  CurParTime := CurParTime + DurationRest
\end{verbatim}

\vspace{2em}
\epsfbox{overlap2-swap.eps}

SLIDE Overlap of more than two tasks

A task that consumes a variable can occur only on the \emph{right}
of the task that generates its value.
Therefore, we process conjuncts from left to right.

\vspace{1em}
\epsfbox{overlap3.eps}

SLIDE Overlap algorithm --- Loop over conjuncts

\begin{verbatim}
find_par_time(Conjs, SeqTimes) returns TotalParTime:
  N := length(Conjs)
  ProdTimeMap := empty
  TotalParTime := 0
  for i in 1 to N:
    CurParTime := 0 + ...
    find_conjunct_par_time(Conjs[i], SeqTimes[i],
      CurParTime, ProdTimeMap)
    TotalParTime := max(TotalParTime, CurParTime)
  TotalParTime := TotalParTime + ...
\end{verbatim}

The \verb|...|s represent estimates of overheads.

SUBSECTION Choosing how to parallelize

SLIDE Choosing how to parallelize

\begin{verbatim}
g1, g2, g3
g1 & (g2, g3)
(g1, g2) & g3
g1 & g2 & g3
\end{verbatim}

Each of these is a parallel conjunction of sequential conjunctions,
with some of the conjunctions having only one conjunct.

If there is a \code{g4}, you can
(a) execute it in parallel with all the other parallel conjuncts, or
(b) execute it in sequence with the goals in the last sequential
conjunction.

There are thus $2^{N-1}$ ways
to parallelize a conjunction of $N$ goals.

If you allow goals to be reordered,
the search space would become larger still.

SLIDE Even simple code can have many conjuncts.

\begin{verbatim}
X = (-B + sqrt(pow(B, 2) - 4*A*C)) / 2 * A
\end{verbatim}

Flattening the above expression gives 12 small goals,
each executing one primitive operation:

\begin{verbatim}
V1 = 0          V5 = 4          V9 = sqrt(V8)
V2 = V1 - B     V6 = V5 * A     V10 = V2 + V9
V3 = 2          V7 = V6 * C     V11 = V3 * A
V4 = pow(B, V3) V8 = V4 - V7    X = V9 / V11
\end{verbatim}

Primitive goals are not worth spawning off.
Nonetheless, they can appear between goals
that should be parallelized against one another,
greatly increasing the value of $N$.

SLIDE Reducing the search space.

Currently we do two things to reduce the size of the search space
from $2^{N - 1}$:

\begin{itemize}
   \item Remove whole subtrees of the search tree that are worse than
     the current best solution (a variant of ``branch and bound'').

   \item During search we always follow the most promising-looking
     branch before backtracking to the alternative branch.

   \item If the search is still taking to long,
     then switch to a greedy search that is approximately linear.
\end{itemize}

This allows us to fully explore the search space when it is small,
while saving time by exploring only part of the search space when it is large.

SUBSECTION Push-into-goal transformation

SLIDE Expensive goals in different conjunctions

The call to \code{typecheck} and the call to \testtt{typecheck\_preds} are
expensive enough to be worth parallelizing.
But the if-then-else that contains the call to \code{typecheck}
has a typical cost 1/10th of the cost of \code{typecheck}.
It is not worth parallelizing the if-then-else against \code{typecheck\_preds}.

\small
\begin{verbatim}
   typecheck_preds([], [], ...).
   typecheck_preds([Pred0 | Preds0], [Pred | Preds], ...) :-
       ( if should_typecheck(Pred0) then
 10        typecheck(Pred0, Pred, ...)
       else
 90        Pred = Pred0
       ),
100    typecheck_preds(Preds0, Preds, ...).
\end{verbatim}
\normalsize

SLIDE Push later goals into earlier compound goals

We can push the call to \code{typecheck\_preds} into the if-then-else
and parallelize only the then-part:

\small
\begin{verbatim}
typecheck_preds([], [], ...).
typecheck_preds([Pred0 | Preds0], [Pred | Preds], ...) :-
    ( if should_typecheck(Pred0) then
        typecheck(Pred0, Pred, ...) &
        typecheck_preds(Preds0, Preds, ...)
    else
        Pred = Pred0,
        typecheck_preds(Preds0, Preds, ...)
    ).
\end{verbatim}
\normalsize

Our analysis can perform this transformation
as part of deciding whether this parallelization is worthwhile.

SLIDE Results on some small programs

\scalebox{0.9}
{
\begin{center}
\begin{tabular}{lrrrrrr}
\hline \hline
\multicolumn{1}{c}{\textbf{Program}} &
\multicolumn{1}{c}{\textbf{Seq}}    &
\multicolumn{1}{c}{\textbf{1 CPU}}   &
\multicolumn{1}{c}{\textbf{2 CPUs}}  &
\multicolumn{1}{c}{\textbf{3 CPUs}}  &
\multicolumn{1}{c}{\textbf{4 CPUs}}  \\
\hline
%\hhline{|-|-||-|-|-|-|-|}
% \hline
% matrixmult & indep & 14.7 (0.76) &  7.6 (1.47) &  5.2 (2.15) &  5.2 (2.15) \\
% seq 11.2   & naive & 14.7 (0.76) &  8.0 (1.40) &  5.7 (1.96) &  4.7 (2.38) \\
% par 14.6   & overlap  & 14.7 (0.76) &  7.6 (1.47) &  6.7 (1.67) &  5.2 (2.15)
% \\
matrixmult & 11.0     & 14.6 (0.75) &  7.5 (1.47) &  6.2 (1.83) &  5.2 (2.12)
\\
%\hhline{|-|-||-|-|-|-|-|}
%\hhline{|-|-||-|-|-|-|-|}
\hline
raytracer  & 22.7     & 25.1 (0.90) & 16.0 (1.42) & 11.2 (2.03) &  9.4 (2.42)
\\
\hline
mandelbrot & 33.4     & 35.6 (0.94) & 17.9 (1.87) & 12.1 (2.76) &  9.1 (3.67)
\\
%\hhline{|-|-||-|-|-|-|-|}
\hline \hline
\end{tabular}
\end{center}
}
\vspace{3mm}

Parallel code needs to use a machine register
to point to thread-specific data,
so enabling parallel execution but not using it leads to slowdowns.

Matrixmult has one memory store for each FP multiply/add pair.
Its speedup is limited by memory bus bandwidth,
which it saturates relatively quickly.

Raytracer generates many intermediate data structures.
The GC system consumes 40\% of the execution time
in stop-the-world collections when using 4 Mercury threads and 4 GC
threads.
When using 1 Mercury thread and 4 GC threads it uses only 5\% of the
program's runtime.

SLIDE Results on the Mercury compiler

\begin{itemize}
    \item There are 53 conjunctions in the compiler with two or more
          expensive conjuncts.

    \item 52 of these are dependent conjunctions.

    \item 31 of these have a predicted speedup of greater than 1\% (the
          default speedup threshold).

    \item Therefore, our analysis can prevent the parallelization of 22
          conjunctions that are not profitable.
\end{itemize}

Unfortunately, many parts of the compiler must be executed sequentially.
Due to Amdahl's law, this limits the overall speedup of the compiler.

SUBSECTION Loop control

SLIDE XXX

Programmers are encouraged to write tail recursive code.
In Mercury,
this means that the last call in a clause is often a recursive call.

Mercury's mode system restricts the order of conjunctions:
produces of varizbles (such as \code{Acc1}) must occur before consumers.
Therefore,


SUBSECTION Conclusion

SLIDE Progress to date

\vspace{-0.5em}

\begin{itemize}
  \item Our analysis is able to find profitable parallelism in small
        programs and generate advice for the compiler.

  \item The analysis explores only the parts of the call graph that
        might be profitably parallelized.

  \item Our novel overlap analysis allows us to estimate how dependencies
        affect parallel execution.

  \item The compiler can act on this advice, and can profitably parallelize
        small programs.
\end{itemize}

Not shown in this presentation:

\begin{itemize}

  \item We can rearrange some computations to make it easier to take
        advantage of parallelism.

  \item We can efficiently search a large space of possible
        parallelizations.

\end{itemize}


