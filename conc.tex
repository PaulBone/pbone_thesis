
\chapter{Conclusion}
\label{chap:conc}

% Each research chapter contains a discussion of related work.
\status{No plan for this chapter yet}

I guess in this chapter I have to unify the contributions of the other
chapters.
I think that perhaps this chapter could be used to relate the contribution
of the thesis to the rest of the world, providing statements about future
work, and applicability of this work to other languages and systems.

\paul{How do you finish a thesis?}

\section{Further work}
\label{sec:further_work}

\paul{Exceptions}

\paul{Tie auto parallelism analysis to different recursion types and their
special case handling such as loop control}

\paul{More special case handling such as D\&C}

\paul{Throttle recursive parallelism based on the cost of each iteration.}
\paul{Handle other recursion times intelligently.}

\paul{XXX: Future work}
As we stated above we do not currently handle recursive code specially other
than putting in effort to calculate the cost of recursive calls.
We assume that the recursive calls themselves call the sequential version of
the procedure.
A simple but imperfect solution for simply recursive code would be to
calculate the \emph{time saving} of one iteration such as in the example
above.
Time saving is defined as:

\begin{equation*}
TimeSaving = SeqTime - ParTime
\end{equation*}

\noindent
Time saving is the benefit of parallelism in absolute terms
the related concept of \emph{speedup} which is the benefit of parallelism in
relative terms.
If we multiply time saving by the average maximum depth (number of
iterations) this would give an indication of the amount of time we could
save given an infinite number of CPUs.
A better solution would be to construct and solve an equation for the
speedup of the loop as a whole.
The equation could be built using the overlap algorithms that we have shown.

\paul{
    Add something about updating overlap's cost model, then we need to
    re-evaluate it.}


We believe that more can be done to improve the effectiveness of
automatic parallelisation,
In some cases it may be possible to transform common programming
pasterns that lack parallelism into equivalent patterns with available
parallelism.
Furthermore, an advanced profiler --- such as Mercury's deep
profiler~\citep{conway:2001:mercury-deep} --- can provide information
that enables a compiler to make good parallelisation choices.
However there are a number of challenges facing automatic
parallelisation.
When using profiler feedback the profiled execution of the program may
not be a typical execution of the program, or there may be several
typical executions of the program.
We cannot control whether users profile typical executions of their
programs, but we may be able to allow users to merge execution
profiles to create a composite profile that is more representative of
their program's usage.
Another challenge can occur when a program has very little parallelism
available in it, it may be difficult to parallelise effectively.
We hope that in some cases the compiler can transform such a program
into an equivalent program with more available parallelism.

% XXX For conclusion?
%Mercury's deep profiler~\citep{conway:2001:mercury-deep} provides
%detailed and accurate profiling information,
%This will make it easier to implement optimisations such as
%parallel specialisation --- generating sequential and parallel
%versions of one procedure and using the sequential version
%in situations where parallelism is not an optimisation.
%Extracting information from the deep profiler to guide compiler
%optimisations is supported by the feedback framework developed by
%\citet{bone:2008:hons}.
%These are examples of the flexibility that the deep profiler provides,
%describing other ideas is outside the context of a literature review.
%
