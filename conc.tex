
\chapter{Conclusion}
\label{chap:conc}

% Each research chapter contains a discussion of related work.
\status{No plan for this chapter yet}

Summarise contributions
=======================

    Chapter 3 implemented many ideas invented elsewhere in the Mercury
    runtime system, such as work stealing.
    It also discusses a number of implementation improvements that make
    parallel execution more efficient.
    The Mercury runtime system provides a unique set of challenges, for
    example dependent AND-parallelism constrains the order that sparks must
    be evaluated in, making the implementation of work stealing non-trivial.
    This adds to the support for techniques such as work stealing.

    Chapter 4 describes a unqiue way to use profiling data to find
    profitable dependent parallelism.
    Chapter 4 makes two novel contributions:
        1. Overlap calculation.
        2. Search for \emph{how} to parallelise a conjunction.
    Chapter 4 also describes our \emph{practical} implementation
    of these ideas.

    Chapter 5 both:
        1. Fixes a long standing problem with parallel execution of most
        loops in Mercury.
        2. Provides a special case optimisation for a common programming
        pattern.

    Chapter 6 shows how we can take advantage of the similarities between
    Mercury ahd GHC's runtime systems to use the \tscope visual profiler.
    (Most of the similarity is accidental, eg both systems use strong
    techniques such as lightweight threads and work stealing.)

Further work
============

We have already discussed some further work within each of the four
research chapters.
Here I will discuss further work combining each of the chapters.

    1. Update the overlap analysis to be aware of loop control and use an
       appropriate cost model for loop-controlled code.
       (We already discussed that it should have better handling for loops
       in general.)

       Do this for more recursion types in the future.  Including
       loop-control-ish special casing and correct analysis of which special
       case should be used and its cost model.  Eg: D\&C.

    2. Throttling 
       \paul{Throttle recursive parallelism based on the cost of each iteration.}

    3. Profile merging, parallelisation as a specialisation and transforming
       code to make it easier to parallelise (reordering and OISU) ---
       if I have not discussed these already in Ch4.

    4. The \tscope work has no concept of loop control.  So it cannot
       properly generate profiles of loop controlled code.

    5. \tscope should be used to feedback more information to the automatic
       parallelism analysis.
       This includes information about how many CSCs occur in one second,
       the real costs of parallel overheads, (How many CSCs does it take to
       spawn a conjunct etc),
       and feedback on the actual costs of computations.

    6. We do not handle exceptions.

Related work
============

I have already discussed each chapter's related work.
I am not sure there is anything left to discuss here as I am not aware of any
comparable whole systems (which is what I have created).

I should probably conclude with statements about how my work (the whole
thesis) fits into the rest of the world.
Perhaps I should revisit my statement that I hope parallelism is one day
``just another optimisation''.


\section{Further work}
\label{sec:further_work}

\paul{XXX: Future work}
As we stated above we do not currently handle recursive code specially other
than putting in effort to calculate the cost of recursive calls.
We assume that the recursive calls themselves call the sequential version of
the procedure.
A simple but imperfect solution for simply recursive code would be to
calculate the \emph{time saving} of one iteration such as in the example
above.
Time saving is defined as:

\begin{equation*}
TimeSaving = SeqTime - ParTime
\end{equation*}

\noindent
Time saving is the benefit of parallelism in absolute terms
the related concept of \emph{speedup} which is the benefit of parallelism in
relative terms.
If we multiply time saving by the average maximum depth (number of
iterations) this would give an indication of the amount of time we could
save given an infinite number of CPUs.
A better solution would be to construct and solve an equation for the
speedup of the loop as a whole.
The equation could be built using the overlap algorithms that we have shown.

\paul{
    Add something about updating overlap's cost model, then we need to
    re-evaluate it.}


We believe that more can be done to improve the effectiveness of
automatic parallelisation,
In some cases it may be possible to transform common programming
pasterns that lack parallelism into equivalent patterns with available
parallelism.
Furthermore, an advanced profiler --- such as Mercury's deep
profiler~\citep{conway:2001:mercury-deep} --- can provide information
that enables a compiler to make good parallelisation choices.
However there are a number of challenges facing automatic
parallelisation.
When using profiler feedback the profiled execution of the program may
not be a typical execution of the program, or there may be several
typical executions of the program.
We cannot control whether users profile typical executions of their
programs, but we may be able to allow users to merge execution
profiles to create a composite profile that is more representative of
their program's usage.
Another challenge can occur when a program has very little parallelism
available in it, it may be difficult to parallelise effectively.
We hope that in some cases the compiler can transform such a program
into an equivalent program with more available parallelism.

% XXX For conclusion?
%Mercury's deep profiler~\citep{conway:2001:mercury-deep} provides
%detailed and accurate profiling information,
%This will make it easier to implement optimisations such as
%parallel specialisation --- generating sequential and parallel
%versions of one procedure and using the sequential version
%in situations where parallelism is not an optimisation.
%Extracting information from the deep profiler to guide compiler
%optimisations is supported by the feedback framework developed by
%\citet{bone:2008:hons}.
%These are examples of the flexibility that the deep profiler provides,
%describing other ideas is outside the context of a literature review.
%
