
\status{This section is ready for reviewing.}

One of the sources of poor parallel performance is the behaviour of the
garbage collector.
Like other pure declarative languages,
Mercury does not allow destructive update.
%\footnote{
%    Destructive update is allowed via support for mutable variables.
%    Their use does not interfere with parallelism as they are used in
%    conjunction with impurity.
%    The compiler will not parallelise impure goals.}
Therefore a call usually returns its value in newly allocated memory
rather than modifying the memory of its parameters.
Likewise, a call cannot modify global memory.
This means that Mercury programs often have a high rate of allocation,
which places significant stress on the garbage collector.
Therefore,
allocation and garbage collection can reduce a program's
performance.
This usually becomes more significant when parallelism is introduced into a
program.

\plan{Introduce Boehm, \& details: conservative, mark and sweep, stop the
world, parallel marking.}
Mercury uses the Boehm-Demers-Weiser conservative garbage collector (Boehm GC)
\citep{boehm_gc},
which is a conservative mark and sweep collector.
Boehm GC supports parallel programming:
it will stop all the program's threads (\emph{stop the world}) during its
marking phase.
It also supports parallel marking:
it will use its own set of pthreads to do parallel marking.

\plan{Introduce collector time, mutator time.}
For the purposes of this section
we separate the program's execution time into two alternating phases:
Collection time, which is when Boehm GC performs marking,
and mutator time, which is when the Mercury program runs.
The name `mutator time' refers to time that mutations (changes) to memory
structures are permitted.
The collector may also perform some actions,
such as sweeping,
concurrently with the mutator.

\plan{Describe theory of GC performance.}
Amdahl's law~\citep{amdahl} describes the maximum speedup that
can theoretically be achieved by parallelising a part of a program.
We use Amdahl's law to predict the speedup of a program whose
mutator is parallelised but whose collector runs sequentially.
Consider a program with a runtime of 20 seconds
which can be separated into one second of collector time and 19 seconds
of mutator time.
The sequential execution time of the program is $1 + 19 = 20$.
If we parallelise the mutator and do not parallelise the
collector then the minimum parallel execution time is $1 + 19/P$
for $P$ processors.
Using four processors the theoretical best speedup is:
%\paul{I prefer how \\frac displays maths but in text the fonts become tiny.}
$(1 + 19) / (1 + 19/4) = 3.48$.
17\% of the parallel runtime ($1 + 19/4$) is collector time.
If we use a machine with 100 processors then this becomes:
$(1 + 19) / (1 + 19/100) = 16.8$;
with 84\% of the runtime spent in the collector.
As the number of processors increases,
the mutator threads spend a larger proportion of their time waiting for
collection to complete.

\plan{Discuss predictions regarding parallel marking, locking and thread
local heaps.}
To reduce this problem,
\citet{boehm-gc} included parallel marking support in their collector.
Ideally this would remove the bottleneck described above.
However,
parallel garbage collection is a continuing area of research and
the Boehm GC project has only modest multicore scalability goals.
Therefore,
we expect parallel marking to only partially reduce the bottleneck,
rather than remove it completely.

Furthermore, thread safety has two significant performance costs;
These types of costs prevent us from achieving the theoretical maximum
speedups that Amdahl's law predicts.
The first of these is that
one less CPU register is available to GCC's code generator when compiling
parallel Mercury programs (Section \ref{sec:backgnd_merpar}).
The second is that
memory allocation routines must use locking to protect shared data
structures,
which will slow down allocation.
Boehm GC's authors recognised this problem and
added support for thread-local resources such as free lists.
Therefore,
during memory allocation,
a thread uses its own free lists rather than locking a global structure.
From time to time, a thread will have to retrieve new free lists
from a global structure and will need to lock the structure.
The costs of locking will be amortised across several memory allocations.

\plan{Describe icfp2000, mandelbrot\_lowalloc and mandelbrot\_highalloc
programs.}
To how well Mercury and Boehm GC scale to multiple cores we used several
benchmark programs with different memory allocation requirements.
Our first benchmark is a raytracer developed for the
ICFP programming contest in 2000.
For each pixel in the image,
the raytracer casts a ray into the scene to determine what colour to paint that pixel.
Two nested loops build the pixels for the image:
the outer loop iterates over the rows in the image and
the inner loop iterates over the pixels in each row.
We manually parallelised the program by introducing an parallel
conjunction into the outer loop;
indicating that rows should be drawn in parallel.
This parallelisation is independent.
The raytracer uses many small structures to represent vectors and a pixel's
colour.
It is therefore memory allocation intensive.
Since we suspected that garbage collection was having a negative affect on
performance,
we developed a mandelbrot image generator.
The mandelbrot image generator has a similar structure to the raytracer:
it draws an image using two nested loops as above.
The mandelbrot image is drawn on the complex number plane.
A complex number $C$ is in the mandelbrot set if
$\forall i \cdot |N_i| < 2$ where $N_0 = 0$ and $N_{i+1} = N_{i}^2 + C$.
For each pixel in the image the program tests if the pixel's coordinates are
in the set.
The pixel's colour is chosen based on how many iterations $i$ are needed
before $|N_i| \ge 2$,
or black if the test survived 5,000 iterations.
We parallelised this program the same way as we did the raytracer:
by introducing an independent parallel conjunction into the outer loop.
We created two versions of this program.
The first version represents coordinates and complex numbers as structures
on the heap.
Therefore it has a high rate of memory allocation.
We call this version `mandelbrot\_highalloc'.
The second version of the mandelbrot program,
called `mandelbrot\_lowalloc',
stores its coordinates and complex numbers on the stack and in registers.
Therefore it has a lower rate of memory allocation.

\input{tab_gc}

\plan{Benchmark data.}
We benchmarked these three programs with different numbers of Mercury
engines and garbage collector threads.
We show the results in Table \ref{tab:gc}.
All benchmarks have an initial heap size of 16MB.
Each result is measured in seconds and represents the mean of eight samples.
The first column is the number of garbage collector threads used.
The second and third columns give sequential execution times
without and with thread safety enabled.
In these columns the programs where executed in such a way so that they did
not execute a parallel conjunction.
The remaining four columns give parallel execution times using one to four
Mercury engines.
The numbers in parentheses show the relative speedup when compared with the
sequential thread safe result for the same program.
\label{cabsav}
We ran our benchmarks on
\paul{Find a brand name i7 I can borrow for some weeks}
a Intel i7-2600K system
with 16GB of memory,
running Debian/GNU Linux 6
with a 2.6.32-5-amd64 Linux kernel and GCC 4.4.5-8.
The garbage collection benchmarks were gathered using a recent version of
Mercury (rotd-2012-04-29).
This version does not have the performance problems described in the
rest of this chapter,
and it does have the loop control transformation described
in Chapter \ref{chap:loop_control}.
Therefore we can observe the effects of garbage collection without
interference from any other performance problems.

\plan{Describe performance in practice.}
The raytracer program benefits from parallelism in both Mercury and the
garbage collector.
Using Mercury's parallelism only (four Mercury engines, and one GC thread) 
speeds the program up by a factor of 1.58,
compared to 1.29 when using the GC's parallelism only (one Mercury engine,
and four GC threads).
When using both Mercury and the GC's parallelism (four engines and four
marker threads)
the raytracer achieves a speedup of 2.73.
These speedups are much lower than we might expect from such a program:
either the mutator, the collector or both are not being parallelised well.
mandelbrot\_lowalloc does not see any benefit from parallel marking.
It achieves very good speedups from multiple Mercury engines.
We know that this program has a low allocation rate
but is otherwise parallelised in the same way as raytracer.
Therefore,
these results support the hypothesis that heavy use of garbage collection
makes it difficult to achieve good speedups when parallelising programs.
The more time spend in garbage collection,
the worse the speedup due to parallelism.
mandelbrot\_highalloc, which stores its data on the heap,
sees similar trends in performance as raytracer.
It is also universally slower than mandelbrot\_lowalloc.
mandelbrot\_highalloc achieves a speedup of 2.16 when using parallelism in
Mercury (four Mercury engines, and one GC thread).
The corresponding figure for the raytracer is 1.58.
When using the GC's parallelism
(one Mercury engine, and four GC threads)
mandelbrot\_highalloc achieves a speedup of 1.11,
compared with 1.29 for the raytracer.

\input{tab_gc_amdahl}

\plan{Introduce the table we use for our discussion of Amdahl's law.}
We can see that mandelbrot\_highalloc benefits from parallelism in Mercury
more than raytracer does,
conversely mandelbrot\_highalloc benefits from parallelism in the garbage
collector less than raytracer does.
Using \tscope (Chapter \ref{chap:tscope}),
we analysed how much time these programs spend running the garbage collector
or the mutator.
These results are shown in Table \ref{tab:gc_amdahl}.
The table shows the elapsed time, collector time, mutator time and the
number of collections for each program using one to four engines, and one
to four collector threads.
The times are averages taken from eight samples,
using an initial heap size of 16MB.
To use \tscope we had to compile the runtime system differently.
Therefore,
these results differ slightly from those in Table \ref{tab:gc}.
Next to both the collector time and mutator time
we show the percentage of elapsed time taken by the collector or mutator
respectively.

\plan{Describe trends in either the mutator or GC time.}
As we expected, mandelbrot\_lowalloc spends very little time running the collector.
Typically, it ran the collector only twice during its execution.
Also, total collector time was usually between 5 and 30 milliseconds.
The other two programs ran the collector hundreds of times.
As we increased the number of Mercury engines
we noticed that these programs made fewer collections.
As we varied the number of collector threads,
we saw no trend in the number of collections.
Any apparent variation is most likely noise in the data.
All three programs see a speedup in the time spent in the mutator as the
number of Mercury engines is increased.
Similarly,
the raytracer and mandelbrot\_highalloc benefit from speedups
in GC time as the number of GC threads is increased.
In mandelbrot\_highalloc,
the GC time also increases slightly as Mercury engines are added.
We expect that as more Mercury engines are used the garbage collector
must use more inter-core communication which has additional costs.
However, in the raytracer,
the GC time decreases slightly as Mercury engines are added.
To understand why this happens we would need to understand the collector in
detail,
however Boehm GC's sources are notorious for being difficult to read and
understand.

\plan{Compare performance with Amdahl's predictions.}
As Amdahl's law predicts,
parallelism in one part of the program has a limited effect on the program
as a whole.
While using one GC thread
we tested with one to four Mercury engines;
the mutator time speedups for two, three and four engines were
1.83, 2.55 and 3.03 respectively.
However, the elapsed time speedups for the same test were only
1.35, 1.53 and 1.66 respectively.
Although there is little change in absolute time spent in the collector;
there is an increase in collector time as a percentage of elapsed time.
The corresponding mutator time speedups for mandelbrot\_highalloc were
similar,
at 1.80, 2.63 and 3.34 for two, three and four Mercury engines.
The elapsed time speedups on the same test for mandelbrot\_highalloc were
1.52, 1.90 and 2.15.
Like raytracer above, the elapsed time speedups are lower than the mutator
time speedups.
Similarly,
when we increase the number of threads used by the collector,
it improves collection time more than it does elapsed time.

When we increased both the number of threads used by the collector and the
number of Mercury engines
(a diagonal path through the table)
both the collector and mutator time decrease.
This shows that parallelism in Mercury and in Boehm GC both contribute to
the elapsed time speedup we saw in the diagonal path in Table \ref{tab:gc}.
As Table \ref{tab:gc_amdahl} give us more detail,
we can also see that collector time as a percentage of elapsed time
increases as we add threads and engines to the collector and Mercury.
This occurs for both the raytracer and mandelbrot\_highalloc.
It suggests that the mutator makes better use of additional Mercury
engines
than the collector makes use of additional threads.
We can confirm this by comparing the speedup for the mutator with the speedup
in the collector.
In the case of raytracer using four Mercury engines and one GC thread
the mutator's speedup is $20.9 / 6.9 = 3.03$ over the case for one Mercury
engine and one GC thread.
The collectors speedup with four GC threads and one Mercury engine over
the case for one GC thread and one Mercury engine is $16.9 / 8.7 = 1.94$.
The equivalent speedups for mandelbrot\_highalloc are:
$40.7 / 12.2 = 3.34$ and $11.0 / 4.5 = 2.44$.
Similar comparisons can be made along the diagonal of the table.

\begin{table}
\begin{center}
\begin{tabular}{l|rr|d{3}}
\Cbr{Program} & \C{Allocations} & \Cbr{Total alloc'd bytes} & \C{Alloc rate (M alloc/sec)} \\
\hline
raytracer   &     561,431,515 &           9,972,697,312 & 26.9 \\
mandelbrot\_highalloc
            &   3,829,971,662 &          29,275,209,824 & 94.1 \\
mandelbrot\_lowalloc
            &       1,620,928 &              21,598,088 &  0.106 \\
\end{tabular}
\end{center}
\caption{Memory allocation rates}
\label{tab:mem_alloc_rate}
\end{table}

\plan{Allocation intensity}
Table \ref{tab:gc_amdahl} also shows that raytracer spends more of its
elapsed time doing garbage collection than mandelbrot\_highalloc does.
This matches the results in Table \ref{tab:gc},
where mandelbrot\_highalloc has better speedups because of parallelism in
Mercury than raytracer does.
Likewise,
raytracer has better speedups because of parallelism in the collector
than mandelbrot\_highalloc does.
This suggests that mandelbrot\_highalloc is less allocation intensive than
raytracer,
but this is not true.
Using Mercury's deep profiler we measured the number and total size of memory
allocations.
This is shown in Table \ref{tab:mem_alloc_rate}.
The rightmost column in this table gives the allocation rate
measured in million allocations per second.
It is calculated by dividing the number of allocations in the second column
by the average mutator time reported in Table \ref{tab:gc_amdahl}.
Therefore, it represents the allocation rate during mutation time.
This is deliberate because,
by definition,
allocation cannot occur during collector time.
mandelbrot\_highalloc has a higher allocation rate than raytracer.
However,
it spends less time doing collection, when measured either absolutely
or by percentage of elapsed time.
Garbage collectors are tuned for particular workloads.
It is likely that Boehm GC handles mandelbrot\_highalloc's workload more
easily than raytracer's workload.

% Old idea:
%This may be due to changing allocation rates throughout the programs'
%executions.
%We noticed that while benchmarking the raytracer that if we rendered the
%image upside-down (by iterating over the rows backwards)
%we would get different performance results.
%This is because the top of the image we used contains sky,
%which has no geometry;
%it is the flat colour of the background,
%and the bottom of the image contains ground;
%which has more detail.
%Rendering the ground will naturally be more complicated and require more
%memory allocation than rendering the sky.
%Memory allocation in one part of a program can affect the performance of
%allocation and collection in another part of the program.
%This is many because the garbage collector will increase the size of the heap
%to satisfy large and/or many allocations.
%This is why rendering the image upside down had a different execution time to
%rendering it the right way up.
%Similarly,
%the mandelbrot program has sections in its image that require more iterations
%of the equation than others.

\input{tab_gc_heapsize_gc4}

\plan{New data, vary the heap size.}
So far,
we have shown that speedups due to Mercury's parallel conjunction
are limited by the garbage collector.
Better speedups can be achieved by using the parallel marking feature in the
garbage collector.
We also attempted to improve performance further by modifying the initial
heap size of the program.
Table \ref{tab:gc_heapsize} shows the performance of the raytracer and
mandelbrot\_highalloc programs with various initial heap sizes.
The first column shows the heap size used;
we picked a number of sizes from 1MB to 512MB.
The remaining columns show the average elapsed times in seconds.
The first of these columns shows timing for the programs compiled for
sequential execution without thread safety;
this means that the collector cannot use parallel marking.
The next column is for sequential execution with thread safety;
the collector uses parallel marking with four threads.
The next four columns give the results for the programs compiled for
parallel execution, and executed with one to four Mercury engines.
These results also use four marker threads.
The numbers in parentheses are the ratio of elapsed time compared with the
sequential thread-safe result for the same initial heap size.
All the results are the averages of eight test runs.
We ran these tests on raytracer and mandelbrot\_highalloc.
We did not use mandelbrot\_lowalloc as its collection time is insignificant
and would not have provided useful data.

\plan{Observations: Programs get faster with a larger heap size.}
Generally, the larger the initial heap size the better the programs
performed.
The Boehm GC will begin collecting if it cannot satisfy a memory allocation
request.
If, after a collection, it still cannot satisfy the memory request then it
will increase the size of the heap.
In each collection,
the collector must read all the stacks, global data, thread local data and
all the in-use memory in the heap 
(memory that is reachable from the stacks, global data and thread local
data).
This causes a lot of cache misses, especially when the heap is large.
The larger the heap size,
the less frequently memory is exhausted and needs to be collected.
Therefore,
programs with larger heap sizes garbage collect less often and
have fewer cache misses due to garbage collection.
This explains the trend of increased performance as we increase the initial
heap size of the program.

Although performance improved with larger heap sizes,
there is an exception.
The raytracer often ran more slowly with a heap size of 64MB
than with a heap size of 32MB.
%\paul{Does the reader know what a TLB is?  People are generally poor at
%remembering to care about normal caches much less TLBs.}
64MB is much larger than the processor's cache size
(this processor's L3 cache is 8MB) and
covers more page mappings than its TLBs can hold (L2 TLB covers 2MB when
using 4KB pages).
The collector's structures and access patterns may be slower at this size
because of these hardware limitations,
and the benefits of a 64MB heap are not enough to overcome the effects of
these limitations,
however the benefits of a 128MB or larger heap are enough.

\plan{Speedup observation with larger heap sizes.}
Above, we said that programs perform better with larger heap sizes.
This measurement compares their elapsed time with one heap
size with the elapsed time using a different heap size.
We also found that programs \emph{exploited parallelism more easily} with
larger heap sizes:
This measurement compares programs' speedups (which are themselves
comparisons of elapsed time).
When the raytracer uses a 16MB initial heap its speedup using four
cores is 2.17 times.
However, when it uses a 512MB initial heap the corresponding speedup is
2.99.
Similarly,
mandelbrot\_highalloc has a 4-engine speedup of 2.72 using a 16MB initial
heap and a speedup of 3.08 using a 512MB initial heap.
Generally,
larger heap sizes allow programs to exploit more parallelism.
There are two reasons for this

\begin{description}

    \item[Reason 1]
    In a parallel program with more than one Mercury engine,
    each collection must \emph{stop-the-world}:
    All Mercury engines are stopped so that they do not modify the heap during
    the marking phase.
    This requires synchronisation which reduces the performance of parallel
    programs.
    Therefore, the less often collection occurs, the less stop-the-world's
    synchronisation affects performance.

    \item[Reason 2]
    Another reason was suggested by Simon Marlow:
    Because Boehm GC uses its own threads for marking and not Mercury's,
    it cannot mark objects with the same thread that allocated the objects.
    Unless Mercury and Boehm GC both mapped and pinned their threads to
    processors,
    and agreed on the mapping then making will cause a large number of cache
    misses.
    For example, during collection, processor one (P1) marks one of processor two's
    (P2) objects,
    causing a cache miss in P1's cache and invalidating the corresponding cache
    line in P2's cache.
    Later, when collection is finished,
    P2's process resumes execution and incurs a cache miss for the object that
    it had been using.
    \paul{Find the citation.}
    Simon Marlow made a similar observation when working with GHC's garbage
    collector \cite{marlow-gc}.

\end{description}

\plan{Discuss local heaps for threads, and their reliability problems.}
We also investigated another area for increased parallel performance.
Boehm GC maintains thread local free lists that allow memory to be allocated
without contending for locks on global free lists.
When a local free list cannot satisfy a memory request, the global free
lists must be used.
The local free lists amortise the costs of locking the global free lists.
We anticipate that increasing the size of the local free lists will cause even
less contention for global locks,
allowing allocation intensive programs to have better parallel
performance.
The size of the local free list can be adjusted by increasing the
\texttt{HBLKSIZE} tunable.
Unfortunately this feature is experimental,
and adjusting \texttt{HBLKSIZE} caused our programs to crash.
Therefore we cannot evaluate how \texttt{HBLKSIZE} affects our
programs.
Once this feature is no-longer experimental,
adjusting \texttt{HBLKSIZE} to improve parallel allocation should be
investigated.

