
\status{
    This section is currently a copy of prose used elsewhere.
    The intention is to discuss:
}

\begin{itemize}
\item the deep profiler,
\item including a description of why deep profiling provides us with
      more useful data than conventional profiling.
\item also explain the limitations of deep profiling,
      for example, that levels within recorsive code don't have seperate
      data recorded for them.
\item Coverage profiling 
\item profiling feedback framework
\end{itemize}

% XXX not sure if we need this intro para.
% The first profiler we built for Mercury
% was basically a port of the Unix utility \code{gprof}\cite{gprof}.
% However, we soon found that this style of profiling
% usually produced less-than-useful results,
% due mostly to the prevalence of parametric polymorphism and higher order calls
% in typical Mercury code.
% For example, while a C program may have
% separate tables for different kinds of entities,
% for whose access functions
% the profiler would gather separate performance data,
% most Mercury programs would use
% the same polymorphic code to handle all those tables,
% making the task of disentangling the characteristics of the different tables
% unfeasibly hard.

The Mercury deep profiler~\cite{conway:2001:mercury-deep}
gathers much more information about the context of each measurement
than traditional profilers like \code{gprof}\cite{gprof}.
When it records the occurrence
of a call, a memory allocation or a profiling clock interrupt,
it records with it the chain of ancestor cliques
\paul{SCCs}
(groups of mutually recursive procedures)
all the way from the current call to the entry point of the program,
a procedure named \code{main}.
This detail allows the deep profiler
to find and present to the user
not just information such as the total number of calls to a procedure
and the average cost of a call,
or even information such as the total number of calls to a procedure
from a particular call site and the average cost of a call from that call site,
but also information such as the total number of calls to a procedure
\emph{from a particular call site
when invoked from a particular chain of ancestor cliques}
and the average cost of a call \emph{in that context}.
For example, it could tell that
procedure \code{h} called procedure \code{i} ten times
when \code{h}'s chain of ancestors was \code{main -> f -> h},
while \code{h} called \code{i} only seven times
when \code{h}'s chain of ancestors was \code{main -> g -> h},
the calls from \code{h} to \code{i} took on average twice as long
from the \code{main -> g -> h} context as from \code{main -> f -> h},
so that despite the fewer calls,
\code{main -> g -> h -> i} took more time than \code{main -> f -> h -> i}.
\paul{Reinforce why this is useful for auto-parallelism.
The reader may want to know why they're being given this information.}

Profilers have traditionally measured time
by sampling the program counter at clock interrupts.
Unfortunately, even on modern machines
the usual infrastructure for clock interrupts (\emph{e.g}., SIGPROF on Unix)
supports only one frequency for such interrupts,
which is usually 60 or 100 Hz.
This frequency is far too low for the kind of detailed measurements
the Mercury deep profiler wants to make,
since for typical program runs of few seconds,
it results in almost all calls having a recorded time of zero,
with the calls recording a nonzero time
(signifying the occurrence of an interrupt during their execution)
being selected almost at random.

We have therefore implemented a finer-grained measure of time
that turned out to be very useful
even though it is inherently approximate.
This measure is call sequence counts or CSCs:
the profiled program basically behaves
as if the occurrence of a call signified
the occurrence of a new kind of profiling interrupt.
In imperative programs, this would be a horrible measure,
since calls to different functions can have hugely different runtimes.
However, in declarative language like Mercury there are no explicit loops;
what a programmer would do with a loop in an imperative language
must be done by a recursive call.
This means that the only thing that the program can execute between two calls
is a sequence of primitive operations such as unifications and arithmetic.
% \paul{usually but not necessarily straight line code}
% \peter{doesn't seem important}
For any given program,
there is a strict upper bound on the maximum length of such sequences,
and the distribution of the length of such sequences
is very strongly biased towards very short sequences
of half-a-dozen to a dozen operations.
In practice, we have found that
the fluctuations between the lengths of different such sequences
can be ignored for any measurement
that covers any significant number of call sequence counts (CSCs),
say more than a hundred.
The only drawback of this scheme that we have found
is that on 32 bit platforms,
its usability is limited to short program runs (a few seconds)
by the wraparound of the global CSC counter;
on 64 bit platforms, the problem would occur
only on a profiling run that lasts for years.

% Feedback
%%%%%%%%%%

% Introduce feedback concept, assert that it is needed for
% feedback-directed implicit parallelism.
% This is the motivation for the rest of the section.

Our implicit parallelism analysis operates on profiling data and a
bytecode representation of the program.
A program compiled for both deep profiling and coverage profiling
produces both of these when executed.
% with the correct environment variable set.

An automatic parallelisation tool
may wish to traverse the entire call tree of the program.
For example, to limit the amount of parallelism at each stage of execution
to the number of CPUs available.
Such a tool needs to be able to work on the whole program.
The Mercury compiler never operates on the whole program:
it compiles separate modules separately.
We therefore implemented our implicit parallelisation analysis in a new tool called
\texttt{mdprof\_feedback}.
For now, its task is to use profiling data and our variable use analysis
to choose the conjunctions that are most profitable to parallelise,
and to record this information in a \emph{feedback file}
that can then be given to the compiler.
The data flow of the feedback process is shown in Figure
\ref{fig:prof_feedback_loop}.

\picfigure{prof_fb}{Profiler feedback loop}

Telling the compiler what to parallelise is just one use of profiler feedback.
Other optimisations (such as inlining) could also benefit
from profiling feedback.

Using a feedback-directed optimisation requires that the user compile
and run their program before feedback information is produced, 
which takes a significant amount of time.
Because of this, feedback-directed optimisations do not make sense
unless
the total execution time of the optimised program --- which depends
mostly on how many times it will be executed --- is significantly
greater than the time it takes to compile and execute the profiling
version of the program.
We expect that feedback-directed optimisations would normally be used
when the programmer is building a \emph{release candidate} version of their
program, after they have performed testing and fixed any bugs.

In all cases the program should be profiled on an input that is
representative of the inputs the program is expected to handle when
it is used normally.
This is important to ensure the compiler makes decisions using
feedback information that improves rather than degrades the performance
of the program in general.
How a representative input is chosen and how important the exact choice
of the profiling input is will change from program to program.

% Describe data types: the feedback data can be any mercury data
% types.  One can query the feedback data by asking it to complete the
% instantiation of a type.
We have designed and implemented a generic feedback framework that allows
tools (both inside or outside the compiler)
to create feedback information for the compiler.
The feedback information can be expressed as any Mercury type, making the
feedback framework very flexible.
New feedback-directed optimisations may require feedback information
from new analyses.
We expect that many new feedback information types will be added to
support these optimisations.
We also expect that an optimisation may use more than one type of
feedback information and that more than one optimisation may use the
same feedback information.

% Describe on-disk format,
The on-disc format of the feedback information file is very simple, it
contains a header that identifies the file format, including a version number.
What follows is a list of feedback information items.
When the file is read in the file format identifier and revision
number are checked,
the list of information items is checked to ensure that no two items
describe the same feedback information, for example there can only be at
most one item in this list that describes how to automatically
parallelise a program.

