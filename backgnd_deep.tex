
\status{
    This section and 2.5 have been merged, as they provide motivation for one
    another.
    This will likely be a large section, so I've broken it into sub-sections
}

% Declaration of work done that contributed to a previous award,
Some of the work described in this section contributed towards my honours research project,
% Note that my degree is not '... with Honours', it's just 'Honours', despite
% what my degree actually says.  Basically, I've been erroneously awarded an
% extra bachelor's degree.
which was in partial fulfilment of the Degree of Bachelor of Computer Science
Honours.
Most parts of the implementation were incomplete and did not work correctly or
at all.
Therefore,
their evaluation could not be included in my honours report.
They have since been completed and corrected during my Ph.D.\ Candidature.
There are some exceptions,
most significantly the coverage profiling transformation (Section \ref{sec:coverage})
developed for my honours research project was complete, and is used, largely
unchanged in my Ph.D.\ research.
The related analysis described in Section \ref{sec:var_use_analysis}
and the feedback framework in Section \ref{sec:feedback}
were also written as part of my honours research project.
Still other parts of this section are not my own work,
namely the work described in
\citet{conway:2001:mercury-deep} and
\citet{tannier:2007:parallel_mercury}.

Most compiler optimisations operate only on the representation of the program
in the compiler's memory.
For most optimisations this is sufficient.
However,
automatic parallelisation is sensitive to variations in the runtime cost of
parallelised tasks.
This sensitivity increases when dependent parallelisation is used.
For example,
a search operation on a small list is cheap, compared to the same operation on
a large list.
It may not be useful to parallelise the search on the small list against some
other computation,
but it will usually be useful to parallelise the search on the large list
against another computation.
It's important not to create too much parallelism;
overheads of parallelisation for which there is no speed up will slow the
program down.
Therefore, not just sub-optimal to parallelise the search of the small list,
but detrimental.
The only way we can know the actual cost of most pieces of code,
is by understanding their typical inputs,
or measuring their runtime cost while operating on typical inputs.
Therefore,
profiling data can be used in auto-parallelisation;
it allows us to predict the runtime cost of computations whose
code is the same but whose inputs are different.
These predictions are used to drive parallelisation decisions,
which are, in turn, used to select where and how parallelism is introduced.

\picfigure{prof_fb}{Profiler feedback loop}

To use profiling data for optimisations the usual workflow for compiling
software must change.
Figure \ref{fig:prof_fb} shows a profiler-feedback workflow.
The source code is compiled, with profiling enabled and the resulting program is
executed on a representative input.
As the program terminates it writes its profiling data to disk,
The profiling data includes a bytecode representation of the program
similar to the compiler's representation.
The analysis tool reads this file,
and writes out a description of how to parallelise the program in the form of a
feedback file.
The program is compiled for a second time,
this time with auto-parallelism enabled.
The compiler reads the feedback file and introduces parallelism into the
resulting program.
Figure \ref{fig:prof_fb} shows this workflow.

A common criticism of profile directed optimisation is that the programmer will
have to compile the program twice,
and run it at least once to generate profiling data.
"If I have to run the program in order to optimise it, then the program has
already
done its job and there's no point continuing with the optimisation?"
The answer to this is that a program's life time is often far more than a
single execution.
A program will usually be used many times, and by many people.
Each time the optimisation will have a benefit,
this benefit will pay off the cost of the feedback directed optimisation.
We expect that feedback-directed optimisations would normally be used
when the programmer is building a \emph{release candidate} version of their
program,
after they have performed testing and fixed any bugs.

Another criticism is that if the program is profiled with one set of input and
used with another that the profile will not necessarily represent the actual use
of the program,
and that the optimisation may not be as good as it could be.
There are many different types of programs, and therefore many different
program inputs;
we cannot hope to address this particular problem directly.
Selecting a typical input for a given program must be done by the programmer.
Other strategies exist that avoid this problem
such as dynamic recompilation.

\subsection{The deep profiler}

Typical Mercury programs make heavy use of code reuse in the form of
parametric polymorphism and higher order calls.
This is also true for other declarative languages.
For example, while a C program may have
separate tables for different kinds of entities,
for whose access functions
the profiler would gather separate performance data,
most Mercury programs would use
the same polymorphic code to handle all those tables,
making the task of disentangling the characteristics of the different tables
infeasibly hard.

This problem has been solved for Mercury by introducing deep profiling
\citep{conway:2001:mercury-deep}.
Mercury's deep profiler gathers much more information about the context of
each measurement than traditional profilers like \code{gprof}\cite{gprof}.
When it records the occurrence of a call,
a memory allocation or a profiling clock interrupt,
it records with it the chain of ancestor calls,
all the way from the current call to the entry point of the program,
a procedure named \code{main}.
To make this tractable,
recursive and mutually recursive calls,
known as \emph{strongly connected components} (SCCs),
must be folded into a single memory structure.
Therefore, the call graph of the program is a tree (there are no cycles)
and SCCs are represented as single nodes in this tree.

Deep profiling allows the profiler to find and present to the user
not just information such as the total number of calls to a procedure
and the average cost of a call,
or even information such as the total number of calls to a procedure
from a particular call site and the average cost of a call from that call site,
but also information such as the total number of calls to a procedure
\emph{from a particular call site
when invoked from a particular chain of ancestor SCCs}
and the average cost of a call \emph{in that context}.
For example, it could tell that
procedure $h$ called procedure $i$ ten times
when $h$'s chain of ancestors was $main \calls f \calls h$,
while $h$ called $i$ only seven times
when $h$'s chain of ancestors was $main \calls g \calls h$,
the calls from $h$ to $i$ took on average twice as long
from the $main \calls g \calls h$ context as from $main \calls f \calls h$,
so that despite the fewer calls,
$main \calls g \calls h \calls i$ took more time than $main \calls f \calls h \calls i$.
\paul{Draw a figure}

% XXX: I considered moving this to the Mercury section, but if I
% introduce it here then it's right next to its use, which is better.
% I also considered a larger example such as the quadratic equation, but
% it is not necessary.
During compilation the arguments of functors and predicate calls are
simplified so that they are single variables.
For instance \code{foo(a(3), X)} is simplified to:

\begin{verbatim}
V_1 = 3,
V_2 = a(X1),
foo(V_2, X)
\end{verbatim}

\noindent
This new conjunction is flattened into the conjunction that contained the
original expression.
Code that is represented in this way is referred to as being in \emph{super
homogeneous form}.
This can create many small conjuncts from just a single expression,
for instance, the quadratic equation
($x=\frac{-b \pm \sqrt {b^2-4ac}}{2a}$) which can be written on one line
is flattened into 12 individual conjuncts (excluding the plus-or-minus).

Therefore the profiler
(and other tools such as the debugger) introduced \emph{goal paths}
to uniquely identify a sub goal in any goal tree such as a procedure's%
\footnote{
A procedure is either a predicate \& mode or a function (Section
\ref{sec:backgnd_mer})}
body.
A goal path is a list of goal path steps, each step describes which
sub-goal to recurse into when traversing a goal structure.
\paul{XXX: An example would make this easier to follow,
but I feel I might be yak-shaving by introducing too many examples.}
Since the debugger was written first goal paths where introduced then,
\paul{XXX: Check that the mdb paper really does describe goal paths}
and the deep profiler was able to re-use this concept~\cite{mdb}.

There are four structures used during profiling,
each one is also reflected in the profiling data.
These are: {\PS}, {\PD}, {\CSS} and {\CSD}.
{\PS} represents the static data about a procedure
This data includes the procedure's name and arity,
the source file it can be found in and the line number within that source file.
\CSS represents static data for a call cite.
This includes the procedure the call site is found in, the line number,
the goal path and the callee (if known statically).
The dynamic variants of these structures, \PD and {\CSD},
form the call tree beginning at \code{main/2} (a \PD structure)
which has a number of \CSD structures each having \PD structures
beneath them.
The actual profiling data for a call and its context are stored in
the \CSD structure for that call,
the context is identified by the sequence of structures pointing to this
structure.
When the profiling data file is read,
the profiling tool will build the call graph and perform a topological
sort.
The result of the sort is a list of SCCs
(which \citet{conway-deep} calls \emph{cliques})
each SCC is used to create a \Clique data structure.
A number of indexes are also constructed.
The indexes, {\Clique}s, {\PS}s, {\PD}s, {\CSS}s and {\CSD}s are used to
efficiently traverse the program's profile in both
the easy to browse user interface,
and automated tools, such as the automatic parallelism analysis.

% Callseqs
Profilers have traditionally measured time
by sampling the program counter at clock interrupts.
Unfortunately, even on modern machines
the usual, and portable infrastructure for clock interrupts
(\emph{e.g}., SIGPROF on Unix)
supports only one frequency for such interrupts,
which is usually 60 or 100Hz.
This frequency is far too low for the kind of detailed measurements
the Mercury deep profiler wants to make,
since for typical program runs of few seconds,
it results in almost all calls having a recorded time of zero,
with the calls recording a nonzero time
(signifying the occurrence of an interrupt during their execution)
being selected almost at random.

We have therefore implemented a finer-grained measure of time
that turned out to be very useful
even though it is inherently approximate.
This measure is \emph{call sequence counts} (CSCs):
the profiled program basically behaves
as if the occurrence of a call signified
the occurrence of a new kind of profiling interrupt.
In imperative programs, this would be a horrible measure,
since calls to different functions can have hugely different runtimes.
However, in declarative language like Mercury there are no explicit loops;
what a programmer would do with a loop in an imperative language
must be done by a recursive call.
This means that the only thing that the program can execute between two calls
is a sequence of primitive operations such as unifications and arithmetic.
For any given program,
there is a strict upper bound on the maximum length of such sequences,
and the distribution of the length of such sequences
is very strongly biased towards very short sequences
of half-a-dozen to a dozen operations.
In practice, we have found that
the fluctuations between the lengths of different such sequences
can be ignored for any measurement
that covers any significant number of call sequence counts,
say more than a hundred.
The only drawback of this scheme that we have found
is that on 32 bit platforms,
its usability is limited to short program runs (a few seconds)
by the wraparound of the global CSC counter;
on 64 bit platforms, the problem would occur
only on a profiling run that lasts for years.

\subsection{Prior Auto-parallelism Work}

\citet{tannier:2007:parallel_mercury} describes a prior attempted at automatic
parallelism in Mercury.
In this approach, the parallelisation feedback data was gathered by an analysis
tool,
this feedback was a list of the procedures with the highest cost in the program,
these were chosen by a configurable measurement and threshold.
The analysis did not use a representation of the program,
as at that time the profiler did not support that capability.
Nor did it use any advanced features of the profiler.
Parallelisation decisions were made in the compiler:
each procedure was searched for calls that appeared in the list of top procedures
and calls to these where parallelised against similar calls if they
where independent or had fewer than some $N$ shared variables.

\paul{XXX:  'my' or 'our' in this work.  I know we usually use plural
first person in papers,
Is this true for dissertations as well?}
For my honours project I attempted automatic parallelism of Mercury
\citep{paul_hons}.
There where a number of differences between my work and Tannier's.
The first differences is that at the time of my work the deep profiler
was able to export a representation of the program
(my work was the motivation for this feature).
This allowed my analysis to access profiling data and a representation
of the program at the same time.
This allows the analysis to measure when variables are produced and
consumed within parallel conjuncts,
and therefore calculate,
for conjunctions with a single shared variable,
how much parallelism is available.
The formula used was:

\begin{eqnarray*}
T_{Sequential} & = & T_A + T_B \\
T_{DependentB} & = & max(T_{BeforeProduceA}, T_{BeforeConsumeB}) +
T_{AfterConsumeB} \\
T_{Parallel} & = & max(T_A, T_{DependentB}) + T_{Overheads} \\
Speedup & = & \frac{T_{Sequential}}{T_{Parallel}}
%\label{eqn:time_deppar}
\end{eqnarray*}

This describes the speedup due to parallelism of two conjuncts, $A$ and $B$.
whose execution times are $T_A$ and $T_B$.
In the parallel case $B$'s execution time is $T_{DependentB}$ since it
accounts for the $B$'s dependency on $A$.
Except for this small difference,
the calculation of parallel execution time and speedup are the commonly
used formulas for many parallel execution cost models.

This cost model requires information about when a shared variable is
produced by the first conjunct and consumed by the second.
To provide this information we introduced coverage profiling to
Mercury's deep profiler,
and used the resulting coverage data in a variable use time analysis.

\subsection{Coverage Profiling}
\label{sec:coverage}

The traditional Prolog port model~\cite{port_model} describes each predicate as
having four ports through which control may flow.
Control can flow into a predicate through either the \emph{call} or
\emph{redo} ports,
control flows out of a predicate through either the \emph{exit} or
\emph{fail} ports.
These describe:
    a call being made to the predicate,
    a call to the predicate being re-entered because there may be more
    solutions,
    the predicate exiting with a solution,
    or the predicate failing because there are no more solutions.
Mercury adds an extra port to these semantics known as the
\emph{exception} port,
which a predicate uses to return control to its caller when it throws an
exception.
Control must leave a predicate the same number of times that it enters:

\begin{equation*}
Calls + Redos = Exits + Fails + Exceptions
\end{equation*}

\noindent
Mercury adds additional invariants for many determinism types.
For example, deterministic code may not redo or fail,
therefore these port counts will be zero for deterministic code.
The deep profiler tracks the number of times each port is used for a given
call.
Port counts provide code coverage data predicate and call-site entry and exit.

% XXX: Actually implement the case for inferring Elses' coverage.
Code coverage of many goals can be inferred,
in the simple case of a conjunction containing deterministic conjuncts.
each conjunct will have the same coverage as the one before it,
(we make the assumption that the code does not throw an exception
as doing so is rare).
It follows that a deterministic conjunction containing a single call.
can have complete coverage information inferred for all of its conjuncts.
Similarly,
the coverage before an \emph{if-then-else} can be inferred as the sum
of the coverage at the beginning of the \emph{then} and \emph{else}
branches,
assuming that the condition has at most one solution.
The same is true for the ends of these branches and the end of the
if-then-else.
The coverage at the beginning of the then part is also equal to the
number of times the \emph{condition} of the if-then-else succeeds.
Therefore,
in a single forward pass,
we will attempt to infer the coverage at the beginning of the branches
based on the coverage before the if-then-else and at the end of the
condition,
and the condition's determinism.
A similar inference can be made for switches,
except that they do not have a condition,
we simply infer the
coverage of the final switch branch based on the coverage of the other
branches and before the switch provided that it is a complete switch.
% A good reader might realize that we've only discussed a forward pass,
% more complicated passes are possible but the reader will have to take
% in on faith that this will be discussed later in this section.

However, port counts alone will not provide complete coverage information.
for example, a conjunction containing a sequence of \dsemidet
unifications will have incomplete coverage information:
We can not know how often each individual unification fails.
We introduced coverage profiling as part of Mercury's deep profiler to
add extra instrumentation to gather coverage data where it would
otherwise be unknown.

When the compiler instruments the program for deep profiling,
it will make a forward pass through the goal tree of each procedure.
During this pass,
a boolean value is maintained that describes whether or not coverage
data would be available at this point on the program.
When this boolean becomes false,
the algorithm will introduce a coverage point at that place in the code
ans set the value to true.
The value will be set to false after a goal with a determinism other than
\ddet or \dccmulti and which does not provide coverage data of its own,
or at the beginning of a branch where the coverage can not be inferred.
This algorithm has the same structure as the coverage inference
algorithm,
this way,
a coverage point will always be placed where the coverage inference
algorithm would expect to find one.

\begin{figure}
\begin{tabular}{l}
\code{map(P, Xs0, Ys) :-} \\
\code{~~~~(} \\
\code{~~~~~~~~}\instr{coverage\_point(ProcStatic, 0);} \\
\code{~~~~~~~~Xs0 = [],} \\
\code{~~~~~~~~Ys = []} \\
\code{~~~~;} \\
\code{~~~~~~~~Xs0 = [X $|$ Xs],} \\
\code{~~~~~~~~P(X, Y),} \\
\code{~~~~~~~~map(P, Xs, Ys0),} \\
\code{~~~~~~~~Ys = [Y $|$ Ys0]} \\
\code{~~~~).} \\
\end{tabular}
\caption{Coverage annotated \code{map/3}.}
\label{fig:map_coverage}
\end{figure}

Figure \ref{fig:map_coverage} shows \code{map/3} annotated with a
coverage point.
The forward pass through the procedure would not be able to infer
coverage at the beginning of this switch arm,
and therefore the similar pass in the deep profiler introduced a
coverage point at that position.
This point along with the coverage of the procedure as a whole
is enough to infer coverage for the whole procedure on a single pass.
It is true that the the coverage point could be removed and the port
counts of the recursive code in this example could be used,
doing so would require multiple passes
for both inference and instrumentation..
Coverage profiling adds only a small cost to profiling;
it was not, and has never been, a high priority to optimise this
further.

Coverage data is tracked in the \PS structure
which we introduced in Section \ref{sec:backgnd_deep}.
Three new fields where added to the \PS structure:
a constant integer representing the number of coverage points in this
procedure;
an array of constant structures,
each describing the static details of a coverage point;
and an array of integers representing the number of times each coverage
point has been executed.
The coverage point instrumentation code refers to the procedure's \PS
structure and increments the coverage point's counter at the appropriate index.
The static coverage data,
which includes the goal path of the coverage point and the type of the
coverage point (branch or post-goal),
is written out with the profile.
Because coverage points are associated with \PS structures,
and not either \PD or \CSD structures,
they are not associated with context in the way that other profiling
data is.
That is to say,
coverage data of a particular predicate covers the general use of that
predicate,
The auto-parallelisation work described in my honours report
\citep{pbone_hons}
would not have benefited from context specific coverage data.

The coverage point in the Figure \ref{fig:map_coverage}
has two arguments,
the first points the instrumetation code at the \PS containing
this procedure's coverage points,
the second is the index within this procedure's coverage point arrays
for this coverage point.
Although the instrumentation code appears to be a call,
it is only a few lines in C and is therefore implemented as a C
preprocessor macro, and therefore inlined into the compiled procedure. 


\subsection{Variable Use Time Analysis}
\label{sec:var_use_analysis}

As described above,
the automatic parallelism analysis used in \citet{pbone-hons} considers
conjunctions with at most two conjuncts and one shared variable.
To predict how much parallelism is available it needs to know
when that variable is produced by the first conjunct
and the first time it is consumed by the second.
These times are calculated in the unit of call sequence counts.

During compilation of dependent conjunctions a future is created to
replace any shared variable.
The compiler will attempt to push that future into the goal that
produces the value so that,
if possible,
the \signal operation will be placed immediately after the unification
that binds the variable.

Therefore,
the algorithm for computing the expected production time
of a shared variable looks at the form of the conjunct that produces it.
For each goal type it determines if it is possible to push the future
into that goal so that the future is signaled as early as possible.
As noted above,
a future cannot be pushed into certain goal types,
such as higher order calls, foreign code or calls that cross module
boundaries.

\begin{itemize}
\item
If the goal is a unification,
the expected production time is zero.
Because call sequence counts measure the time between calls
and the unification is not a call. 
\item
If the goal is a foreign code goal,
the expected production time is the total cost of the goal.
If the foreign code calls back into Mercury,
then the cost of the goal is one CSC plus the cost of the goals executed
when re-entering mercury.
Otherwise, it is as exactly one CSC.
\item
If the goal is a first order call,
we recurse on the body of the callee.
\item
If the goal is a higher order call,
the expected production time is the cost of the call,
because the compiler cannot insert
the signalling of the future into the callee's body.
\item
If the goal is a conjunction $G_1,~\ldots,~G_n$,
and the variable is generated by $G_k$,
then we add up the total time taken by $G_1,~\ldots,~G_{k-1}$,
and add the sum to the result of invoking the algorithm recursively on $G_k$.
\item
If the goal is a switch,
we invoke the algorithm recursively on each switch arm,
and compute a weighted average of the results,
with the weights being the arms' entry counts as determined by coverage
inference.
\item
If the goal is an if-then-else,
we need the weighted average of the two possible cases:
the variable being generated by the then arm versus the else arm.
(It cannot be generated by the condition:
variables generated by the condition are visible only from the then-arm.)
if they were visible from anywhere else,
they would not have a value if the condition failed.)
% TODO: implement this behaviour
To find the first number,
we invoke the algorithm on the then-arm,
and add the result to the time taken by the condition.
To find the second,
we invoke the algorithm on the else-arm,
and add the result to the expected time taken by the condition when it fails.
To compute this, we use a version of this algorithm
that weights the time taken by each conjunct in any inner conjunction
by the probability of its execution,
which we know by comparing its execution count
with the count of the number of times the condition was entered.
\item
The goal cannot be a negation, because negated goals cannot bind variables.
\item
The goal cannot be a disjunction,
Disjunctions may only be {\dsemidet}, \dmulti or {\dnondet}.
\dsemidet code cannot be embedded in \ddet code,
which is the only type of code that we consider parallelising.
And \dmulti and \dnondet disjunctions would have to have their solutions
quantified away to be used in \ddet code,
meaning that they cannot produce any variables.
\item
If the goal is a quantification,
then the inner goal must be det,
in which case we invoke the algorithm recursively on it.
If the inner goal were not det,
then the outer quantification goal could be det
only if the inner goal did not bind any variables visible from the outside.
\end{itemize}

\noindent
Using the weighted average for switches and if-then-elses is meaningful because
the Mercury mode system dictates
that if one arm of a switch or if-then-else generates a variable,
then they \emph{all} must do so.
The sole exception is arms that are guaranteed to abort the program,
whose determinism is erroneous.
We use a weight of zero for erroneous arms.
Coverage profiling was introduced to provide the weights of switch and
if-then-else arms used by this algorithm.

The algorithm we use for computing the time
at which a shared variable is first consumed by the second conjunct
is similar to this one,
the main differences being that
negated goals, conditions and disjunctions are allowed to consume variables,
and some arms of a switch or if-then-else
may consume a variable even if other arms do not.
In switches and if-then-elses that wait on a future in some arms but
not in all,
we assume that a wait operation will be inserted at the end of each
switch arm that does not wait on a future ---
this is a conservative assumption and could prevent introducing
parallelism that leads to a slow-down.
A similar assumption may be made when a variable is not used in a goal,
such as when a procedure (followed by a call) never used one of its
arguments.
% XXX: This is not yet implemented.
The other important difference is that when making 
any other conservative assumption 
such as when the algorithm finds a higher order call,
the consumption time used is zero
(it was the cost of the call when analysing for productions) 

\subsection{Feedback framework}
\label{sec:feedback}

Automatic parallelism is just one use of profiler feedback.
Other optimisations
such as inlining,
branch hints
and type specialisation
can also benefit from profiling feedback.

We have designed and implemented a generic feedback framework that allows
tools to create feedback information for the compiler.
These tools may include the profiler, the compiler its self,
or any other tool that is able to link to the feedback library code.
Any Mercury type can be used as feedback information,
making the feedback framework very flexible.
New feedback-directed optimisations may require feedback information
from new analyses.
We expect that many new feedback information types will be added to
support these optimisations.
We also expect that an optimisation may use more than one type of
feedback information and that more than one optimisation may use the
same feedback information.
These where considered requirements of the feedback framework.

% Describe on-disk format,
The on-disc format for the feedback information is very simple,
it contains a header that identifies the file format,
including a version number.
What follows is a list of feedback information items,
these are stored in the format that Mercury uses for reading and writing
terms.
When the file is read in the file format identifier and revision
number are checked,
the list of information items is checked to ensure that no two items
describe the same feedback information,
for example,
at most one item in this list can describe how to automatically parallelise
a program.

The API allows developers to open an existing file or create a new one,
query and set information in the in-memory copy of the file,
and write the file back out to disk.
It is easy to open the file,
update a specific feedback item in it,
and close it;
essentially leaving the other feedback items in the file unchanged.



% Incorrect background, some of this prose can be used later.
%This approach also used profiling feedback data similar to Tannier's
%work.
%However, it also made use of and some of the advanced
%features of the deep profiler.
%The deep profiler was modified to export a representation of the profiled
%program.
%This representation, along with the profiling data could both be accessed by
%the analysis.
%My approach differed from Tannier's in that the analysis was done outside of the
%compiler in an analysis tool.
%The feedback given to the compiler was the result of this analysis:
%a list of candidate parallel conjunctions that the compiler would attempt to parallelise.
%Each candidate parallel conjunction describes its location in the original
%program and how its conjuncts should be constructed of smaller goals.
%However, at the completion of my honours project the implementation was incomplete
%and no feedback data was actually given to the compiler.
%Figure \ref{fig:prof_fb} closely describes the workflow for our feedback
%analysis.
%
%% call graph traversal.
%
%The auto-parallelism analysis traverses the call graph looking for parallelism
%opportunities.
%The traversal begins at the root of the call graph,
%namely \code{main/2}.
%Each node in the graph is an SCC, and each edge is a call from a call site to a
%distinct callee.
%Each procedure in an SCC is searched for parallelisation candidates before any
%child-nodes are checked (which are also SCCs).
%The search continues in this fashion until it finds an edge
%representing a call whose cost is too low such that it is not worth parallelising
%anything within the callee, and therefore, in the callee's callees.
%In practice, only a small part of the callgraph is traversed,
%since most parts of a program have a very low runtime cost and are not worth
%parallelising.
%The search will also track the number of processors available for spawned-off work
%at any point in the call graph,
%as parallelisation candidates are found, this number will decrease.
%Once it reaches zero, the search will stop as parallelising anything below this
%point in the call graph will not create any useful parallelism.
%This traversal could only be done with access to both a representation of the
%program being profiled and profiling data of the program.
%Therefore, implementing the analysis in a venerate tool was the obvious choice.
%
%Because each node in the graph is an SCC, and the graph is a tree
%the traversal does not need to worry about following cycles infinitely.
%However,
%determining the cost of recursive code is problematic.
%If a predicate calls itself, we cannot know the cost of the recursive
%call,
%this cost is attributed to the cost of the call of that predicate which
%is already active.
%The automatic parallelism analysis described in \citet{paul_hons}
%treats recursive calls naively,
%A solution to this is presented in Section
%\ref{sec:recursive_call_costs}.
