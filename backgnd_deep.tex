
\status{
    This section and 2.5 have been merged, as they provide motivation for one
    another.
    This will likly be a large section, so I've broken it into sub-sections
}

% Declaration of work done that contributed to a previous award,
Some of the work described in this section contributed towards my honours research project,
% Note that my degree is not '... with Honours', it's just 'Honours', despite
% what my degree actually says.  Basically, I've been erroneously awarded an
% extra bachelor's degree.
which was in partial forfillment of the Degree of Batchelor of Computer Science
Honours.
Most parts of the implementation were incomplete and did not work correctly or
at all.
Therefore,
their evaluation could not be included in my honours report.
They have since been completed and corrected during my Ph.D.\ candidature.
There are some exceptions,
most sagnificantly the coverage profiling transformation (Section \ref{sec:coverage})
developed for my honours research project was complete, and is used, largly
unchanged in my Ph.D.\ research.
The related analysis described in Section \ref{sec:var_use_analysis}
and the feedback framework in Section \ref{sec:feedback}
were also writen as part of my honours research project.
Still other parts of this section are not my own work,
namely the work described in
\citet{conway:2001:mercury-deep} and
\citet{tannier:2007:parallel_mercury}.

\status{Plan for this section:}

\begin{itemize}
\item Why profile directed parallelisation.
\item the deep profiler,
\item including a description of why deep profiling provides us with
      more useful data than conventional profiling.
\item Prior auto-parallelism work.
\begin{itemize}
    \item My honours thesis
\end{itemize}
\item Coverage profiling.
\item Variable use analysis
\item profiling feedback framework.
\end{itemize}

\paul{XXX: These items aren't background.}
\begin{itemize}
\item also explain the limitations of deep profiling,
      for example, that levels within recorsive code don't have seperate
      data recorded for them.
      (The solution to this will probably be in the overlap chapter)
      XXX: This problem didn't exist in my honours work, it
\item Callgraph search
\end{itemize}

Most compiler optimizations operate only on the representation of the program
in the compiler's memory.
For most optimizations this is sufficient.
However,
automatic parallelisation is sensative to variations in the runtime cost of
parallelised tasks.
This sensativity increases when dependent parallelisation is used.
For example,
a search operation on a small list is cheap, compared to the same operation on
a large list.
It may not be useful to parallelise the search on the small list against some
other computation,
but it will usually be useful to parallelise the search on the large list
against another computation.
It's important not to create too much parallelism;
overheads of parallelisation for which there is no speed up will slow the
program down.
Therefore, not just sub-optimal to parallelise the search of the small list,
but detrimental.
The only way we can know the actual cost of most peices of code,
is by understanding their typical inputs,
or measuring their runtime cost while operating on typical inputs.
Therefore,
profiling data can be used in auto-parallelisation;
it allows us to predict the runtime cost of computations whose
code is the same but whose inputs are different.
These predictions are used to drive parallelisation decisions,
which are, in turn, used to select where and how parallelism is introduced.

\picfigure{prof_fb}{Profiler feedback loop}

To use profiling data for optimizations the usual workflow for compiling
software must change.
Figure \ref{fig:prof_fb} shows a profiler-feedback workflow.
The source code is compiled, with profling enabled and the resulting program is
executed on a representative input.
As the program terminates it writes its profiling data to disk,
The profling data includes a bytecode representation of the program
similar to the compiler's representation.
The analysis tool reads this file,
and writes out a description of how to parallelise the program in the form of a
feedback file.
The program is compiled for a second time,
this time with auto-parallelism enabled.
The compiler reads the feedback file and introduces parallelism into the
resulting program.
Figure \ref{fig:prof_fb} shows this workflow.

A common criticism of profile directed optimisation is that the programmer will
have to compile the program twice,
and run it at least once to generate profiling data.
"If I have to run the program in order to optimize it, then the program has
already
done its job and there's no point continuing with the optimisation?"
The answer to this is that a program's life time is often far more than a
single execution.
A program will usually be used many times, and by many people.
Each time the optimization will have a benifit,
this benifit will pay off the cost of the feedback directed optimisation.
We expect that feedback-directed optimisations would normally be used
when the programmer is building a \emph{release candidate} version of their
program,
after they have performed testing and fixed any bugs.

Another criticism is that if the program is profiled with one set of input and
used with another that the profile will not necessarly represent the actual use
of the program,
and that the optimisation may not be as good as it could be.
There are many different types of programs, and therefore many different
program inputs;
we cannot hope to address this particular problem directly.
Selecting a typical input for a given program must be done by the programmer.
Other stratergies exist that avoid this problem
such as dynamic recompilation.

\subsection{The deep profiler}

Typical Mercury programs make heavy use of code reuse in the form of
parametric polymorphism and higher order calls.
This is also true for other declarative languages.
For example, while a C program may have
separate tables for different kinds of entities,
for whose access functions
the profiler would gather separate performance data,
most Mercury programs would use
the same polymorphic code to handle all those tables,
making the task of disentangling the characteristics of the different tables
infeasibly hard.

This problem has been solved for Mercury by introducing deep profiling
\citep{conway:2001:mercury-deep}.
Mercury's deep profiler gathers much more information about the context of
each measurement than traditional profilers like \code{gprof}\cite{gprof}.
When it records the occurrence of a call,
a memory allocation or a profiling clock interrupt,
it records with it the chain of ancestor calls,
all the way from the current call to the entry point of the program,
a procedure named \code{main}.
To make this tractable,
recursive and mutually recursive calls,
known as \emph{strongly connected components} (SCCs),
must be folded into a single memory structure.
Therefore, the call graph of the program is a tree (there are no cycles)
and SCCs are represented as single nodes in this tree.

Deep profiling allows the profiler to find and present to the user
not just information such as the total number of calls to a procedure
and the average cost of a call,
or even information such as the total number of calls to a procedure
from a particular call site and the average cost of a call from that call site,
but also information such as the total number of calls to a procedure
\emph{from a particular call site
when invoked from a particular chain of ancestor SCCs}
and the average cost of a call \emph{in that context}.
For example, it could tell that
procedure $h$ called procedure $i$ ten times
when $h$'s chain of ancestors was $main \calls f \calls h$,
while $h$ called $i$ only seven times
when $h$'s chain of ancestors was $main \calls g \calls h$,
the calls from $h$ to $i$ took on average twice as long
from the $main \calls g \calls h$ context as from $main \calls f \calls h$,
so that despite the fewer calls,
$main \calls g \calls h \calls i$ took more time than $main \calls f \calls h \calls i$.
\paul{Draw a figure}

Profilers have traditionally measured time
by sampling the program counter at clock interrupts.
Unfortunately, even on modern machines
the usual, and portable infrastructure for clock interrupts
(\emph{e.g}., SIGPROF on Unix)
supports only one frequency for such interrupts,
which is usually 60 or 100Hz.
This frequency is far too low for the kind of detailed measurements
the Mercury deep profiler wants to make,
since for typical program runs of few seconds,
it results in almost all calls having a recorded time of zero,
with the calls recording a nonzero time
(signifying the occurrence of an interrupt during their execution)
being selected almost at random.

We have therefore implemented a finer-grained measure of time
that turned out to be very useful
even though it is inherently approximate.
This measure is call sequence counts (CSCs):
the profiled program basically behaves
as if the occurrence of a call signified
the occurrence of a new kind of profiling interrupt.
In imperative programs, this would be a horrible measure,
since calls to different functions can have hugely different runtimes.
However, in declarative language like Mercury there are no explicit loops;
what a programmer would do with a loop in an imperative language
must be done by a recursive call.
This means that the only thing that the program can execute between two calls
is a sequence of primitive operations such as unifications and arithmetic.
For any given program,
there is a strict upper bound on the maximum length of such sequences,
and the distribution of the length of such sequences
is very strongly biased towards very short sequences
of half-a-dozen to a dozen operations.
In practice, we have found that
the fluctuations between the lengths of different such sequences
can be ignored for any measurement
that covers any significant number of call sequence counts,
say more than a hundred.
The only drawback of this scheme that we have found
is that on 32 bit platforms,
its usability is limited to short program runs (a few seconds)
by the wraparound of the global CSC counter;
on 64 bit platforms, the problem would occur
only on a profiling run that lasts for years.

\subsection{Prior Auto-parallelism Work}

\citet{tannier:2007:parallel_mercury} describes a prior attempted at automatic
parallelism in Mercury.
In this approch, the parallelisation feedback data was gathered by an analysis
tool,
this feedback was a list of the procedures with the highest cost in the program,
these were choosen by a configurable measurement and threshold.
The analysis did not use a representation of the program,
as at that time the profiler did not support that capability.
Nor did it use any advanced features of the profiler.
Parallelisation decisions were made in the compiler:
each procedure was searched for calls that appeared in the list of top procedures
and calls to these where parallelised against similar calls if they
where independent or had fewer than some $N$ shared variables.

\paul{XXX:  'my' or 'our' in this work.  I know we usually use plural
first person in papers,
Is this true for dissertations as well?}
For my honours project I attempted automatic parallelism of Mercury
\citep{paul_hons}.
There where a number of differences between my work and Tannier's.
The first differences is that at the time of my work the deep profiler
was able to export a representation of the program
(my work was the motivation for this feature).
This allowed my analysis to access profiling data and a representation
of the program at the same time.
This allows the analysis to measure when variables are produced and
consumed within parallel conjuncts,
and therefore calculate,
for conjunctions with a single shared variable,
how much parallelism is available.
The formula used was:

\begin{eqnarray*}
T_{Sequential} & = & T_A + T_B \\
T_{DependantB} & = & max(T_{BeforeProduceA}, T_{BeforeConsumeB}) +
T_{AfterConsumeB} \\
T_{Parallel} & = & max(T_A, T_{DependantB}) + T_{Overheads} \\
Speedup & = & \frac{T_{Sequntial}}{T_{Parallel}}
%\label{eqn:time_deppar}
\end{eqnarray*}

This describes the speedup due to parallelism of two conjuncts, $A$ and $B$.
whose secution times are $T_A$ and $T_B$.
In the parallel case $B$'s execution time is $T_{DependantB}$ since it
accounts for the $B$'s dependency on $A$.
Except for this small difference,
the calculation of parallel execution time and speedup are the commonly
used formulas for many parallel execution cost models.

This cost model requires information about when a shared variable is
produced by the first conjunct and consumed by the second.
To provide this information we introduced coverage profiling to
Mercury's deep profiler,
and used the resulting coverage data in a variable use time analysis.

\subsection{Coverage Profiling}
\label{sec:coverage}

The deep profiler records port counts for predicates.
The traditional port model~\cite{port_model} describes each predicate as
having four ports through which control may flow.
Control can flow into a predicate through either the call or redo ports,
control flows out of a predicate through either the exit or fail ports.
These describe:
    a call being made to the predicate,
    a call to the predicate being re-entered because there may be more
    solutions,
    the predicate exiting with a solution,
    or the predicate failing because there are no more solutions.
Mercury adds an extra port to these semantics known as the exception port,
which a predicate uses to return control to its caller when it throws an
exception.
The following invarient must hold,
control must leave a predicate the same number of times that it enters.

\begin{equation*}
Calls + Redos = Exits + Fails + Exceptions
\end{equation*}

\noindent
Mercury adds additional invariants for many determinism types.
For example, determinisitic code may not Redo or Fail.
The deep profiler tracks the number of times each port is used for a given
call.
This data can be uesd to measure code coverage in a Mercury program.

However, port counts alone will not provide complete coverge information.
We introduced coverage points to address this problem.
When the compiler instruments the program for deep profiling it will make
a forward pass through the goal tree of each predicate.
During this pass it will track whether or not it will be possible to
calculate the coverage of the current goal based on those that came before
it.
For example, a unifcation (which does not have any port counts of its own)
that follows a call will be called the same number of times as the call
exits.
If the unification is determinstic then it will also have the same number of
exits.
Coverage points are introduced often at the beginning of braches such as
switches --- but never in the last case of a swich whose coverage can be
inferred from the others.
They're also inserted after any goal whose exit port count is unknown.
Each coverage point referrs to a counter which is incremented each time the
coverage point is executed.
These counters are not associated with context the way other performance
data in Mercury is.
Therefore,
coverage data of a particular predicate covers the general use of that
predicate,
The auto-parallelisation work described in my honours report
\citep{pbone_hons}
would not have benifited from context specific coverage data.

\begin{figure}
\begin{verbatim}
\code{map(P, Xs0, Ys) :-} \\
\code{~~~~(} \\
\code{~~~~~~~~}\instr{coverage\_point(ProcStatic, 0);} \\
\code{~~~~~~~~Xs0 = [],} \\
\code{~~~~~~~~Ys = []} \\
\code{~~~~;
\code{~~~~~~~~Xs0 = [X $|$ Xs],} \\
\code{~~~~~~~~P(X, Y),} \\
\code{~~~~~~~~map(P, Xs, Ys0),} \\
\code{~~~~~~~~Ys = [Y $|$ Ys0]} \\
\code{~~~~).} \\
\end{verbatim}
\caption{Coverage annotated \code{map/3}.}
\label{fig:map_coverage}
\end{figure}

\paul{Explain changes to data structures.}

Figure \ref{fig:map_coverage} shows \code{map/3} instrumented for coverage
profiling.
There is a single coverage point inserted in the beginning of the first
case of the switch on the list.
The call to the instrumetnation code 
refers to the ProcStatic structure,
and an index into the coverage point array contained in it.

a structure from the deep profiler where information about this procedure is
stored.
\paul{XXX Write about ProcStatic and other structures in the deep profiler
section.}


\paul{XXX: Add example}

\status{These two subsections are the only ones not written in this chapter
now.}

\subsection{Variable Use Time Analysis}
\label{sec:var_use_analysis}


% Incorrect background, some of this prose can be used later.
%This approch also used profiling feedback data similar to Tannier's
%work.
%However, it also made use of and some of the advanced
%features of the deep profiler.
%The deep profiler was modified to export a representation of the profiled
%program.
%This representation, along with the profiling data could both be accessed by
%the analysis.
%My approch differed from Tannier's in that the analysis was done outside of the
%compiler in an analysis tool.
%The feedback given to the compiler was the result of this analysis:
%a list of candidate parallel conjunctions that the compiler would attempt to parallelise.
%Each candidate parallel conjunction describes its location in the origianal
%program and how its conjuncts should be constructed of smaller goals.
%However, at the completion of my honours project the implementation was incomplete
%and no feedback data was actually given to the compiler.
%Figure \ref{fig:prof_fb} closely describes the workflow for our feedback
%analysis.
%
%% call graph traversal.
%
%The auto-parallelism analysis traverses the call graph looking for parallelism
%opertunities.
%The traversal begins at the root of the call graph,
%namely \code{main/2}.
%Each node in the graph is an SCC, and each edge is a call from a call site to a
%distinct callee.
%Each procedure in an SCC is searched for parallelisation candidates before any
%child-nodes are checked (which are also SCCs).
%The search continues in this fassion until it finds an edge
%representing a call whose cost is too low such that it is not worth parallelising
%anything within the callee, and therefore, in the callee's callees.
%In practice, only a small part of the callgraph is traversed,
%since most parts of a program have a very low runtime cost and are not worth
%parallelising.
%The search will also track the number of processors available for spawned-off work
%at any point in the call graph,
%as parallelisation candidates are found, this number will decrease.
%Once it reaches zero, the search will stop as parallelising anything below this
%point in the call graph will not create any useful parallelism.
%This traversal could only be done with access to both a representation of the
%program being profiled and profiling data of the program.
%Therefore, implementing the analysis in a seperate tool was the obvious choice.
%
%Because each node in the graph is an SCC, and the graph is a tree
%the traversal does not need to worry about following cycles infinitly.
%However,
%determining the cost of recursive code is problematic.
%If a predicate calls itsself, we cannot know the cost of the recursive
%call,
%this cost is attributed to the cost of the call of that predicate which
%is already active.
%The automatic parallelism analysis described in \citet{paul_hons}
%treats recursive calls naively,
%A solution to this is presented in Section
%\ref{sec:recursive_call_costs}.


\subsection{Feedback framework}
\label{sec:feedback}

Automatic parallelism is just one use of profiler feedback.
Other optimisations
such as inlining,
branch hints
and type specialisation
can also benefit from profiling feedback.

We have designed and implemented a generic feedback framework that allows
tools to create feedback information for the compiler.
These tools may include the profiler, the compiler its self,
or any other tool that is able to link to the feedback library code.
Any Mercury type can be used as feedback information,
making the feedback framework very flexible.
New feedback-directed optimisations may require feedback information
from new analyses.
We expect that many new feedback information types will be added to
support these optimisations.
We also expect that an optimisation may use more than one type of
feedback information and that more than one optimisation may use the
same feedback information.
These where considered requirements of the feedback framework.

% Describe on-disk format,
The on-disc format for the feedback information is very simple,
it contains a header that identifies the file format,
including a version number.
What follows is a list of feedback information items.
When the file is read in the file format identifier and revision
number are checked,
the list of information items is checked to ensure that no two items
describe the same feedback information,
for example,
at most one item in this list can describe how to automatically parallelise
a program.

\paul{Should I explain the API in more detail.}

