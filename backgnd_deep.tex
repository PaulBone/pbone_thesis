
\status{
    This section and 2.5 have been merged, as they provide motivation for one
    another.
    This will likly be a large section, so I've broken it into sub-sections
}

% Declaration of work done that contributed to a previous award,
Some of the work described in this section contributed towards my honours research project,
% Note that my degree is not '... with Honours', it's just 'Honours', despite
% what my degree actually says.  Basically, I've been erroneously awarded an
% extra bachelor's degree.
which was in partial forfillment of the Degree of Batchelor of Computer Science
Honours.
Most parts of the implementation were incomplete and did not work correctly or
at all.
Therefore,
their evaluation could not be included in my honours report.
They have since been completed and corrected during my Ph.D.\ candidature.
There are some exceptions,
most sagnificantly the coverage profiling transformation (Section \ref{sec:coverage})
developed for my honours research project was complete, and is used, largly
unchanged in my Ph.D.\ research.
The other exceptions are the ideas within the variable use analsys code
(Section \ref{sec:var_use_analysis} and
the call graph traversal code \ref{sec:call_graph_traversal}.
Still other parts of this section are not my own work, namely
\citet{tannier:2007:parallel_mercury} and \citet{conway:2001:mercury-deep}.

\status{Plan for this section:}

\begin{itemize}
\item Why profile directed parallelisation.
\item the deep profiler,
\item including a description of why deep profiling provides us with
      more useful data than conventional profiling.
\item also explain the limitations of deep profiling,
      for example, that levels within recorsive code don't have seperate
      data recorded for them.
      (The solution to this will probably be in the overlap chapter)
\item Prior auto-parallelism work.
\begin{itemize}
    \item My honours thesis
\end{itemize}
\item Callgraph search
\item Variable use analysis
\item Motivation for coverage profiling (variable production times) (prereq:
      varuse analysis)
\item Coverage profiling. 
\item profiling feedback framework.
\end{itemize}

Most compiler optimizations operate only on the representation of the program
in the compiler's memory.
For most optimizations this is sufficient.
However,
automatic parallelisation is sensative to variations in the runtime cost of
parallelised tasks.
This sensativity increases when dependent parallelisation is used.
For example,
a search operation on a small list is cheap, compared to the same operation on
a large list.
It may not be useful to parallelise the search on the small list against some
other computation,
but it will usually be useful to parallelise the search on the large list
against another computation.
It's important not to create too much parallelism;
overheads of parallelisation for which there is no speed up will slow the
program down.
Therefore, not just sub-optimal to parallelise the search of the small list,
but detrimental.
The only way we can know the actual cost of most peices of code,
is by understanding their typical inputs,
or measuring their runtime cost while operating on typical inputs.
Therefore,
profiling data can be used in auto-parallelisation;
it allows us to predict the runtime cost of computations whose
code is the same but whose inputs are different.
These predictions are used to drive parallelisation decisions,
which are, in turn, used to select where and how parallelism is introduced.

\picfigure{prof_fb}{Profiler feedback loop}

To use profiling data for optimizations the usual workflow for compiling
software must change.
Figure \ref{fig:prof_fb} shows a profiler-feedback workflow.
The source code is compiled, with profling enabled and the resulting program is
executed on a representative input.
As the program terminates it writes its profiling data to disk,
The profling data includes a bytecode representation of the program
similar to the compiler's representation.
The analysis tool reads this file,
and writes out a description of how to parallelise the program in the form of a
feedback file.
The program is compiled for a second time,
this time with auto-parallelism enabled.
The compiler reads the feedback file and introduces parallelism into the
resulting program.
Figure \ref{fig:prof_fb} shows this workflow.

A common criticism of profile directed optimisation is that the programmer will
have to compile the program twice,
and run it at least once to generate profiling data.
"If I have to run the program in order to optimize it, then the program has
already
done its job and there's no point continuing with the optimisation?"
The answer to this is that a program's life time is often far more than a
single execution.
A program will usually be used many times, and by many people.
Each time the optimization will have a benifit,
this benifit will pay off the cost of the feedback directed optimisation.
Another criticism is that if the program is profiled with one set of input and
used with another that the profile will not necessarly represent the actual use
of the program,
and that the optimisation may not be as good as it could be.
There are many different types of programs, and therefore many different
program inputs;
we cannot hope to address this particular problem directly.
Selecting a typical input for a given program must be done by the programmer.
Other stratergies exist that avoid this problem
such as dynamic recompilation.

\subsection{The deep profiler}

Typical Mercury programs make heavy use of code reuse in the form of
parametric polymorphism and higher order calls.
This is also true for other declarative languages.
For example, while a C program may have
separate tables for different kinds of entities,
for whose access functions
the profiler would gather separate performance data,
most Mercury programs would use
the same polymorphic code to handle all those tables,
making the task of disentangling the characteristics of the different tables
infeasibly hard.

This problem has been solved for Mercury by introducing deep profiling
\citep{conway:2001:mercury-deep}.
Mercury's deep profiler gathers much more information about the context of
each measurement than traditional profilers like \code{gprof}\cite{gprof}.
When it records the occurrence of a call,
a memory allocation or a profiling clock interrupt,
it records with it the chain of ancestor calls,
all the way from the current call to the entry point of the program,
a procedure named \code{main}.
To make this tractable,
recursive and mutually recursive calls,
known as \emph{strongly connected components} (SCCs),
must be folded into a single memory structure.
Therefore, the call graph of the program is a tree (there are no cycles)
and SCCs are represented as single nodes in this tree.

Deep profiling allows the profiler to find and present to the user
not just information such as the total number of calls to a procedure
and the average cost of a call,
or even information such as the total number of calls to a procedure
from a particular call site and the average cost of a call from that call site,
but also information such as the total number of calls to a procedure
\emph{from a particular call site
when invoked from a particular chain of ancestor SCCs}
and the average cost of a call \emph{in that context}.
For example, it could tell that
procedure $h$ called procedure $i$ ten times
when $h$'s chain of ancestors was $main \calls f \calls h$,
while $h$ called $i$ only seven times
when $h$'s chain of ancestors was $main \calls g \calls h$,
the calls from $h$ to $i$ took on average twice as long
from the $main \calls g \calls h$ context as from $main \calls f \calls h$,
so that despite the fewer calls,
$main \calls g \calls h \calls i$ took more time than $main \calls f \calls h \calls i$.
\paul{Draw a figure}

Profilers have traditionally measured time
by sampling the program counter at clock interrupts.
Unfortunately, even on modern machines
the usual, and portable infrastructure for clock interrupts
(\emph{e.g}., SIGPROF on Unix)
supports only one frequency for such interrupts,
which is usually 60 or 100Hz.
This frequency is far too low for the kind of detailed measurements
the Mercury deep profiler wants to make,
since for typical program runs of few seconds,
it results in almost all calls having a recorded time of zero,
with the calls recording a nonzero time
(signifying the occurrence of an interrupt during their execution)
being selected almost at random.

We have therefore implemented a finer-grained measure of time
that turned out to be very useful
even though it is inherently approximate.
This measure is call sequence counts (CSCs):
the profiled program basically behaves
as if the occurrence of a call signified
the occurrence of a new kind of profiling interrupt.
In imperative programs, this would be a horrible measure,
since calls to different functions can have hugely different runtimes.
However, in declarative language like Mercury there are no explicit loops;
what a programmer would do with a loop in an imperative language
must be done by a recursive call.
This means that the only thing that the program can execute between two calls
is a sequence of primitive operations such as unifications and arithmetic.
For any given program,
there is a strict upper bound on the maximum length of such sequences,
and the distribution of the length of such sequences
is very strongly biased towards very short sequences
of half-a-dozen to a dozen operations.
In practice, we have found that
the fluctuations between the lengths of different such sequences
can be ignored for any measurement
that covers any significant number of call sequence counts,
say more than a hundred.
The only drawback of this scheme that we have found
is that on 32 bit platforms,
its usability is limited to short program runs (a few seconds)
by the wraparound of the global CSC counter;
on 64 bit platforms, the problem would occur
only on a profiling run that lasts for years.

\subsection{Prior Auto-parallelism Work}

\citet{tannier:2007:parallel_mercury} describes a prior attempted at automatic
parallelism in Mercury.
In this approch, the parallelisation feedback data was gathered by an analysis
tool,
this feedback was a list of the procedures with the highest cost in the program,
these were choosen by a configurable measurement and threshold.
The analysis did not use a representation of the program,
as at that time the profiler did not support that capability.
Nor did it use any advanced features of the profiler.
Parallelisation decisions were made in the compiler:
each procedure was searched for calls that appeared in the list of top procedures
and calls to these where parallelised against similar calls
provided that some other critera not important here was true.

For my honours project I attempted automatic parallelism of Mercury
\citep{paul_hons}.
This approch also used profiling feedback data and some of the advanced
features of the deep profiler.
The deep profiler was modified to export a representation of the profiled
program.
This representation, along with the profiling data could both be accessed by
the analysis.
My approch differed from Tannier's in that the analysis was done outside of the
compiler in an analysis tool.
The feedback given to the compiler was the result of this analysis:
a list of candidate parallel conjunctions that the compiler would attempt to parallelise.
Each candidate parallel conjunction describes its location in the origianal
program and how its conjuncts should be constructed of smaller goals.
However, at the completion of my honours project the implementation was incomplete
and no feedback data was actually given to the compiler.
Figure \ref{fig:prof_fb} closely describes the workflow for our feedback
analysis.

% call graph traversal.

The auto-parallelism analysis traverses the call graph looking for parallelism
opertunities.
The traversal begins at the root of the call graph,
namely \code{main/2}.
Each node in the graph is an SCC, and each edge is a call from a call site to a
distinct callee.
Each procedure in an SCC is searched for parallelisation candidates before any
child-nodes are checked (which are also SCCs).
The search continues in this fassion until it finds an edge
representing a call whose cost is too low such that it is not worth parallelising
anything within the callee, and therefore, in the callee's callees.
In practice, only a small part of the callgraph is traversed,
since most parts of a program have a very low runtime cost and are not worth
parallelising.
The search will also track the number of processors available for spawned-off work
at any point in the call graph,
as parallelisation candidates are found, this number will decrease.
Once it reaches zero, the search will stop as parallelising anything below this
point in the call graph will not create any useful parallelism.
This traversal could only be done with access to both a representation of the
program being profiled and profiling data of the program.
Therefore, implementing the analysis in a seperate tool was the obvious choice.

\status{XXX Currently here}

Telling the compiler what to parallelise is just one use of profiler feedback.
Other optimisations (such as inlining) could also benefit
from profiling feedback.

Using a feedback-directed optimisation requires that the user compile
and run their program before feedback information is produced, 
which takes a significant amount of time.
Because of this, feedback-directed optimisations do not make sense
unless
the total execution time of the optimised program --- which depends
mostly on how many times it will be executed --- is significantly
greater than the time it takes to compile and execute the profiling
version of the program.
We expect that feedback-directed optimisations would normally be used
when the programmer is building a \emph{release candidate} version of their
program, after they have performed testing and fixed any bugs.

In all cases the program should be profiled on an input that is
representative of the inputs the program is expected to handle when
it is used normally.
This is important to ensure the compiler makes decisions using
feedback information that improves rather than degrades the performance
of the program in general.
How a representative input is chosen and how important the exact choice
of the profiling input is will change from program to program.




% Describe data types: the feedback data can be any mercury data
% types.  One can query the feedback data by asking it to complete the
% instantiation of a type.
We have designed and implemented a generic feedback framework that allows
tools (both inside or outside the compiler)
to create feedback information for the compiler.
The feedback information can be expressed as any Mercury type, making the
feedback framework very flexible.
New feedback-directed optimisations may require feedback information
from new analyses.
We expect that many new feedback information types will be added to
support these optimisations.
We also expect that an optimisation may use more than one type of
feedback information and that more than one optimisation may use the
same feedback information.

% Describe on-disk format,
The on-disc format of the feedback information file is very simple, it
contains a header that identifies the file format, including a version number.
What follows is a list of feedback information items.
When the file is read in the file format identifier and revision
number are checked,
the list of information items is checked to ensure that no two items
describe the same feedback information, for example there can only be at
most one item in this list that describes how to automatically
parallelise a program.

