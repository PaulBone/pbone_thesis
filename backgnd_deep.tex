
\status{
    This section and 2.5 have been merged, as they provide motivation for one
    another.
    This will likely be a large section, so I have broken it into sub-sections
}

% Declaration of work done that contributed to a previous award,
Some of the work described in this section contributed towards my honours research project,
% Note that my degree is not '... with Honours', it is just 'Honours', despite
% what my degree actually says.  Basically, I have been erroneously awarded an
% extra bachelor's degree.
which was in partial fulfilment of the Degree of Bachelor of Computer Science
Honours.
These contributions have been improved and in some places corrected during
my Ph.D.\ candidature.
The following where part of my honours project:
the coverage profiling transformation (Section \ref{sec:coverage}),
the related variable use analysis (Section \ref{sec:var_use_analysis})
and the feedback framework (Section \ref{sec:feedback}).
Some other parts of this section are not my own work:
the Mercury deep profiler \citep{conway:2001:mercury-deep} (Section
\ref{sec:deep}) and
\citet{tannier:2007:parallel_mercury}'s work on automatic parallelism
(Section \ref{sec:tannier}).

Most compiler optimisations work on the representation of the program in
the compiler's memory alone.
For most optimisations this is enough.
However,
automatic parallelisation is sensitive to variations in the runtime cost of
parallelised tasks.
This sensitivity increases when dependent parallelisation is used.
For example,
a search operation on a small list is cheap, compared with the same operation on
a large list.
It may not be useful to parallelise the search on the small list against some
other computation,
but it will usually be useful to parallelise the search on the large list
against another computation.
It is important not to create too much parallelism:
The hardware is limited in how many parallel tasks it can execute,
any more and the overheads of parallel execution will slow the program down.
Therefore, it is not just sub-optimal to parallelise the search of the small list,
but detrimental.
The only way we can know the actual cost of most pieces of code
is by understanding their typical inputs,
or measuring their runtime cost while operating on typical inputs.
Therefore,
profiling data should be used in auto-parallelisation;
it allows us to predict the runtime cost of computations whose
code is the same but whose inputs are different.
These predictions are used to drive parallelisation speedup analysis,
which is, in turn, used to select where and how parallelism is introduced.

\picfigure{prof_fb}{Profiler feedback loop}

To use profiling data for optimisations the usual workflow for compiling
software must change.
The source code is compiled with profiling enabled, and the resulting program is
executed on a representative input.
As the program terminates,
it writes its profiling data to disk.
The profiling data includes a bytecode representation of the program
similar to the compiler's representation.
The analysis tool reads this file,
and writes out a description of how to parallelise the program as a feedback
file.
The program is compiled for a second time,
this time with auto-parallelism enabled.
The compiler reads the feedback file and introduces parallelism into the
resulting program.
Figure \ref{fig:prof_fb} shows this workflow.

A common criticism of profile directed optimisation is that the programmer will
have to compile the program twice,
and run it at least once to generate profiling data.
``If I have to run the program to optimise it, then the program has
already done its job,
therefore there is no point continuing with the optimisation?''
The answer to this is that a program's lifetime is often far more than a
single execution.
A program will usually be used many times, and by many people.
Each time the optimisation will have a benefit,
this benefit will pay off the cost of the feedback directed optimisation.
We expect that feedback-directed optimisations would normally be used
when the programmer is building a release candidate version of their
program,
after they tested the software and fixed any bugs.

Another criticism is that if the program is profiled with one set of input and
used with another that the profile will not necessarily represent the actual use
of the program,
and that the optimisation may not be as good as it should be.
Many different types of programs exist, each with many different inputs;
we cannot hope to address this particular problem directly.
Selecting a typical input for a given program must be done by the programmer.
Other strategies exist to avoid this problem,
such as dynamic recompilation.

\subsection{The deep profiler}
\label{sec:backgnd_deep}

Typical Mercury programs make heavy use of code reuse,
especially through parametric polymorphism and higher order calls.
This is also true for other declarative languages.
For example, while a C program may have
separate tables for different kinds of entities,
for whose access functions
the profiler would gather separate performance data,
most Mercury programs would use
the same polymorphic code to handle all those tables,
making the task of disentangling the characteristics of the different tables
infeasibly hard.

This problem has been solved for Mercury by introducing deep profiling
\citep{conway:2001:mercury-deep}.
Mercury's deep profiler gathers much more information about the context of
each measurement than traditional profilers such as \code{gprof} \citep{gprof} do.
When it records the event of a call,
a memory allocation or a profiling clock interrupt,
it records with it the chain of ancestor calls,
all the way from the current call to the entry point of the program,
a procedure named \code{main}.
To make this tractable,
recursive and mutually recursive calls,
known as \emph{strongly connected components} (SCCs),
must be folded into a single memory structure.
Therefore, the call graph of the program is a tree (it has no cycles)
and SCCs are represented as single nodes in this tree.

Deep profiling allows the profiler to find and present to the user
not just information such as the total number of calls to a procedure
and the average cost of a call,
or even information such as the total number of calls to a procedure
from a particular call site and the average cost of a call from that call site,
but also information such as the total number of calls to a procedure
\emph{from a particular call site
when invoked from a particular chain of ancestor SCCs}
and the average cost of a call \emph{in that context}.
For example, it could tell that
procedure $h$ called procedure $i$ ten times
when $h$'s chain of ancestors was $main \calls f \calls h$,
while $h$ called $i$ only seven times
when $h$'s chain of ancestors was $main \calls g \calls h$,
the calls from $h$ to $i$ took on average twice as long
from the $main \calls g \calls h$ context as from $main \calls f \calls h$,
so that despite making fewer calls to $i$,
$main \calls g \calls h \calls i$ took more time than $main \calls f \calls h \calls i$.
\paul{Draw a figure.}

% XXX: I considered moving this to the Mercury section, but if I
% introduce it here then it is right next to its use, which is better.
% I also considered a larger example such as the quadratic equation, but
% it is not necessary.
During compilation, the arguments of functors and predicate calls are
simplified so that they are single variables.
For instance \code{foo(a(3), X)} is simplified to:

\begin{verbatim}
V_1 = 3,
V_2 = a(X1),
foo(V_2, X)
\end{verbatim}

\noindent
This new conjunction is flattened into the conjunction that contained the
original expression.
Code flattened in this way is said to be in
\emph{super homogeneous form}.
This often creates many small conjuncts from just a single expression,
for instance, the quadratic equation
($x=\frac{-b \pm \sqrt {b^2-4ac}}{2a}$), which can be written on one line,
is flattened into 12 individual conjuncts (excluding the plus-or-minus).

Therefore the profiler
(and other tools such as the debugger) introduced \emph{goal paths}
to uniquely identify a sub goal in any goal tree such as a procedure's
body.
A goal path is a list of goal path steps, each step describes which
sub-goal to recurse into when traversing a goal structure from its root
to either its leaves or some compound goal.
The goal path ``c3;d2'' refers to the second disjunct ``d2'' within the
third conjunct ``c3''.
Goal paths where first introduced to support debugging in Mercury \citep{mdb}.
The deep profiler was written after the debugger and was able to re-use them.

Four structures are used during profiling,
each one is also reflected in the profiling data.

\begin{description}

    \item[\PD]
    represents a procedure within the program's call tree.
    There can be multiple \PD structures for any given procedure:
    the same procedure will usually be called from a number of different
    contexts in the program.

    \item[\CSD]
    represents a call site in the program's call tree.
    As with \PD, there may be multiple \CSD structures for any given
    call site.
    Profiling for the call site \& context is stored within its \CSD
    structure.
    Each \CSD structure has one pointer to the \PD stucture representing
    its callee\footnote{
        This is not exactly true,
        there are rare circumstances where a dynamic call site can have
        multiple callees.
        However, the conditions that create this and how it is handled
        are not important.}.
    Likewise, each \PD structure has an array of pointers to the \CSD
    structures representing the calls used during the execution of the
    procedure.
    These links represent the call graph of the program.

    \item[\PS]
    represents the static data about a procedure
    This data includes the procedure's name, arity, source file and line
    number.
    \paul{Does a \PS point to its {\CSS}s?}
    Because multiple \PD structures may exist for the same procedure,
    this stucture is used to factor out common information.
    Each \PD structure has a pointer to its \PS structure.
    Each procedure in the program has a single \PS structure.

    \item[\CSS]
    represents static data for a call site.
    This includes the procedure the call site is found in, the line number,
    the goal path and a pointer to the \PS of the callee (if known
    statically).
    \paul{Does a \CSS really point to its \PS?}
    As with \PS structures,
    \CSS structures reduce the memory usage of \CSD structures by
    factoring out common information.
    Each \CSD structure has a pointer to its \CSS structure.
    Each call site in the program has a single \CSS structure.

\end{description}

\noindent
When the profiling data file is read,
the profiling tool will build the call graph and do a topological
sort.
The result of the sort is a list of SCCs
(which \citet{conway:2001:mercury-deep} call \emph{cliques})
each SCC is used to create a \Clique data structure.
Several indexes are also constructed.
The indexes, {\Clique}s, {\PS}s, {\PD}s, {\CSS}s and {\CSD}s are used to
efficiently traverse the program's profile in both
the interactive user interface,
and automated tools, such as the automatic parallelism analysis.

% Callseqs
Profilers have traditionally measured time
by sampling the program counter at clock interrupts.
Unfortunately, even on modern machines,
the usual portable infrastructure for clock interrupts
(\emph{e.g}., SIGPROF on Unix)
supports only one frequency for such interrupts,
which is usually 60 or 100Hz.
This frequency is far too low for the kind of detailed measurements
the Mercury deep profiler wants to make.
For typical program runs of few seconds,
it results in almost all calls having a recorded time of zero,
with the calls recording a nonzero time
(signifying an interrupt during their execution)
being selected almost at random.

We have therefore implemented a finer-grained measure of time
that turned out to be very useful
even though it is inherently approximate.
This measure is \emph{call sequence counts} (CSCs):
the profiled program basically behaves
as if the occurrence of a call signified
the occurrence of a new kind of profiling interrupt.
In imperative programs, this would be a horrible measure,
since calls to different functions often have hugely different runtimes.
However, in declarative languages like Mercury there are no explicit loops;
what a programmer would do with a loop in an imperative language
must be done by a recursive call.
This means that the only thing that the program can execute between two calls
is a sequence of primitive operations such as unifications and arithmetic.
For any given program,
there is a strict upper bound on the maximum length of such sequences,
and the distribution of the length of such sequences
is very strongly biased towards very short sequences
of half-a-dozen to a dozen operations.
In practice, we have found that
the fluctuations in the lengths of different sequences
can be ignored for any measurement
that covers any significant number of call sequence counts,
say more than a hundred.
The only drawback of this scheme that we have found
is that on 32 bit platforms,
its usability is limited to short program runs (a few seconds)
by the wraparound of the global CSC counter;
on 64 bit platforms, the problem would only occur
on a profiling run that lasted for years.

\subsection{Prior Auto-parallelism Work}

\citet{tannier:2007:parallel_mercury} describes a prior attempted at automatic
parallelism in Mercury.
In his approach, the parallelisation feedback data was gathered by an analysis
tool.
This feedback was a list of the procedures with the highest cost in the program,
chosen by a configurable measurement (mean or median of time in CSCs) and threshold.
The analysis did not use a representation of the program,
as at that time the profiler did not support that capability.
Nor did it use any advanced features of the profiler.
Parallelisation decisions were made in the compiler:
each procedure was searched for calls that appeared in the list of top procedures
and calls to these where parallelised against similar calls if they
where independent or had fewer shared variables than another configurable limit.

\paul{XXX:  'my' or 'our' in this work.  I know we usually use plural
first person in papers,
Is this true for dissertations as well?}
For my honours project I attempted automatic parallelism of Mercury
\citep{bone:2008:hons}.
There are a number of differences between my work and Tannier's.
The first differences is that at the time of my work the deep profiler
was able to export a representation of the program
(my work was the motivation for this feature).
This allowed my analysis to access profiling data and a representation
of the program at the same time;
making it possible to measure to the times at which variables are produced
and consumed within parallel conjuncts.
The analysis uses this to calculate how much parallelism is available,
for conjunctions with a single shared variable.
\paul{Draw an overlap diagram.}
The formula used was:

\begin{eqnarray*}
T_{Sequential} & = & T_A + T_B \\
T_{DependentB} & = & max(T_{BeforeProduceA}, T_{BeforeConsumeB}) +
T_{AfterConsumeB} \\
T_{Parallel} & = & max(T_A, T_{DependentB}) + T_{Overheads} \\
Speedup & = & \frac{T_{Sequential}}{T_{Parallel}}
%\label{eqn:time_deppar}
\end{eqnarray*}

This describes the speedup due to parallelism of two conjuncts, $A$ and $B$.
whose execution times are $T_A$ and $T_B$.
In the parallel case $B$'s execution time is $T_{DependentB}$ since it
accounts for the $B$'s dependency on $A$.
Except for this small difference,
the calculation of parallel execution time and speedup are the commonly
used formulas for many parallel execution cost models.

This cost model requires information about when a shared variable is
produced by the first conjunct and when it is consumed by the second.
To provide this information we introduced coverage profiling to
Mercury's deep profiler (Section \ref{sec:coverage}),
and used the resulting coverage data in a variable use time analysis
(Section \ref{sec:var_use_time}).

\subsection{Coverage Profiling}
\label{sec:coverage}

Code \emph{coverage} is a term often used in the context of debugging and
testing.
It refers to which parts of a program get executed and which do not,
it is used to determine the completeness of a test-suite.
We use the same term,
often simply ``coverage'' rather than ``code coverage'',
to describe how \emph{often} a given piece of code is executed.
Thus, we have extended the concept of coverage from a binary ``has been
executed'' or ``has not been executed'' to a more informative
``has been executed $N$ times''.

The traditional Prolog box
model~\citep{box-model} describes each predicate
as having four ports through which control may flow.
Control can flow into a predicate through either the \emph{call} or
\emph{redo} ports,
control flows out of a predicate through either the \emph{exit} or
\emph{fail} ports.
These describe:
    a call being made to the predicate,
    a call to the predicate being re-entered because there may be more
    solutions,
    the predicate exiting with a solution,
    or the predicate failing because there are no more solutions.
Mercury adds an extra port to these semantics known as the
\emph{exception} port,
which a predicate uses to return control to its caller when it throws an
exception.
Control must leave a predicate the same number of times that it enters:

\begin{equation*}
Calls + Redos = Exits + Fails + Exceptions
\end{equation*}

\noindent
Mercury adds additional invariants for many determinism types.
For example, deterministic code may not redo or fail,
therefore these port counts will be zero.
The deep profiler tracks the number of times each port is used for a given
call.
Port counts provide code coverage information for predicate and call-site
entry and exit.

% XXX: Actually implement the case for inferring Elses' coverage.
Code coverage of many goals can be inferred,
in the simple case of a conjunction containing deterministic conjuncts.
Each conjunct will have the same coverage as the one before it,
(we make the assumption that the code does not throw an exception
as doing so is rare).
It follows that a deterministic conjunction containing a single call
can have complete coverage information inferred for all of its conjuncts.
Similarly,
the coverage before an \emph{if-then-else} can be inferred as the sum
of the coverage at the beginning of the \emph{then} and \emph{else}
branches,
assuming that the condition has at most one solution.
The same is true for the ends of these branches and the end of the
if-then-else.
The coverage at the beginning of the then part is also equal to the
number of times the \emph{condition} of the if-then-else succeeds.
Therefore,
in a single forward pass,
we will attempt to infer the coverage at the beginning of the branches
based on the coverage before the if-then-else and at the end of the
condition,
and the condition's determinism.
A similar inference can be made for switches,
except that they do not have a condition:
we simply infer the
coverage of the final switch branch based on the coverage of the other
branches,
and before the switch,
provided that it is a complete switch.
% A good reader might realize that we have only discussed a forward pass,
% more complicated passes are possible but the reader will have to take
% in on faith that this will be discussed later in this section.

However, port counts alone will not provide complete coverage information.
for example, a conjunction containing a sequence of \dsemidet
unifications will have incomplete coverage information:
We cannot know how often each individual unification fails.
We introduced coverage profiling as part of Mercury's deep profiler to
add extra instrumentation to gather coverage data where it would
otherwise be unknown.
At the same time, we try to reduce the amount of instrumentation used,
since extra instrumentation can slow the program down and in some cases
distort the profile.\footnote{While we use call sequence counts
instead of actually measuring time the profile
cannot be distorted.}

When the compiler instruments the program for deep profiling,
it makes a forward pass through the goal tree of each procedure.
During this pass,
a boolean value is maintained that indicates whether or not coverage
data would be available at this point in the program.
When this boolean becomes false,
the algorithm will introduce a coverage point at that place in the code
and set the value to true.
The value will be set to false after a goal with a determinism other than
\ddet or \dccmulti and which does not provide coverage data of its own,
or at the beginning of a branch where the coverage cannot be inferred.
This algorithm has the same structure as the coverage inference
algorithm:
a coverage point will always be placed where the coverage inference
algorithm would expect to find one.

\begin{figure}
\begin{tabular}{l}
\code{map(P, Xs0, Ys) :-} \\
\code{~~~~(} \\
\code{~~~~~~~~}\instr{coverage\_point(ProcStatic, 0);} \\
\code{~~~~~~~~Xs0 = [],} \\
\code{~~~~~~~~Ys = []} \\
\code{~~~~;} \\
\code{~~~~~~~~Xs0 = [X $|$ Xs],} \\
\code{~~~~~~~~P(X, Y),} \\
\code{~~~~~~~~map(P, Xs, Ys0),} \\
\code{~~~~~~~~Ys = [Y $|$ Ys0]} \\
\code{~~~~).} \\
\end{tabular}
\caption{Coverage annotated \code{map/3}.}
\label{fig:map_coverage}
\end{figure}

Figure \ref{fig:map_coverage} shows \code{map/3} annotated with a
coverage point.
The forward pass through the procedure would not be able to infer
coverage at the beginning of this switch arm,
and therefore the similar pass in the deep profiler introduced a
coverage point at that position.
This point, along with the coverage of the procedure as a whole,
is enough to infer coverage for the whole procedure on a single pass.
It is true that the coverage point could be removed and the port
counts of the recursive code in this example could be used,
but doing so would require multiple passes
for both inference and instrumentation.
Coverage profiling adds only a small cost to profiling;
it is not a high priority to optimise this further.

Coverage data is tracked in the \PS structure,
which we introduced in Section \ref{sec:backgnd_deep}.
Three new fields where added to the \PS structure:
a constant integer representing the number of coverage points in this
procedure;
an array of constant structures,
each describing the static details of a coverage point;
and an array of integers representing the number of times each coverage
point has been executed.
The coverage point instrumentation code refers to the procedure's \PS
structure and increments the coverage point's counter at the appropriate index.
The static coverage data,
which includes the goal path of the coverage point and the type of the
coverage point (branch or post-goal),
is written out with the profile.
Because coverage points are associated with \PS structures,
and not either \PD or \CSD structures,
they are not associated with contexts in the way that other profiling
data is.
That is to say,
coverage data of a particular predicate covers the general use of that
predicate,
The auto-parallelisation work described in my honours report
\citep{bone:2008:hons}
would not have benefited from context specific coverage data.

The coverage point in Figure \ref{fig:map_coverage}
has two arguments,
the first points to the \PS containing this procedure's coverage points,
the second is the index within this procedure's coverage point arrays
for this coverage point.
Although the instrumentation code appears to be a call,
it is implemented as a C preprocessor macro as it is very small and should
be inlined into the compiled procedure.


\subsection{Variable Use Time Analysis}
\label{sec:var_use_time}

As described above,
the automatic parallelism analysis used in \citet{bone:2008:hons} considers
conjunctions with at most two conjuncts and one shared variable.
To predict how much parallelism is available it needs to know
when that variable is produced by the first conjunct
and the first time it is consumed by the second.
These times are calculated in units of call sequence counts.

During compilation of dependent conjunctions,
a future is created to replace any shared variable.
The compiler will attempt to push that future into the goal that
produces the value so that,
if possible,
the \signal operation will be placed immediately after the unification
that binds the variable.

\begin{algorithm}
\paul{XXX: I want to fix the spacing between rows, where a row is larger
than a single line the spacing is wrong}
\[
\begin{array}{l @{}l @{}l}
\prodtime V\,& G                            &=
    \begin{cases}
        0                & \text{if } G \text{ is not executed or
                            \derroneous} \\
        \prodtimep V\,G  & \text{if } G \text{ is \ddet or \dccmulti} \\
        undefined        & \text{otherwise} \\
    \end{cases} \\
\end{array}
\]
\[
\begin{array}{l @{}l @{}l}
\prodtimep V\,& X = Y                        &= 0 \\
\prodtimep V\,& X = f(\ldots)                &= 0 \\
\prodtimep V\,& p(X_1,\,\ldots,\,X_n)        &=
    \callprodtime V\,p\,[X_1,\,\ldots,\,X_n] \\
%    \prodtime V\,(\operatorname{body} p) \\
\prodtimep V\,& X_{n+1} = \,f(X_1,\,\ldots,\,X_n)  &=
    \callprodtime V\,f\,[X_1,\,\ldots,\,X_n,\,X_{n+1}] \\
%    \prodtime V\,(\operatorname{body} f) \\
\prodtimep V\,& X_0(X_1,\,\ldots,\,X_n)      &= time\_of\_call \\
\prodtimep V\,& m(X_1,\,\ldots,\,X_n)        &= time\_of\_call \\
\prodtimep V\,& foreign(\ldots)              &= time\_of\_call + 1 \\
\prodtimep V\,& (G_1,\,G_2,\,\ldots,\,G_n)   &=
   \begin{cases}
       \prodtime V\,G_1
            & \text{if } G_1 \text{ binds } V \\
       \begin{split}
           \timef G_1 + \\ \prodtime V\,(G_2,\,\ldots,\,G_n)
       \end{split}
            & \text{otherwise}
   \end{cases} \\
\prodtimep V\,& (G_1\,\&\,\ldots\,\&\,G_n)   &= \undef \\
\prodtimep V\,& (G_1\,;\,\ldots\,;\,G_n)     &= \undef \\
\prodtimep V\,& \hbox{switch}\,X\,(f_1:\,G_1\,;\,\ldots\,,f_n:\,G_n)\, &=
    \Avg_{1 \leq i \leq n}\,\prodtime\,V\,G_i \\
\prodtimep V\,& (\hbox{if}\,Cond\,\hbox{then}\,Then\,\hbox{else}\,Else) &=
        \begin{split}
            Pr_{Then}(\timef Cond + \prodtime V\,Then) + \\
            Pr_{Else}(\timef FailingCond + \prodtime V\,Else)
        \end{split} \\
%    \Avg_{G \in \{Then,\,Else\}} \prodtime V\,G \\
\prodtimep V\,& not\,G                       &= \undef \\
\prodtimep V\,& some\,[X_1,\ldots,X_n]\,G    &= \prodtime V\,G \\
\prodtimep V\,& all\,[X_1,\ldots,X_n]\,G     &= \prodtime V\,G \\
\prodtimep V\,& promise\_pure\,G             &= \prodtime V\,G \\
\prodtimep V\,& promise\_semipure\,G         &= \prodtime V\,G \\
\end{array}
\]
\[
\begin{array}{l @{}l}
\callprodtime V\,p\,Args        \,&=
    \begin{cases}
        \begin{split}
            \prodtime (\operatorname{arg\_to\_param} V Args) \\
            (\operatorname{body} p)
        \end{split} &
            \text{if } p \text{ is in the current module} \\
        \timef p &
            \text{otherwise}
    \end{cases} \\
\end{array}
\]
\caption{Variable production time analysis}
\label{alg:var_prod_time}
\end{algorithm}

The algorithm for computing the expected production time
of a shared variable looks at the form of the conjunct that produces it,
this is shown in Algorithm \ref{alg:var_prod_time}.
This algorithm is only invoked on goals that are either \ddet or \dccmulti,
and that produce the variable $V$.
For each goal type,
it determines if it is possible to push the future into that goal,
so that the future is signalled as early as possible.
As noted above,
a future cannot be pushed into certain goal types,
such as higher order calls, method calls,
foreign code or calls that cross module boundaries.
Therefore,
conservative answers are used for these cases.
The case for foreign calls does not measure the call sequence count for the
foreign call itself, so we add one.
Goals such as unifications are trivial, and always have zero cost,
therefore they must produce their variable after zero call sequence counts
of execution.
Parallel conjunctions cannot appear in the goal types analysed,
since they are always converted to sequential conjunctions as parallelism and
profiling are not supported in the same build,
we can safely leave their production times undefined.
Because the algorithm is only invoked on goals that are \ddet or \dccmulti,
then it is never used on negations and disjunctions,
which cannot meaningfully be \ddet.
\paul{Hrm, if I handle \dccmulti then I probably need to handle
disjunctions!
I think I have a solution to this.}
We allow the production time of a variable in a negation or disjunction to
be undefined.

A conjunction 
($G_1,G_2,~\ldots,~G_n$),
is handled by determining if $G_1$ produces the variable,
if it does then we invoke the algorithm recursively on $G_1$.
If $G_1$ does not produce $V$ then the algorithm is invoked recursively on
the remainder of the conjunction ($G_2,\ldots,G_n$),
and the execution time of $G_1$ is added to the result.

We handle switches by invoking the algorithm recursively on each switch
arm,
and computing a weighted average of the results,
with the weights being the arms' entry counts as determined by coverage
inference.
We handle if-then-elses similarly.
we need the weighted average of the two possible cases:
the variable being generated by the then arm versus the else arm.
(It cannot be generated by the condition:
variables generated by the condition are visible only from the then-arm.)
% TODO: implement this behaviour
To find the first number,
we invoke the algorithm on the then-arm,
and add the result to the time taken by the condition.
To find the second,
we invoke the algorithm on the else-arm,
and add the result to the expected time taken by the condition when it fails.
To compute this, we use a version of this algorithm
that weights the time taken by each conjunct in any inner conjunction
by the probability of its execution,
which we know by comparing its execution count
with the count of the number of times the condition was entered.

Using the weighted average for switches and if-then-elses is meaningful because
the Mercury mode system dictates
that if one arm of a switch or if-then-else generates a variable,
then they \emph{all} must do so.
The sole exception is arms that are guaranteed to abort the program,
whose determinism is erroneous.
We use a weight of zero for erroneous arms.
Coverage profiling was introduced to provide the weights of switch and
if-then-else arms used by this algorithm.

If the goal is a quantification,
then the inner goal must be \ddet,
in which case we invoke the algorithm recursively on it.
If the inner goal were not \ddet,
then the outer quantification goal could be \ddet
only if the inner goal did not bind any variables visible from the outside.
Therefore, if the analysis is invoked on a quantification,
we already know that this is because the quantification produces the
the variable, and therefore the inner goal must be \ddet.

The algorithm we use for computing the time
at which a shared variable is first consumed by the second conjunct
is similar to this one,
the main differences being that
negated goals, conditions and disjunctions are allowed to consume variables,
and some arms of a switch or if-then-else
may consume a variable even if other arms do not.
\paul{I am going to set out an Algorithm for this, and show the differences.
But I would like feedback about the algorithm above first,
it took a non-trivial amount of time to fight with latex to get it to look
reasonable,
I do not want to waste that time again on a second algorithm.}
Not all branches of an if-then-else or switch need to consume a variable.
The code that pushes wait operations into goals can be configured to include
a \wait operation on every execution path so that when the goal completes,
it is guaranteed that a \wait operation has taken place.
Our analysis makes the conservative assumption that the system is configured
in this way,
this could prevent introducing parallelism that leads to a slow-down.
To make this assumption,
the algorithm checks if a goal consumes a variable, if it does not it
returns the goal's execution time as the consumption time of the variable.
\paul{I wonder if the actual code works this way}
This also guarantees that any call expected to wait on a future will wait on
that future even though it might not use that parameter.
% XXX: This is not yet implemented.
The other important difference is that when making
any other conservative assumption
such as when the algorithm finds a higher order call,
the consumption time used is zero
(it was the cost of the call when analysing for productions)

\subsection{Feedback framework}
\label{sec:feedback}

Automatic parallelism is just one use of profiler feedback.
Other optimisations
such as inlining,
branch hints
and type specialisation
can also benefit from profiling feedback.

We have designed and implemented a generic feedback framework that allows
tools to create feedback information for the compiler.
These tools may include the profiler, the compiler itself,
or any other tool that is able to link to the feedback library code.
Any Mercury type can be used as feedback information,
making the feedback framework very flexible.
New feedback-directed optimisations may require feedback information
from new analyses.
We expect that many new feedback information types will be added to
support these optimisations.
We also expect that an optimisation may use more than one type of
feedback information and that more than one optimisation may use the
same feedback information.
These were considered requirements of the feedback framework.

% Describe on-disk format,
The on-disc format for the feedback information is very simple:
it contains a header that identifies the file format,
including a version number,
followed by a list of feedback information items,
stored in the format that Mercury uses for reading and writing
terms.
When the file is read in;
the file format identifier and revision number are checked,
the list of information items is checked to ensure that no two items
describe the same feedback information
(for example,
at most one item in this list can describe how to automatically parallelise
a program).

The API allows developers to open an existing file or create a new one,
query and set information in the in-memory copy of the file,
and write the file back out to disk.
It is easy to open the file,
update a specific feedback item in it,
and close it,
essentially leaving the other feedback items in the file unchanged.



% Incorrect background, some of this prose can be used later.
%This approach also used profiling feedback data similar to Tannier's
%work.
%However, it also made use of and some of the advanced
%features of the deep profiler.
%The deep profiler was modified to export a representation of the profiled
%program.
%This representation, along with the profiling data could both be accessed by
%the analysis.
%My approach differed from Tannier's in that the analysis was done outside of the
%compiler in an analysis tool.
%The feedback given to the compiler was the result of this analysis:
%a list of candidate parallel conjunctions that the compiler would attempt to parallelise.
%Each candidate parallel conjunction describes its location in the original
%program and how its conjuncts should be constructed of smaller goals.
%However, at the completion of my honours project the implementation was incomplete
%and no feedback data was actually given to the compiler.
%Figure \ref{fig:prof_fb} closely describes the workflow for our feedback
%analysis.
%
%% call graph traversal.
%
%The auto-parallelism analysis traverses the call graph looking for parallelism
%opportunities.
%The traversal begins at the root of the call graph,
%namely \code{main/2}.
%Each node in the graph is an SCC, and each edge is a call from a call site to a
%distinct callee.
%Each procedure in an SCC is searched for parallelisation candidates before any
%child-nodes are checked (which are also SCCs).
%The search continues in this fashion until it finds an edge
%representing a call whose cost is too low such that it is not worth parallelising
%anything within the callee, and therefore, in the callee's callees.
%In practice, only a small part of the callgraph is traversed,
%since most parts of a program have a very low runtime cost and are not worth
%parallelising.
%The search will also track the number of processors available for spawned-off work
%at any point in the call graph,
%as parallelisation candidates are found, this number will decrease.
%Once it reaches zero, the search will stop as parallelising anything below this
%point in the call graph will not create any useful parallelism.
%This traversal could only be done with access to both a representation of the
%program being profiled and profiling data of the program.
%Therefore, implementing the analysis in a venerate tool was the obvious choice.
%
%Because each node in the graph is an SCC, and the graph is a tree
%the traversal does not need to worry about following cycles infinitely.
%However,
%determining the cost of recursive code is problematic.
%If a predicate calls itself, we cannot know the cost of the recursive
%call,
%this cost is attributed to the cost of the call of that predicate which
%is already active.
%The automatic parallelism analysis described in \citet{paul_hons}
%treats recursive calls naively,
%A solution to this is presented in Section
%\ref{sec:recursive_call_costs}.
