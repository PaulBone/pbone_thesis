
\plan{Introduce work stealing}
Work stealing is a popular method for managing parallel work in a shared
memory multiprocessor system.
% XXX: There may be earlier papers by keller 1984 as cited by halstead.
Multilisp \citep{halstead:1985:multilisp} was one of the first systems to
use work stealing,
which Halstead calls ``an unfair scheduling policy''.
The term ``unfair'' is not an expression of the morals of stealing (work),
instead it refers to the unfairness of \emph{cooperative multitasking} when
compared to something like \emph{round-robin scheduling}.
Each processor in Multilisp has a currently running task,
and a stack of suspended tasks.
If the current task is suspended on a future or finishes,
the processor will execute task at the top of its task stack.
If there is no such task,
it attempts to steal a task from the bottom of another processor's stack.
\paul{XXX: I have not read this paper yet.
It ranks highly in google scholar results and Peter used it so I suspect
that it is notable.}
Another notable example is
\citet{blumofe:1999:work-stealing},
which quantifiably proves that work stealing is more efficient than
\emph{work sharing}
(using a global pool of work).

%Using a task stack encourages a task execution order that is similar to
%seuential execution (we explain why below),
%Halstead reports that this results in lower parallel execution overheads,
%especially for embarassingly parallel workloads.
%We agree that this can improve 

\paul{XXX: Make sure these paragraphs flow once I read the papers above}
\plan{Refer to Peter Wang's conclusion of the early scheduling problem}
\citet{wang:2006:hons} recognises the premature spark scheduling problem
that we described in the previous section.
He proposed work stealing as a solution to the problem.
In a work stealing system,
sparks are placed on a context's local spark stack
is not committed to running in that context;
it may be executed in a different context if it is stolen.
This delays the decision of where to execute a spark until the moment
before it is executed.
Work stealing's other benefits can be summarised as follows.

\begin{itemize}

    \item
    It is quicker to schedule sparks because
    a context can place sparks on its local spark stack more quickly
    than it can place sparks on the global spark queue.
    This is because there there is less contention on a context's
    local spark stack.
    The only source of contention is work stealing which is balanced all of
    the contexts.
    Assume each of the $C$ contexts' spark stacks contains sparks, then
    contention on any one of these stacks is $1/C$ less than it would be on
    a global queue.

    \item
    A idle worker can take work from its own queue quickly.
    Again, there is less contention on a local queue.

    \item
    In ideal circumstances,
    an idle worker rarely needs to steal work from another's queue.
    Stealing work is more costly as it requires communication between
    processors.

\end{itemize}

The initial work stealing implementation was built jointly by
Peter Wang and myself,
Wang contributed about 80\% of the work
and I contributed the remaining 20\%.
Wang's honours thesis \citep{wang:2006:hons} describes his proposal on which
our implementation is heavily based.

\begin{figure}
\begin{center}

\begin{minipage}[b][3in]{0.29\textwidth}
\subfigure[Nested parallel conjunctions]{%
\label{fig:nexted_par_conjunctions}
\begin{tabular}{l}
\code{p(...) :-} \\
\code{~~~~(} \\
\code{~~~~~~~~q(Y::out, ...)} \\
\code{~~~~\&} \\
\code{~~~~~~~~r(Y::in, ...)} \\
\code{~~~~\&} \\
\code{~~~~~~~~s(...)} \\
\code{~~~~).} \\
\code{} \\
\code{q(Y::out, ...) :-} \\
\code{~~~~(} \\
\code{~~~~~~~~t(X::out, ...)} \\
\code{~~~~\&} \\
\code{~~~~~~~~u(X::in, Y::out)} \\
\code{~~~~).} \\
\end{tabular}
}
\hfill
\end{minipage}
\begin{minipage}[b][3in]{0.69\textwidth}
\begin{minipage}[b]{0.32\textwidth}
\subfigure[Step 1]{%
\label{fig:spark_stack_step1}
\picfigurenofloat{spark_stack_step1} }
\hfill
\end{minipage}
\begin{minipage}[b]{0.32\textwidth}
\subfigure[Step 2]{%
\label{fig:spark_stack_step2}
\picfigurenofloat{spark_stack_step2} }
\hfill
\end{minipage}
\begin{minipage}[b]{0.32\textwidth}
\subfigure[Step 3]{%
\label{fig:spark_stack_step3}
\picfigurenofloat{spark_stack_step3} }
\hfill
\end{minipage}

\vspace{0.5in}
\begin{minipage}[b]{0.32\textwidth}
\subfigure[Step 4]{%
\label{fig:spark_stack_step4}
\picfigurenofloat{spark_stack_step2} }
\hfill
\end{minipage}
\begin{minipage}[b]{0.32\textwidth}
\subfigure[Step 5]{%
\label{fig:spark_stack_step5}
\picfigurenofloat{spark_stack_step5} }
\hfill
\end{minipage}
\begin{minipage}[b]{0.32\textwidth}
\subfigure[Step 6]{%
\label{fig:spark_stack_step6}
\picfigurenofloat{spark_stack_step1} }
\hfill
\end{minipage}
\hfill
\end{minipage}

\end{center}
\caption{Spark execution order}
\label{fig:spark_execution_order}
\end{figure}

\plan{Mercury's needs for a deque}
Until now each context has used a stack to manage sparks.
The last item pushed onto the stack is the first to be popped off the
stack.
This \emph{last-in-first-out} order is important for Mercury's parallel
conjunctions.
Parallel conjunctions may be nested, either in the same procedure or through
procedure calls such as in Figure
\ref{fig:nested_par_conjunctions}.
Consider a context whose spark stack's contents are initially undefined,
the spark stack is represented by
Figure \ref{fig:spark_stack_step1}.
When the context calls \code{p} it creates a spark 
for the second and third conjuncts \code{r(\ldots) \& q(\ldots)}
and places the spark on its local stack (Figure
\ref{fig:spark_stack_step2}).
It then calls \code{q} where it creates another spark,
this spark represents the call to \code{u},
and now the spark stack looks like
Figure \ref{fig:spark_stack_step3}.
Assuming that no work stealing has occurred,
when the context finishes executing \code{t} it pops the spark from the top
of its stack;
the popped spark represents the call to \code{u} because
the stack returns items in a last-in-first-out order.
This is desirable because it follows the same execution order as sequential
execution would,
we call this left-to-right execution since by default the left operand of a
conjunction is executed first.
The conjunction in \code{q} contains a shared variable named \code{X},
This execution order ensures that the \signal operation for \code{X}'s future
occurs before the \wait operation;
the context is not blocked by the call to \wait.
Note that the mode annotations are illustrative,
they not part of Mercury's syntax.
Once the spark for \code{u} is popped off the spark stack then the spark
stack looks like Figure \ref{fig:spark_stack_step4}.
After the execution of \code{u}, \code{q} returns control to \code{p},
which then continues the execution of its own parallel conjunction.
\code{p} pops a spark off the spark stack,
the spark popped off is the parallel conjunction
\code{r(\ldots) \& s(\ldots)}.
This immediately creates a spark for \code{s} and pushes it onto the spark
stack (Figure \ref{fig:spark_stack_step5},
it then executes \code{r}.
The execution of \code{r} at this point is also the same as what would occur
if sequential conjunctions were used.
This is straightforward because
the spark represented the \emph{rest} of the parallel
conjunction.
Finally \code{r} returns and \code{p} pops the spark for \code{s} off of the
spark stack and executes it.
At this point the contest's spark stack looks like Figure
\ref{fig:spark_stack_step6}.

If at step 3,
the context executed the spark for \code{r(\ldots) \& s(\ldots)} then
\code{r} would have blocked on the future for \code{Y}.
This would have used more contexts than necessary and more overheads than
necessary.
It may have also caused a deadlock:
since stealing a spark may create a context which may not be permitted due
to the context limit.
Furthermore
the original context would not be able to continue the execution in \code{p}
without special stack handling routines,
this would make the implementation more complicated than necessary.
%The system must be able to make progress by executing contexts from the
%global context run queue or by executing sparks from a context's own local
%spark stack.
Therefore from a context's perspective its local spark storage must behave
like a stack.
Keeping this sequential execution order for parallel conjunctions has
an additional benefit,
it ensures that a popped spark's data has a good chance of being hot in the
processor's cache.

Prior to work stealing,
context local spark stacks returned sparks in last-in-first-out
order,
the global spark queue returns sparks in \emph{first-in-first-out} order.
This does not encourage a left to right execution order,
however \citet{wang:2006:hons} proposes that this order may be better:

\begin{quote}
The global spark queue, however, is a queue because we assume that a
spark generated earlier will have greater amounts of work underneath it
than the latest generated spark, and hence is a better candidate for
parallel execution.
\end{quote}

\noindent
If we translate this execution order into the work stealing system,
it means that work is stolen from the bottom end of a context's local spark
stack.
Consider step 3 of the example above.
If a thief removes the spark for \code{r \& s}
then only the spark for \code{u} is on the stack,
the original context can continue its execution by taking the spark to
\code{u}.
By comparison if the spark for \code{u} was stolen,
it would force the original context to try to execute \code{r \& s}
which, for the reasons above, is less optimal.
Therefore a thief steals work from the bottom of a context local stack
so that a first-in-first-out order is used for parallel tasks.

\citet{halstead:1985:multilisp} also uses processor local task stacks,
so that when a processor executes a task from its own stack,
a last-in-first-out order is used.
He also chose to have thieves steal work from the bottom of a spark stack.
Halstead's reasons are similar to ours,
he asserts that maintaining a near-sequential execution order is
desirable because it has lower overheads,
however he goes on to say that this can improve performance for
embarrassingly parallel workloads.
We believe that work stealing can only reduce overheads in general;
an embarrassing workload will still create more parallelism than the system
can make use of,
and whose overheads must still be paid.
There are better methods for reducing these overheads
such as the use of sparks to represent computations whose execution has not
begun,
or granularity control.

\plan{Describe the data structure used to implement these stacks and its
properties.}
To preserve the last-in-first-out behaviour of context local spark
stacks,
and first-in-first-out behaviour of the global queue,
we chose to use double ended queues (deques) for context local spark
storage.
Since the deque will be used by multiple Mercury engines we must either
choose a data structure that is thread safe or use a mutex to protect a non
thread safe data structure.
The pop and push operations are always made by the same thread,%
\footnote{
    Contexts move between engines by being suspended and resumed,
    but this always involves the context run queue's lock or other
    protection.
    Therefore the context's data structures including the
    spark deque are protected from concurrent access.}
therefore the stealing of work by another context is what creates the need
for synchronisation.
The deque described by \citet{Chase_2005_wsdeque} supports lock free,
nonblocking
operation, it has very low overheads (especially for pop and push operations),
and is dynamically resizable;
all of these qualities are very important to our runtime system's
implementation.
The deque provides the following operations.

\begin{description}

    \item[\code{void push\_spark(deque *d, spark *s)}]
    The deque's owner can call this to push an item onto the
    top%
\footnote{
        Note when we refer to the top of the deque
        \citet{Chase_2005_wsdeque} refers to the bottom and vice-versa.
        Most people prefer to imagine that stacks grow upwards,
        and since the deque is most often used as a stack we consider this
        end of the deque to be the top.
        We also call this the \emph{hot end} as it is the busiest end of the
        deque.}
    or \emph{hot end} of the deque.
    This can be done without synchronisation,
    atomic increment is used to manipulate a counter.
    If necessary,
    \push will grow the array that is used to implement the
    deque.

    \item[\code{bool pop\_spark(deque *d, spark *s)}]
    The deque's owner can call this to pop an item from the top of the
    queue.
    This can also be done without synchronisation,
    except for when the deque contains only one item.
    In this case an atomic compare and swap operation is used to
    determine if the thread lost a race to another thread attempting to
    steal the only task from the deque (a \emph{thief}).
    When this happens, the single item was stolen by the thief and the
    owner's call to \pop returns false,
    indicating that the deque was empty.
    Internally the deque is stored as an array of sparks, not spark
    pointers.
    This is why the second argument in which the result is returned is not a
    double pointer as one might expect.
    This implementation detail avoids memory allocation for sparks inside
    the deque implementation.
    A callers of any of these functions temporarily stores the spark on 
    its program stack.

    \item[\code{result steal\_spark(deque *d, spark *s)}]
    A thread other than the deque's owner can steal
    items from the bottom of the deque.
    This always uses an atomic compare and swap operation as multiple
    thieves may call \steal on the same deque at the same time.
    \steal can return one of three different values:
    ``success'', ``failure'' and ``abort''.
    ``abort'' indicates that the thief lost a race with either the owner
    (\emph{victim}) or another thief.

\end{description}

%All the atomic compare and swap operations here are wait free,
%rather than looping \pop will return false and \steal will abort.
%\push and \pop are very fast, this is good as a context uses its own
%spark deque more often than calling \steal on another's.
%\steal is slightly slower than either \push or \pop,
%but is still quite fast.
%This is good because a context will
%\push and \pop sparks onto and off of its local deque
%more often than it will \steal a spark from another's deque.
%This means that the top of the deque is used more often than the bottom.
%Therefore, we refer to the top as the \emph{hot} end,
%and to the bottom as the \emph{cold} end.
%These terms are less confusing than the disagreement between the stack
%like behaviour of a deque from its context's perspective,
%and the terminology used by \citet{Chase_2005_wsdeque}.

Mercury was already using \citet{Chase_2005_wsdeque}'s deques for spark
storage,
most likely because Wang had always planned to implement work stealing.
We did not need to replace the data structure used for a context local
spark storage,
but we will now refer to it as a deque rather than a stack.
We have removed the global spark queue,
as work stealing does not use one.
Consequently,
when a context creates a spark that spark is always placed on the
context's local spark queue.

\begin{algorithm}
\paul{XXX: Place these environments once we know what pagination will be
used}
\begin{algorithmic}[1]
\Procedure{MR\_join\_and\_continue}{$ST, ContLabel$}
  \State $finished \gets$ atomic\_dec\_and\_is\_zero($ST.num\_outstanding$)
  \If{$finished$}
    \If{$ST.orig\_context = this\_context$}
      \Goto{$ContLabel$}
    \Else
      \While{$ST.orig\_context.resume\_label \neq ContLabel$}
        \State CPU\_relax
      \EndWhile
      \State schedule($ST.parent$, $ContLabel$)
      \Goto{MR\_get\_global\_work}
    \EndIf
  \Else
    \State $got\_spark \gets$ pop\_spark($this\_context.spark\_deque$,
        \&$spark$)
    \If{$got\_spark$}
      \Goto{$spark.code\_label$}
    \Else
      \If{$ST.orig\_context = this\_context$}
         \State suspend($this\_context$)
         \State $this\_context.resume\_label \gets ContLabel$
         \State $this\_context \gets$ NULL
      \EndIf
      \Goto{MR\_get\_global\_work}
    \EndIf
  \EndIf
\EndProcedure
\end{algorithmic}
\caption{MR\_join\_and\_continue}
\label{alg:join_and_continue_ws1}
\end{algorithm}

\plan{Barrier code}
A context accesses its own local spark queue in the \joinandcontinue barrier
introduced in Section \ref{sec:original_scheduling}.
The introduction of work stealing has allowed us to optimise
\joinandcontinue,
the new version of \joinandcontinue is shown in Algorithm
\ref{alg:join_and_continue_ws1}.
The previous version is shown in
Algorithm \ref{alg:join_and_continue_peterw}
on page \pageref{alg:join_and_continue_peterw}.
The first change to this algorithm,
is that this version is lock free.
All the synchronisation is performed by atomic CPU instructions, memory
write ordering and one busy loop.
The number of outstanding contexts in the synchronisation term is
decremented and the result is checked for zero atomically.\footnote{
    On x86/x86\_64 this is a \instruction{lock dec} instruction, we read the
    zero flag to determine if the decrement caused the value to become
    zero.}
This optimisation could have been made without introducing work stealing,
it was convenient to make both changes at the same time.

The next change prevents a race condition that would otherwise be possible
without locking, and occur as follows.
A conjunction of two conjuncts is executed in parallel.
The original context, $C_{Orig}$,
enters the barrier, decrements the counter from two to one,
and because there is another outstanding conjunct,
it executes the else branch on line 12.
At almost the same time another context, $C_{Other}$,
enters the barrier, decrements the counter and finds that there is no more
outstanding work.
$C_{Other}$ attempts to schedule $C_{Orig}$ on line 10.
However attempting to schedule $C_{Orig}$ before it has finished suspending
would cause an inconsistent state and memory corruption.
Therefore lines 7--10 wait until $C_{Orig}$ has been suspended.
The engine that is executing $C_{Orig}$ first suspends $C_{Orig}$ and then
indicates that it has been suspended by setting $C_{Orig}$'s resume label
(lines 18--19).
The busy loop on lines 7--10 includes a \code{CPU\_relax} instruction,
This is the \instruction{pause} instruction on x86 and x86\_64,
It instructs the CPU to pipeline the loop differently in order to reduce
memory traffic and allow the loop to exit without a pipeline stall
\citep{intel:pause}.
Lines 18--19 must also include memory write ordering.

The other change is around line 17,
when the context pops a spark of its stack, it does not check if the spark
was created by a callee's parallel conjunction.
Because all sparks are placed on the context local spark deques and
sparks are stolen from the cold end of the deque,
if there is an outstanding conjunct then its spark will be on the top of the
deque,
otherwise the deque will be empty.

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{MR\_get\_global\_work}{}
  \State acquire\_lock($MR\_runqueue\_lock$)
  \Loop
    \If{$MR\_exit\_now$}
      \State release\_lock($MR\_runqueue\_lock$)
      \State MR\_destroy\_thread()
    \EndIf
    \State $ctxt \gets$ MR\_get\_runnable\_context()
    \If{$ctxt$}
      \State release\_lock($MR\_runqueue\_lock$)
      \If{$current\_context$}
        \State MR\_release\_context($current\_context$)
      \EndIf
      \State MR\_load\_context($ctxt$)
      \Goto $ctxt.resume$
    \EndIf
    \If{$MR\_num\_outstanding\_contexts < MR\_max\_contexts$}
      \State $result \gets$ MR\_try\_steal\_spark(\&$spark$)
      \If{$result$}
        \State release\_lock($MR\_runqueue\_lock$)
        \If{$\neg current\_context$}
          \State $ctxt \gets$ MR\_get\_free\_context()
          \If{$\neg ctxt$}
            \State $ctxt \gets$ MR\_create\_context()
          \EndIf
          \State MR\_load\_context($ctxt$)
        \EndIf
        \State $MR\_parent\_sp \gets spark.parent\_sp$
        \State $ctxt.thread\_local\_mutables \gets
          spark.thread\_local\_mutables$
        \Goto $spark.resume$
      \EndIf
    \EndIf
    \State timed\_wait($MR\_runqueue\_cond, MR\_runqueue\_lock$, $timeout$)
  \EndLoop
\EndProcedure
\end{algorithmic}
\caption{MR\_get\_global\_work}
\label{alg:MR_get_global_work_wsinitial}
\end{algorithm}

\plan{Other changes to the idle loop?}
We have also modified \getglobalwork to use spark stealing,
its new code is shown in Algorithm \ref{alg:MR_get_global_work_wsinitial}.
We have replaced the old code which dequeued a spark from the global spark
queue
with a call to \trystealspark (see below).
Also as spark creation does not affect the runqueue condition,
engines must wake up and check if there is any parallel work available.
This is done on line 26 using a call to \code{timed\_wait} with a
configurable timeout (it defaults to 2ms).
We discuss this timeout later in Section \ref{sec:idle_loop}.

\plan{Accessing the array of deques}
Engines that attempt to steal a spark must have access to all the spark
deques.
We do this with a global array of pointers to contexts' deques.
When a context is created,
the runtime system attempts to add the context's deque to this array by
finding an empty slot,
one containing a null pointer,
and writing the pointer to the context's deque to that index in the array.
If there is no unused slot in this array, the runtime system will resize the
array.
When the runtime system destroys a context it writes \NULL to that context's
index in the deque array.
To prevent concurrent access from corrupting the array these operations are
protected by the \code{MR\_spark\_deques\_lock} lock.

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{MR\_try\_steal\_spark}{$spark\_ptr$}
  \State acquire\_lock($MR\_spark\_deques\_lock$)

  \State $max\_attempts \gets$ min($MR\_max\_spark\_deques$,
    $MR\_worksteal\_max\_attempts$)

  \For{$attempt = 1$ to $max\_attempts$}
    \State $MR\_victim\_counter$++
    \State $deque \gets
       MR\_spark\_deques$[$MR\_victim\_counter \bmod MR\_max\_spark\_deques$]
    \If{$deque \neq$ NULL}
      \State $result \gets$ steal\_spark($deque$, $spark\_ptr$)
      \If{$result$}
        \State release\_lock($MR\_spark\_deques\_lock$)
        \State \Return $true$
      \EndIf
    \EndIf
  \EndFor
  \State release\_lock($MR\_spark\_deques\_lock$)
  \State \Return $false$
\EndProcedure
\end{algorithmic}
\caption{MR\_try\_steal\_spark}
\label{alg:try_steal_spark_initial}
\end{algorithm}

\plan{Thief behaviour}
The algorithm for \trystealspark is shown in
Algorithm \ref{alg:try_steal_spark_initial}.
It must also acquire the lock before using the array.
A thief may need to make several attempts before it successfully finds a
deque with work it can steal.
Line 3 sets the number of attempts to make, which is either the user
configurable \var{MR\_worksteal\_max\_attempts} or the size of the array,
whichever is smallest.
A loop (beginning on line 4) attempts to steal work until it succeeds or it
has made \var{max\_attempts}.
We use a global variable, \var{MR\_victim\_counter},
to implement round-robin selection of the victim.
On line 8 we attempt to steal work from a deque,
provided that its deque pointer in the array array is non-null.
If the call to \steal succeeded,
it will have written spark data into the memory pointed to by
\var{spark\_ptr},
then \trystealspark releases the lock and returns true.
Eventually \trystealspark may give up (lines 13--14).
If it does so it will release the lock and return false.
Whether or not a thief steals work or gives up,
the next thief will resume the round-robin selection where the previous
thief left off,
this is guaranteed because \var{MR\_victim\_counter} is protected by the
lock acquired on line 2.

\plan{Work stealing fixes the premature scheduling decision problem}
All sparks created by a context are placed on its local spark deque.
Sparks are only removed to be executed in parallel when an idle engine
executes \trystealspark.
Therefore
the decision to execute a spark in parallel is only made once an engine is
idle and able to run the spark.
We expect this to correct the premature scheduling decision problem we
described in Section \ref{sec:original_scheduling_performance}.

\paul{XXX: Fix the context limit condition (allow it to succeed if the engine
has an empty context.
}
\input{tab_work_stealing_initial}

\plan{Benchmark}
We benchmarked our initial work stealing implementation with the mandelbrot
program from previous sections.
Table \ref{tab:work_stealing_initial} shows the results of our benchmarks.
The first and third row groups are the same results as the first and second
row groups from Table \ref{tab:2009_left_nolimit} respectively.
They are included to allow for easy comparison.
The second and fourth row groups were generated with a newer version of
Mercury that implements work stealing as described above.\footnote{
    The version of Mercury used is slightly modified,
    due to an oversight we had forgotten to include the context limit
    condition in \getglobalwork.
    The version we benchmarked includes this limit and matches the
    algorithms shown in this section.}
The figures in parenthesis are speedup figures.

The first conclusion that we can draw from the results is that
left recursion with work stealing is much faster than left recursion without
work stealing.
We can see that work stealing has solved the premature spark scheduling problem
problem for mandelbrot.
Given that left recursive mandelbrot represented a pathological case of this
problem,
we are confident that the problem has generally been solved.

We can also see that the context limit no longer affects performance
with left recursion.
There is no significant difference between the rows with 8, 16 or 32
context limits.
Specifying a limit of four contexts limits performance when using three or
four Mercury engines, but not with two engines.
The runtime system allows the user to specify the context limit as a
multiple of the number of engines.
Therefore in setting up our tests we divided the desired context limit by
the number of engines, then the runtime system multiplied it,
this process used integer division and results in a context limit that
does not always match the desired context limit.
The context limits used for 1--4 engines are 4, 4, 2, 4.
In the case for three engines the division operation rounded down affecting
the actual context limit used.
\paul{XXX: Re-test then finish this discussion}
%after each engine but the initial one had stolen a spark and created a
%context for it, there will be $N$ contexts in use for $N$ engines.
%Next time an engine executes \getglobalwork it would not attempt to steal a
%spark as doing so would exceed the context limit.
%As the default value for the number of contexts per engine is two,
%this is not a problem under normal circumstances.

\paul{XXX: Consider always putting left and right in italics.}
Work stealing improves the performance of right recursion with high context
limits.
This is somewhat expected since work stealing is known as a very
efficient way of managing parallel work on a shared memory system.
However we did not expect to get such a significant improvement in
performance.
In the non work stealing system, a spark is placed on the global spark queue
if both the context limit has not been reached, and there is an idle engine.
We believe that the premature scheduling problem was also affecting right
recursion.
When a spark is being created there may not be an idle engine,
however an engine may become idle soon and wish to execute a spark.
With work stealing,
all sparks are placed on local deques and an idle
engine will attempt to steal work when it is ready ---
the decision to run a spark in parallel is no longer premature.
This is the same reason as why work stealing benefits left recursive
programs so much:
left recursive programs simply happen to have a pathological case of
the premature scheduling problem.

There is a second observation we can make about right recursion.
In the cases for 256 or 512 contexts work stealing performs worse than
without work stealing.
We believe that the original results were faster because more work was done
in parallel.
When a context spawned off a spark,
it would often find that there was no free engine,
and place the spark on its local queue.
After completing the first conjunct of the parallel conjunction it would
execute \joinandcontinue.
\joinandcontinue would would find that the context had a spark on its local
stack and would execute this spark directly.
When a spark is executed directly the existing context is re-used and so the
context limit does not increase as quickly.
The work stealing version does not do this,
it always places the spark on its local stack and almost always
another engine will steal that spark,
and the creation of a new context will bring the system closer to the
context limit.
The work stealing program is more likely to reach the context limit earlier,
where it will be forced into sequential execution.

\paul{XXX: Consider verifying this point with instrumentation.}

