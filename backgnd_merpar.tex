\status{This section is complete.}

Mercury has several backends,
we are only concerned with the low-level C backend.
The low-level C backend uses an abstract machine implemented using the C
preprocessor.
The compiler writes out procedures as sequences of macro uses.
The C compiler expands these macros and generate native code.
The abstract machine has 1024 general purpose registers and some
special registers, such as the program counter.
These registers are virtual:
they are C macros which expand to global variables.
Commonly used virtual registers are mapped to real machine registers
using GCC's \citep{gcc} global register extension.
The number of mapped registers depends upon the architecture of the physical
machine.

The compiler knows the determinism of every procedure.
It will generate more efficient code for deterministic procedures.
In particular,
code with at most one solution uses the \emph{det stack} (deterministic
stack),
which operates in much the same way as a stack in an imperative or
functional language.
The det stack has a single pointer called the stack pointer,
it points to the top of the stack.
Code with one or more solutions uses the \emph{nondet stack}:
When a nondeterministic procedure produces a solution it returns control to
its caller;
it leaves its stack frame is on the nondet stack so that backtracking can
use it to check for other solutions.
Therefore,
two stack pointers are used with the nondet stack,
the first points to the top of the stack where new frames are allocated
and the second points to the current stack frame.
Mercury only supports parallel conjunctions in deterministic code,
therefore the nondet stack and its two stack pointers are largely irrelevant.
\citet{mercury_jlp} explains Mercury's execution algorithm in more detail.

To support parallel execution \citet{conway:2002:par} grouped the variables
used to implement the abstract machine's registers into a structure
called an \emph{engine}.
Each engine corresponds to a POSIX Thread (pthread)
\citep{butenhof1997:pthreads}.
Multiple engines can reside in memory at the same time,
allowing each pthread to maintain its own execution state.
The number of engines that a parallel Mercury program will allocate on startup
is configurable by the user.
Once the runtime system has been stared the number of engines in use is
fixed.

In non-parallel grades a single engine is allocated statically and is known
to reside at a particular address.
However,
in parallel grades the multiple engine's addresses are unknown at compile
time.
An extra real cpu register\footnote{
    If GCC global registers are unavailable,
    POSIX thread-local storage is used.
}
is used if available to maintain the address of
the pthread's engine,
therefore there is one less real cpu register that can be used for mapping
virtual registers.

\citet{conway:2002:par} also introduced a new structure called a
\emph{context}, known elsewhere as a green thread.
Contexts represent computations in progress.
An engine can either be idle, or executing a context;
a context can either be running on an engine, or be suspended.
Separate computations must have separate stacks,
therefore the stacks are associated with a context.
When a computation is suspended,
the registers from the engine are copied into the context,
making the engine free to load a new context.

Stacks account for most of a context's memory usage.
When a context finishes execution
it can either be retained by the engine or
have its storage released to the free context pool.
This decision depends on what the engine will do next:
if the engine will execute a different context or go to sleep
(because there is no work to do),
then the current context is released.
Contexts cannot be preempted.

Following
\citet{blumofe:1994:work-stealing,halstead:1985:multilisp,kranz:1989:mul-t_article},
we economise on memory by using \emph{sparks}
to represent goals that have been spawned off
but whose execution has not yet been started.
This is also similar to \citet{simonmar_2009_multicore_rts}.
Sparks are very light weight,
being three words long.
Therefore, compared with contexts,
they represent outstanding parallel work very efficiently.
When an engine executes a spark it will convert the spark into a context.
If the engine was already holding a context,
that context will be used
(the engine's current context is always free when the engine attempts to run
a spark),
or if it does not have a context,
a new one is allocated
(from the pool of free contexts if the pool is not empty,
otherwise a new context is created).
Unlike \citet{simonmar_2009_multicore_rts},
Mercury's sparks cannot be garbage collected and must be executed.

The only parallel construct in Mercury is parallel conjunction,
which is denoted $(G_1~\&~\ldots~\&~G_n)$.
All the conjuncts must be \ddet or \dccmulti,
that is, they must all have exactly one solution,
or commit to exactly one solution.
This restriction greatly simplifies the implementation,
since it guarantees that there can never be any need
to execute $(G_2~\&~\ldots~\&~G_n)$ multiple times,
just because $G_1$ has succeeded multiple times.
(Any local backtracking inside $G_1$ will not be visible to the other conjuncts;
bindings made by \ddet code are never retracted.)
The current Mercury implementation supports parallelism for \ddet and
\dccmulti only,
since supporting it for code that might have no solutions
entails speculative execution,
while supporting for code that might have more than one solution
requires significant new infrastructure for managing bindings.
However, this is not a significant limitation;
since the design of Mercury strongly encourages deterministic code,
in our experience, about 75 to 85\% of all Mercury procedures are \ddet,
and most programs spend an even greater fraction of their time in \ddet code.
\peter{Do we have any statistics regarding what proportion of execution time
  is spent in det code?  That would be a more relevant statistic.}
Existing algorithms for executing nondeterministic code in parallel
have very significant overheads, generating slowdowns by integer factors.
Thus we have given priority to parallelising deterministic code,
which we can do with \emph{much} lower overhead.
We think that avoiding such slowdowns is a good idea,
even if it does mean foregoing the parallelisation of 15 to 25\% of a program.

\begin{figure}
\begin{verbatim}
    /*
    ** A SyncTerm. One syncterm is created for each parallel conjunction.
    */
struct MR_SyncTerm_Struct {
    MR_Context              *MR_st_orig_context;
    MR_Word                 *MR_st_parent_sp;
    volatile MR_Unsigned    MR_st_count;
};

    /*
    ** A Mercury Spark.  A spark is created for a spawned off parallel
    ** conjunct.
    */
struct MR_Spark {
    MR_SyncTerm             *MR_spark_sync_term;
    MR_Code                 *MR_spark_resume;
    MR_ThreadLocalMuts      *MR_spark_thread_local_mutables;
}
\end{verbatim}
\caption{Syncterms and sparks}
\label{fig:spark_and_syncterm}
\end{figure}

The Mercury compiler implements $(G_1~\&~G_2~\&~\ldots~\&~G_n)$
by creating a \emph{syncterm} (synchronisation term), a data structure
representing a barrier for the whole conjunction.
The syncterm contains:
a pointer to the context that began executing the parallel conjunction
(the parent context),
a pointer to the stack frame for the procedure containing this parallel
conjunction,
and a thread safe counter that tracks the number of conjuncts that have not
yet executed their barrier.
After initialising this syncterm, the engine then
spawns off $(G_2~\&~\ldots~\&~G_n)$ as a spark and continues by executing
$G_1$ itself.
The spark contains a pointer to the syncterm,
as well as a pointer to the code it must execute
and another pointer to a set of thread local mutables.\footnote{
    Mercury has a module-local mutable feature that supports
    per thread mutables.
    This is not relevant to the dissertation.}
Figure \ref{fig:spark_and_syncterm} shows these two data structures.
Chapter \ref{chap:rts} describes how sparks are managed.

When the engine finishes the execution of the first conjunct ($G_1$)
it executes the barrier code \joinandcontinue and the end of the conjunct.
The barrier prevents the original context from executing the code that
follows the parallel conjunction
until each of the conjuncts has been executed.
The barrier also makes several scheduling decisions, see Chapter
\ref{chap:rts},

\begin{figure}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\textbf{Source code}} &
\multicolumn{1}{c}{\textbf{Transformed pseudo-code}} \\
\hline
                    & \code{~~MR\_SyncTerm ST;} \\
\code{~~}$($        & \code{~~spawn\_off(\&ST, Spawn\_Label);} \\
\code{~~~~}$G_1$    & \code{~~}$G_1$ \\
\code{~~}$\&$       & \code{~~MR\_join\_and\_continue(\&ST, Cont\_Label);} \\
                    & \code{Spawn\_Label:} \\
\code{~~~~}$G_2$    & \code{~~}$G_2$ \\
\code{~~}$)$        & \code{~~MR\_join\_and\_continue(\&ST, Cont\_Label);} \\
                    & \code{Cont\_Label:} \\
\end{tabular}
\end{center}
\caption{The implementation of a parallel conjunction}
\label{fig:par_conj}
\end{figure}

Since $(G_2~\&~\ldots~\&~G_n)$ is itself a conjunction,
it is handled in a similar way:
the context executing it
first spawns off $(G_3~\&~\ldots~\&~G_n)$ as a spark that points to the sync
term created earlier,
and then executes $G_2$ itself.
Eventually, the spawned-off remainder of the conjunction
consists only of the final conjunct, $G_n$,
and the context just executes it.
Once each conjunct synchronises using {\joinandcontinue},
the original context will continue execution after the parallel conjunction.
An example of the instrumentation necessary to implement parallel
conjunctions is shown in Figure \ref{fig:par_conj}.

