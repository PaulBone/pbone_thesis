
\status{This section is complete}

The low-level C backend of the Mercury implementation uses C as a
pseudo-assembler;
A Mercury \emph{engine} structure represents a CPU that can execute Mercury
Abstract Machine instructions.
An engine has fields that represent virtual registers,
some of which are mapped (using GCC's~\citep{gcc} global register extension) to
physical machine registers - depending on the architecture of the physical
machine.
These registers include 1024 general-purpose registers,
three stack pointers for the two stacks,
and a success continuation pointer for implementing tail-calls.
The structure contains many other fields for bookkeeping,
we will discuss them as they become relevant.
The abstract machine instructions are implemented as C preprocessor macros,
which use the engine structure.
More information about Mercury's execution algorithm can be found in
\citet{mercury_jlp}.

To make parallel execution possible \citet{conway:2002:par} allowed
multiple instances of the engine structure to be used at once.
The Operating System's threading library,
namely POSIX Threads~\citep{pthreads},
is used to expose parallelism to the OS.
Each POSIX Thread (pthread) owns a single engine.
The number of engines that a parallel Mercury program will allocate on startup
is configurable by the user,
but it defaults to the actual number of CPUs.
Users can also configure the runtime system to pin engines to CPUs
ensure there is an even distribution of engines across the machine.
\paul{Maybe discuss how thread pinning and SMT interact.  Although all
this thread pinning work is new work - it's not necessary part of the
background.  I think this discussion may belong in \ref{chap:rts}}

Each POSIX Thread must maintain a pointer to its engine;
this is kept in a physical machine register as it is accessed
frequently.
In sequential code,
there was only one engine structure,
so a compiled-in constant could easily be used.
Therefore, in parallel builds there are fewer physical registers that
virtual registers may be mapped onto.

\citet{conway:2002:par} also introduced a new structure called a
\emph{context}, also known as a green thread.
Contexts represent computations in progress.
An engine may be idle, or it may be executing a context;
a context can be running on an engine, or it may be suspended.
Each context has two stacks, a det stack and a nondet stack;
Procedures that can succeed more than once
store their frames on the nondet stack;
all other procedures use the det stack
(which behaves like a stack in an imperative language).
These stacks account for the majority of the context's memory usage.
When a context finishes execution,
it can either be retained by the engine or have its storage released to the
free context pool.
This decision depends on what the engine will do next,
if the engine will execute a different context or go to sleep
(because there's no work to do);
then the current context is released,
otherwise it is retained.
Contexts cannot be preempted.

Following \citet{simonmar_2009_multicore_rts},
we economise on memory by using \emph{sparks}
to represent goals that have been spawned off
but whose execution has not yet been started.
Sparks are very light weight being only three words long.
Therefore, compared to contexts,
they represent outstanding parallel work very efficiently.
When an engine executes a spark it will convert the spark into a context.
If the engine was already holding a context that context will be used
(the engine's current context is always free when the engine attempts to run
a spark),
or if it does not have a context a new one is allocated
(from the pool of free contexts if the pool is not empty,
and a newly created context otherwise).
Unlike \citet{simonmar_2009_multicore_rts},
Mercury's sparks cannot be garbage collected and must be executed.

The only parallel construct in Mercury is parallel conjunction,
which is denoted $(G_1~\&~\ldots~\&~G_n)$.
All the conjuncts must be \ddet or \dccmulti,
that is, they must all have exactly one solution,
or commit to exactly one solution.
This restriction greatly simplifies the implementation,
since it guarantees that there can never be any need
to execute $(G_2~\&~\ldots~\&~G_n)$ multiple times,
just because $G_1$ has succeeded multiple times.
(Any local backtracking inside $G_1$ will not be visible to the other conjuncts;
bindings made by \ddet code are never retracted.)
The current Mercury implementation supports parallelism only for \ddet and \dccmulti,
since supporting it for code that may have no solution
would represent speculative execution,
while supporting for code that may have more than one solution
would require significant new infrastructure for managing bindings.
However, this is not a significant limitation.
Since the design of Mercury strongly encourages deterministic code,
in our experience, about 75 to 85\% of all Mercury procedures are \ddet,
and most programs spend an even greater fraction of their time in \ddet code.
\peter{Do we have any statistics regarding what proportion of execution time
  is spent in det code?  That would be a more relevant statistic.}
Existing algorithms for executing nondeterministic code in parallel
have very significant overheads, generating slowdowns by integer factors.
Thus we have given priority to parallelising deterministic code,
which we can do with \emph{much} lower overhead.
We think that avoiding such slowdowns is a good idea,
even if it does mean foregoing the parallelisation of 10 to 25\% of a program.

\begin{figure}
\begin{verbatim}
    /*
    ** A SyncTerm. One syncterm is created for each parallel conjunction.
    */
struct MR_SyncTerm_Struct {
    MR_Context              *MR_st_orig_context;
    MR_Word                 *MR_st_parent_sp;
    volatile MR_Unsigned    MR_st_count;
};

    /*
    ** A Mercury Spark.  A spark is created for a spawned off parallel
    ** conjunct.
    */
struct MR_Spark {
    MR_SyncTerm             *MR_spark_sync_term;
    MR_Code                 *MR_spark_resume;
    MR_ThreadLocalMuts      *MR_spark_thread_local_mutables;
}
\end{verbatim}
\caption{Syncterms and sparks}
\label{fig:spark_and_syncterm}
\end{figure}

The Mercury compiler implements $(G_1~\&~G_2~\&~\ldots~\&~G_n)$
by creating a \emph{syncterm} (synchronisation term), a data structure
representing a barrier for the whole conjunction.
The syncterm contains:
a pointer to the context that begun executing the parallel conjunction
(the parent context),
a pointer to the stack frame for the procedure containing this parallel
conjunction,
and a thread safe counter that tracks the number of conjuncts that have not
yet executed their barrier.
After initialising this syncterm, the engine then
spawns off $(G_2~\&~\ldots~\&~G_n)$ as a spark and continues by executing
$G_1$ itself.
The spark contains a pointer to the syncterm,
as well as a pointer to the code it must execute
and another pointer to any thread local mutables that should be available to the
spawned off code.
Figure \ref{fig:spark_and_syncterm} shows these two data structures.
The spark is added to a global run queue of sparks, or if that queue is too
full, because there's already enough parallelism,
then the spark is added to a queue owned by the current context.
\citet{wang_hons_thesis} intended to use the local queues for work stealing
but had not completed his implementation,
The work-stealing dequeue structure
is described in \citet{Chase_2005_wsdeque}.
see Chapter \ref{chap:rts} for details.

If the context finishes the execution of the first conjunct ($G_1$)
and finds that spark is still at the head of its queue,
it will pick it up and run it itself.
This is a useful optimisation,
% it's also really well-known.
since it avoids using a separate context in the relatively common case
that all the other CPUs are busy with their own work.
This optimisation is also useful since it can avoid the creation of superfluous
contexts and their stacks.

\begin{figure}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\textbf{Source code}} &
\multicolumn{1}{c}{\textbf{Transformed pseudo-code}} \\
\hline
                    & \code{~~MR\_SyncTerm ST;} \\
\code{~~}$($        & \code{~~spawn\_off(\&ST, Spawn\_Label);} \\
\code{~~~~}$G_1$    & \code{~~}$G_1$ \\
\code{~~}$\&$       & \code{~~join\_and\_continue(\&ST, Cont\_Label);} \\
                    & \code{Spawn\_Label:} \\
\code{~~~~}$G_2$    & \code{~~}$G_2$ \\
\code{~~}$)$        & \code{~~join\_and\_continue(\&ST, Cont\_Label);} \\
                    & \code{Cont\_Label:} \\
\end{tabular}
\end{center}
\caption{The implementation of a parallel conjunction}
\label{fig:par_conj}
\end{figure}

Since $(G_2~\&~\ldots~\&~G_n)$ is itself a conjunction,
it is handled in a similar way:
the context executing it
first spawns off $(G_3~\&~\ldots~\&~G_n)$ as a spark that points to the barrier
created earlier,
and then executes $G_2$ itself.
Eventually, the spawned-off remainder of the conjunction
consists only of the final conjunct, $G_n$,
and the context just executes it.
The code of each conjunct synchronises on the barrier once it has
completed its job.
When all conjuncts have done so,
the original context will continue execution after the parallel conjunction.
An example of the instrumentation necessary to implement parallel
conjunctions is shown in Figure \ref{fig:par_conj}.

