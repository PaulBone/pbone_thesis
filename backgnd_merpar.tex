\status{This section is complete}

Mercury has a number of backends
the low-level C backend uses an asbract machine implemented using the C
preprocessor.
The compiler writes out procedures as sequences of macro uses.
The C compiler is used to expand the macros and generate native code.
The abstract machine has 1024 general purpose registers and a number of
special registers, such as the program counter.
These registers are virtual:
they are C macros which expand to global variables.
Commonly used virtual registers are mapped to real machine registers
using GCC's \citep{gcc} global register extension.
The number of mapped registers depends upon the achitecture of the physical
machine.

The compiler hass access to determinism information of a procedure.
This means that if a procedure is known to be determinisic more efficient
code can be generated for it.
In particular,
code with at most one solution uses the \emph{det stack} (deterministic
stack),
which operatates in much the same way as a stack in an imperative or
functional language.
The detstack has one pointer to the top of the stack,
called the stack pointer.
Code with more than one solution uses the \emph{nondet stack}:
When a nondeterministic procedure produces a solution it returns control to
its caller,
however, its stack frame is kept on the nondet stack incase backtracking
requires it to check for other solutions.
Therefore,
two stack pointers are used with the nondet stack,
one points to the top of the stack where new frames are allocated,
the other points to the current stack frame.
Parallel conjunctions are only supported in deterministic code,
therefore the nondet stack and its two stack pointers are largly irrelevent.
More information about Mercury's execution algorithm can be found in
\citet{mercury_jlp}.

To support parallel exececution \citet{conway_par} grouped the variables
used to implement the abstract machine's registers into a structure
called an \emph{engine}.
Each engine corresponds to a POSIX Thread (pthread)
\citep{butenhof1997:pthreads}.
Multiple engines can reside in memory at the same time,
allowing each pthread to maintain its own execution state.
The number of engines that a parallel Mercury program will allocate on startup
is configurable by the user,
but it defaults to the actual number of CPUs.

In non-parallel grades a signel engine is allocated statically and is known
to reside at a particular address.
However,
in parallel grades the multiple engine's addresses are uknown at compile
time.
An extra real cpu register\footnote{
    If GCC global registers are unavailable,
    POSIX thread-local storage is used.
}
is used if available to maintain the address of
the pthread's engine,
therefore there is one less real cpu register that can be used for mapping
virtual registers.

\citet{conway:2002:par} also introduced a new structure called a
\emph{context}, known elsewhere as a green thread.
Contexts represent computations in progress.
An engine may be idle, or it may be executing a context;
a context can be running on an engine, or it may be suspended.
Separate computations are required to have seperate stacks,
therefore the storage for stacks is associated with a context.
When a computation is suspended,
the registers from the engine are copied into the context,
making the engine free to load a new context.

Stacks account for the majority of a context's memory usage.
When a context finishes execution
it can either be retained by the engine or
have its storage released to the free context pool.
This decision depends on what the engine will do next:
if the engine will execute a different context or go to sleep
(because there is no work to do),
then the current context is released.
Contexts cannot be preempted.

\paul{Not really following Simon, It's Peter who proposed this and I'm not
sure where he got the idea from. (but it is pretty common)}
Following \citet{simonmar_2009_multicore_rts},
we economise on memory by using \emph{sparks}
to represent goals that have been spawned off
but whose execution has not yet been started.
Sparks are very light weight,
being only three words long.
Therefore, compared to contexts,
they represent outstanding parallel work very efficiently.
When an engine executes a spark it will convert the spark into a context.
If the engine was already holding a context,
that context will be used
(the engine's current context is always free when the engine attempts to run
a spark),
or if it does not have a context,
a new one is allocated
(from the pool of free contexts if the pool is not empty,
otherwise a new context is created).
Unlike \citet{simonmar_2009_multicore_rts},
Mercury's sparks cannot be garbage collected and must be executed.

The only parallel construct in Mercury is parallel conjunction,
which is denoted $(G_1~\&~\ldots~\&~G_n)$.
All the conjuncts must be \ddet or \dccmulti,
that is, they must all have exactly one solution,
or commit to exactly one solution.
This restriction greatly simplifies the implementation,
since it guarantees that there can never be any need
to execute $(G_2~\&~\ldots~\&~G_n)$ multiple times,
just because $G_1$ has succeeded multiple times.
(Any local backtracking inside $G_1$ will not be visible to the other conjuncts;
bindings made by \ddet code are never retracted.)
The current Mercury implementation supports parallelism only for \ddet and \dccmulti,
since supporting it for code that may have no solution
would represent speculative execution,
while supporting for code that may have more than one solution
would require significant new infrastructure for managing bindings.
However, this is not a significant limitation.
Since the design of Mercury strongly encourages deterministic code,
in our experience, about 75 to 85\% of all Mercury procedures are \ddet,
and most programs spend an even greater fraction of their time in \ddet code.
\peter{Do we have any statistics regarding what proportion of execution time
  is spent in det code?  That would be a more relevant statistic.}
Existing algorithms for executing nondeterministic code in parallel
have very significant overheads, generating slowdowns by integer factors.
Thus we have given priority to parallelising deterministic code,
which we can do with \emph{much} lower overhead.
We think that avoiding such slowdowns is a good idea,
even if it does mean foregoing the parallelisation of 15 to 25\% of a program.

\begin{figure}
\begin{verbatim}
    /*
    ** A SyncTerm. One syncterm is created for each parallel conjunction.
    */
struct MR_SyncTerm_Struct {
    MR_Context              *MR_st_orig_context;
    MR_Word                 *MR_st_parent_sp;
    volatile MR_Unsigned    MR_st_count;
};

    /*
    ** A Mercury Spark.  A spark is created for a spawned off parallel
    ** conjunct.
    */
struct MR_Spark {
    MR_SyncTerm             *MR_spark_sync_term;
    MR_Code                 *MR_spark_resume;
    MR_ThreadLocalMuts      *MR_spark_thread_local_mutables;
}
\end{verbatim}
\caption{Syncterms and sparks}
\label{fig:spark_and_syncterm}
\end{figure}

The Mercury compiler implements $(G_1~\&~G_2~\&~\ldots~\&~G_n)$
by creating a \emph{syncterm} (synchronisation term), a data structure
representing a barrier for the whole conjunction.
The syncterm contains:
a pointer to the context that began executing the parallel conjunction
(the parent context),
a pointer to the stack frame for the procedure containing this parallel
conjunction,
and a thread safe counter that tracks the number of conjuncts that have not
yet executed their barrier.
After initialising this syncterm, the engine then
spawns off $(G_2~\&~\ldots~\&~G_n)$ as a spark and continues by executing
$G_1$ itself.
The spark contains a pointer to the syncterm,
as well as a pointer to the code it must execute
and another pointer to a set of thread local mutables.\footnote{
    Mercury has a module-local mutable feature that supports
    per thread mutables.
    However, this is not relevant to this dissertation.}
Figure \ref{fig:spark_and_syncterm} shows these two data structures.
Chapter \ref{chap:rts} describes how sparks are managed.

When the engine finishes the execution of the first conjunct ($G_1$)
it executes the barrier code \joinandcontinue and the end of the conjunct.
The barrier ensures that the orignal context cannot execute the code that
follows the parallel conjunction
until every one of the conjuncts has been exexuted.
It also makes several scheduling decisions which are described in Chapter
\ref{chap:rts},

\begin{figure}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\textbf{Source code}} &
\multicolumn{1}{c}{\textbf{Transformed pseudo-code}} \\
\hline
                    & \code{~~MR\_SyncTerm ST;} \\
\code{~~}$($        & \code{~~spawn\_off(\&ST, Spawn\_Label);} \\
\code{~~~~}$G_1$    & \code{~~}$G_1$ \\
\code{~~}$\&$       & \code{~~MR\_join\_and\_continue(\&ST, Cont\_Label);} \\
                    & \code{Spawn\_Label:} \\
\code{~~~~}$G_2$    & \code{~~}$G_2$ \\
\code{~~}$)$        & \code{~~MR\_join\_and\_continue(\&ST, Cont\_Label);} \\
                    & \code{Cont\_Label:} \\
\end{tabular}
\end{center}
\caption{The implementation of a parallel conjunction}
\label{fig:par_conj}
\end{figure}

Since $(G_2~\&~\ldots~\&~G_n)$ is itself a conjunction,
it is handled in a similar way:
the context executing it
first spawns off $(G_3~\&~\ldots~\&~G_n)$ as a spark that points to the sync
term created earlier,
and then executes $G_2$ itself.
Eventually, the spawned-off remainder of the conjunction
consists only of the final conjunct, $G_n$,
and the context just executes it.
Once each conjunct synchronises using {\joinandcontinue},
the original context will continue execution after the parallel conjunction.
An example of the instrumentation necessary to implement parallel
conjunctions is shown in Figure \ref{fig:par_conj}.

