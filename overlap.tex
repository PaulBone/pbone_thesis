% vim: ts=4 sw=4 et ft=tex

\chapter{Overlap Analysis for Dependent AND-parallelism}
\label{chap:overlap}

\status{This is work in progress, some sections are ready for review.}

Introducing parallelism into a Mercury program is easy.
A programmer can use the parallel conjunction operator (an ampersand)
instead of the plain conjunction operator (a comma) to tell the compiler
that the conjuncts of the conjunction should be executed in parallel with
one another.
However,
in many places where parallelism can be introduced it will not be
profitable as it is not worth while parallelising small computations.
Making a task available to another CPU
may take thousands of instructions,
spawning off a task that takes only a hundred instructions is clearly a loss.
Even spawning off a task of a few thousand instructions is not a win;
it should only be done for computations
that take long enough to benefit from parallelism.
It is difficult for a programmer to determine if parallelisation of any
particular computation is worth while.
Researchers have therefore worked towards automatic parallelisation.
Autoparallelising compilers have long tried to use granularity analysis to
ensure that they only spawn off computations whose cost will probably exceed
the spawn-off cost by a comfortable margin.
However, this is not enough to yield good results,
because data dependencies may \emph{also} limit
the usefulness of running computations in parallel.
If a spawned off computation blocks almost immediately
and can resume only after another has completed its work,
then the cost of parallelisation again exceeds the benefit.

%This chapter is based on and extends our paper:
%
%\begin{quote}
%\pubauthor{Paul Bone, Zoltan Somogyi and Peter Schachte.}
%% This spelling of parallelisation is okay.
%\pubtitle{Estimating the overlap between dependent computations for automatic
%parallelization}
%\pubhow{Theory and Practice of Logic Programming,}{11(4--5):575--591, 2011.}
%\end{quote}

%\noindent
This chapter presents a set of algorithms for recognising places in a
program where it is worthwhile to execute two or more computations in
parallel,
algorithms that pay attention to the second of these issues as well as the
first.
Our system uses profiling information to compute
times at which a procedure call is expected to consume the values of its
input arguments
and the times at which it is expected to produce the values of its output
arguments.
Given two calls that may be executed in parallel,
our system uses the times of production and consumption
of the variables they share
to determine how much their executions are likely to overlap
when run in parallel,
and therefore whether executing them in parallel is a good idea or not.

We have implemented this technique for Mercury
in the form of a tool
that uses data from Mercury's deep profiler
to generate recommendations about what to parallelise.
The compiler applies these recommendations the next time the program is
compiled.
We present preliminary results that show that
this technique can yield useful parallelisation speedups,
while requiring nothing more from the programmer
than representative input data for the profiling run.

The structure of this chapter is as follows.
Section \ref{sec:overlap_aims} states our two aims for this chapter.
Then Section \ref{sec:overlap_approach} outlines our general approach
including information about the call graph search for parallelisation
opportunities.
Section \ref{sec:overlap_reccalls}
describes how we calculate information about recursive calls missing from
the profiling data.
Section \ref{sec:overlap_coverage}
describes our change to coverage profiling which provides more accurate
coverage data for the new call graph based search for parallelisation
opportunities.
Section \ref{sec:overlap_overlap_alg} describes our algorithm for
calculating the execution overlap between two or more dependent conjuncts.
A conjunction with more than two conjuncts can be parallelised
in several different ways;
section \ref{sec:overlap_howto} shows how we choose the best way.
Section \ref{sec:overlap_pragmatic} discusses some pragmatic issues.
Section \ref{sec:overlap_perf} evaluates
how our system works in practice on some example programs, and
section \ref{sec:overlap_conc} concludes
with comparisons to related work.

\section{Aims}
\label{sec:overlap_aims}

When parallelising Mercury programs,
the best parallelisation opportunities occur
where two goals take a significant and roughly similar time to execute.
Their execution time should be as large as possible
so that the relative costs of parallel execution are small,
and they should be independent to minimise synchronisation costs.
Unfortunately, goals expensive enough to be worth executing in parallel
are rarely independent.
For example, in the Mercury compiler itself,
there are 53 conjunctions containing two or more expensive goals,
but in only one of those conjunctions are the expensive goals independent.
This is why Mercury supports the parallel execution of dependent conjunctions
through the use of futures and a compiler transformation
\citep{wang:2011:dep-par,wang:2006:hons} (Section \ref{sec:backgnd_deppar}).
If the \emph{consumer} of the variable attempts to retrieve the variable's value
before it has been produced, then its execution is blocked
until the \emph{producer} makes the variable available.

\picfigure{overlap_compare}{Ample vs smaller parallel overlap between \code{p} and \code{q}}

Dependent parallel conjunctions differ widely
in the amount of parallelism they have available.
Consider a parallel conjunction with two similarly-sized conjuncts,
\code{p} and \code{q}, that share a single variable \code{A}.
If \code{p} produces \code{A} late but \code{q} consumes it early,
as shown on the right side of figure \ref{fig:overlap_compare},
there will be little parallelism,
since \code{q} will be blocked soon after it starts,
and will be unblocked only when \code{p} is about to finish.
Alternatively, if \code{p} produces \code{A} early
and \code{q} consumes it late,
as shown on the left side of in figure \ref{fig:overlap_compare},
we would get much more parallelism.
The top part of each scenario
shows the execution of the sequential form of the conjunction.

Unfortunately, in real Mercury programs,
almost all conjunctions are dependent conjunctions,
and in most of them,
shared variables are produced very late and consumed very early.
Parallelising them would therefore yield slowdowns instead of speedups,
because the overheads of parallel execution would far outweigh the
benefit of the small amount of parallelism that is available.
We want to parallelise only conjunctions
in which any shared variables are produced early, consumed late,
or (preferably) both;
such computations expose more parallelism.
The first purpose of this chapter is to show how one can find these conjunctions.

\begin{figure}
\begin{center}
\begin{minipage}[b]{0.49\textwidth}
\subfigure[Sequential \mapfoldl]{%
\label{fig:map_foldl_seq}
{\small
\begin{tabular}{l}
\code{map\_foldl(\_, \_, [], Acc, Acc).} \\
\code{map\_foldl(M, F, [X $|$ Xs], Acc0, Acc) :-} \\
\code{~~~~M(X, Y),} \\
\code{~~~~F(Y, Acc0, Acc1),} \\
\code{~~~~map\_foldl(M, F, Xs, Acc1, Acc).} \\
\end{tabular}}
}
\end{minipage}
%
\begin{minipage}[b]{0.49\textwidth}
\subfigure[Parallel \mapfoldl with overlap]{%
\label{fig:map_foldl_par}
{\small
\begin{tabular}{l}
\code{map\_foldl(\_, \_, [], Acc, Acc).} \\
\code{map\_foldl(M, F, [X $|$ Xs], Acc0, Acc) :-} \\
\code{~~~~(} \\
\code{~~~~~~~~M(X, Y),} \\
\code{~~~~~~~~F(Y, Acc0, Acc1)} \\
\code{~~~~) \&} \\
\code{~~~~map\_foldl(M, F, Xs, Acc1, Acc).} \\
\end{tabular}}
}
\end{minipage}

\end{center}
\caption{Sequential and parallel \mapfoldl}
% the recursive call is less dependent
% on the conjunction of the first two calls.
\label{fig:map_foldl}
%\vspace{-2\baselineskip}
\end{figure}

\picfigure{mapfoldl-overlap}{Overlap of \mapfoldl (figure
\ref{fig:map_foldl_par})}

The second purpose of this chapter is to find the best way to parallelise
these conjunctions.
Consider the \mapfoldl predicate in figure~\ref{fig:map_foldl_seq}.
The body of the recursive clause has three conjuncts.
We could make each conjunct execute in parallel,
or we could execute two conjuncts in sequence
(either the first and second, or the second and third),
and execute that sequential conjunction in parallel with the remaining conjunct.
In this case, there is little point in executing
the higher order calls to the \M and \F predicates
%(herein map and fold respectivly)
in parallel with one another,
since in virtually all cases,
\M will generate \code{Y} very late and
\F will consume \code{Y} very early.
However, executing the sequential conjunction of the calls to \M and \F
in parallel with the recursive call \emph{will} be worthwhile
if \M is time-consuming,
because this implies that
a typical recursive call will consume its fourth argument late.
The recursive call processing the second element of the list
will have significant execution overlap (mainly the cost of \M)
with its parent processing the first element of the list
even if (as is typical) the fold predicate generates \code{Acc1} very late.
This parallel version of \mapfoldl is shown in figure
\ref{fig:map_foldl_par}.
A representation of the first three iterations of it 
is shown in figure \ref{fig:mapfoldl-overlap}.
(This is the kind of computation that
Reform Prolog \citep{bevemyr:reform} was designed to parallelise.)

\section{Our general approach}
\label{sec:overlap_approach}

\plan{Goal}
We want to find the conjunctions in the program
whose parallelisation would be the most profitable.
This means finding the conjunctions with conjuncts
whose execution cost exceeds the spawning-off cost by the highest margin,
and whose interdependencies, if any,
allow their executions to overlap the most.
It is better to spawn off a medium-sized computation
whose execution can overlap almost completely
with the execution of another medium-sized computation,
than it is to spawn off a big computation
whose execution can overlap only slightly
with the execution of another big computation,
but it is better still to spawn off a big computation
whose execution can overlap almost completely
with the execution of another big computation.
Essentially, the more the tasks' executions can overlap with one another,
the greater the margin by which
the likely runtime of the parallel version of a conjunction beats
the likely runtime of the sequential version (speedup),
and the more beneficial parallelising that conjunction will be.

\plan{Profiler feedback}
To compute this likely benefit,
we need information
both about the likely cost of calls
and the execution overlap allowed by their dependencies.
A compiler may be able to estimate some cost information from static
analysis.
However, this will not be accurate;
static analysis cannot take into account sizes of data terms or other
values that are only available at runtime.
Therefore, we use profiler feedback information in our implementation.
This was introduced in section \ref{sec:backgnd_autopar}
which also includes a description of Mercury's deep profiler.
To generate the profiler feedback data,
we require programmers to follow this sequence of actions after they have
tested and debugged the program.

\begin{enumerate}
\item
Compile the program
with options asking for profiling
for automatic parallelisation.
\item
Run the program on a representative set of input data.
This will generate a profiling data file.
\item
Invoke our feedback tool on the profiling data file.
This will generate a parallelisation feedback file.
\item
Compile the program for parallel execution,
specifying the feedback file.
The file tells the compiler
\emph{which} sequential conjunctions to convert to parallel conjunctions,
and exactly \emph{how}.
For example, \code{c1, c2, c3} can be converted
into \code{c1 \& (c2, c3)},
into \code{(c1, c2) \& c3}, or
into \code{c1 \& c2 \& c3},
and as the \code{map\_foldl} example shows,
the speedups you get from them can be strikingly different.
\end{enumerate}

\noindent
A visual representation of such a workflow is shown in figure
\ref{fig:prof_fb} on page \pageref{fig:prof_fb}.
It is up to the programmer using our system
to select training input for the profiling run in step 2.
Obviously, programmers should pick input that is as representative as possible,
but the recommended parallelisation can be useful
even for input data that is quite different from the training input.
The main focus of this chapter is on step 3;
we give the main algorithms used by the feedback tool.
However, we will also touch on steps 1 and 4.
We believe that step 2 can only be addressed by the programmer,
as they understand what input is representative for their program.

\plan{DFS \& limits}
Our feedback tool looks for parallelisation opportunities
by doing a depth-first search of the call tree of the profiling run,
each node of which is an SCC (strongly connected component) of procedures.
It explores the subtree below a node in the tree
only if the per-call cost of the subtree is greater than a configurable
threshold,
and if the amount of parallelism it has found at and above that node
is below another configurable threshold.
The first test lets us avoid looking at code
that would take more work to spawn off than to execute,
while the second test lets us avoid creating
more parallel work than the target machine can handle.
Together these tests dramatically reduce the portions of a program that need
analysis,
reducing the time required to search for parallelisation opportunities.

For each procedure in the call tree,
we search its body for conjunctions that contain two or more calls with
execution times above yet another configurable threshold.
This test also reduces the parts of the program that will be analysed
further;
it quickly rejects procedures that cannot contain any profitable
parallelism.
Parallelising a conjunction
requires partitioning the original conjuncts into two or more groups,
with the conjuncts in each group being executed sequentially
but different groups being executed in parallel.
Each group represents a hypothetical sequential conjunction,
and the set of groups represents a hypothetical parallel conjunction.
As this parallel conjunction represents a possible parallelisation of the
original conjunction, we call it a \emph{candidate parallelisation}.
Most conjunctions can be partitioned into several alternative candidate
parallelisations,
for example, we showed above that \mapfoldl has three alternative
parallelisations of its recursive branch.
We use the algorithms of section \ref{sec:overlap_overlap_alg}
to compute the expected parallel execution time of each parallelisation.
These algorithms take into account the runtime overheads of parallel execution.
Large conjunctions can have a very large number of
candidate parallelisations ($2^{n-1}$ for $n$ conjuncts).
Therefore,
we use the algorithms of section \ref{sec:overlap_howto}
to heuristically reduce the number of parallelisations whose expected
execution time we calculate.
If the best-performing parallelisation we find
shows a nontrivial speedup over sequential execution,
we remember that we want to perform that parallelisation on this conjunction.
A procedure can contain several conjunctions with two or more goals that we
consider parallelising,
therefore multiple candidate parallelisations may be generated for different
conjunctions in a procedure.
The same procedure may also appear more than once in the call graph,
and therefore multiple parallelisations may be generated for the same
conjunctions within the procedure.
We discuss how we resolve conflicting recommendations for the same procedure
in section \ref{sec:overlap_pragmatic}.

\paul{Move this paragraph elsewhere, conclusion or introduction?}
An important benefit of profile-directed parallelisation is that
since programmers do not annotate the source program,
it can be re-parallelised easily after a change to the program
obsoletes some old parallelisation opportunities and creates others.
Nevertheless, if programmers want to parallelise some conjunctions manually,
they can do so: our system will not override the programmer.

% \section{Traversing the call graph}
% \label{sec:overlap_dfs}
% 
% % XXX Further work.
% \paul{TODO: This feature is not yet implemented.}
% If the depth first search later finds
% some of the conjuncts to have parallelisable code inside them,
% we revisit this conjunction,
% this time using updated data about the cost of those conjuncts.
% Otherwise,
% we add a recommendation to perform the selected parallelisation
% to the feedback advice we generate for the compiler.



% \paul{This is not yet implemented and will not be for this version of the paper.}
% \peter{Then you need to say that.}

% GREEDY_SEARCH The top level algorithm of the feedback tool
% GREEDY_SEARCH is a traversal of the tree of cliques
% GREEDY_SEARCH recorded in the deep profiling data file.
% GREEDY_SEARCH Each clique has its own unique entry point,
% GREEDY_SEARCH which will be a call site in a higher clique;
% GREEDY_SEARCH this higher clique is the parent node of this clique.
% GREEDY_SEARCH Likewise, every call site
% GREEDY_SEARCH in every procedure in the clique
% GREEDY_SEARCH will be the entry point of another clique,
% GREEDY_SEARCH provided that
% GREEDY_SEARCH (a) it is actually executed and (b) the callee is not in this clique.
% GREEDY_SEARCH These lower cliques are the children of this clique.

% GREEDY_SEARCH % We will describe our traversal algorithm in detail
% GREEDY_SEARCH % in section \ref{sec:bestfirst},
% GREEDY_SEARCH % but for now, consider this traversal
% GREEDY_SEARCH % as operating on a \emph{candidates list},
% GREEDY_SEARCH % a list of cliques sorted on total cost.
% GREEDY_SEARCH Our traversal algorithm operates on a \emph{candidates list},
% GREEDY_SEARCH which contains a list of cliques sorted on total cost.
% GREEDY_SEARCH We start with the list containing only
% GREEDY_SEARCH the clique of the top level call to \code{main},
% GREEDY_SEARCH the predicate where every Mercury program starts execution.
% GREEDY_SEARCH Then, at each step,
% GREEDY_SEARCH \begin{itemize}
% GREEDY_SEARCH \item
% GREEDY_SEARCH we remove the clique at the start of the candidates list;
% GREEDY_SEARCH \item
% GREEDY_SEARCH we process this clique
% GREEDY_SEARCH by looking at the conjunctions in the clique's procedures
% GREEDY_SEARCH to see whether they should be parallelised; and then
% GREEDY_SEARCH \item
% GREEDY_SEARCH we insert the child cliques (if any) of this clique into the candidates list.
% GREEDY_SEARCH \end{itemize}
% GREEDY_SEARCH We stop when either even the highest cost candidate
% GREEDY_SEARCH is too cheap to be worth parallelising,
% GREEDY_SEARCH or we have achieved our target CPU utilisation
% GREEDY_SEARCH for all phases of the program's execution.

% GREEDY_SEARCH This is only an outline of our traversal algorithm.
% GREEDY_SEARCH In section \ref{sec:pragmatic}, we will describe it in detail,
% GREEDY_SEARCH together with our solutions to several issues that come up in practice.


% \zoltan{this is wrong: the overheads should be PART OF the parallel time}
% \begin{equation*}
% Speedup = \frac{Time_{Seq}}{Time_{Par} + ParOverheads}
% \end{equation*}

\section{The cost of recursive calls}
\label{sec:overlap_reccalls}

% leave discussion of granularity estimation by static analysis
% for the related work section;
% mention that this work has not extended to large programs.

The Mercury deep profiler gives us directly
the costs of all non-recursive call sites in a clique.
For recursive calls,
the costs of the callee are mingled together
with the costs of the caller,
which is either the same procedure as the callee,
or is mutually recursive with it.
Therefore if we want to know the cost of a recursive call site (and we do),
we have to infer this
from the cost of the clique as a whole,
the cost of each call site within the procedures of the clique,
the structures of the bodies of those procedures,
and the frequency of execution of each path through those bodies.

For now, we will restrict our attention to SCCs
that contain only a single procedure and where that procedure matches one of
the three recursion patterns below.
These are among the most commonly used recursion patterns and the
inference processes for them are also among the simplest.
Later, we will discuss how partial support could be added for mutually
recursive procedures.

{\bf Pattern 1: no recursion at all.}
This is not a very interesting pattern, but we support it completely.
We do not need to compute the costs of recursive calls if a procedure is not
recursive.

{\bf Pattern 2: simply recursive procedures.}
The first pattern consists of procedures whose bodies
have just two types of execution path through them:
base cases, and recursive cases containing a single recursive call site.
Our example for this category is \code{map\_foldl},
whose code is shown in figure \ref{fig:map_foldl}.

Let us say that of the 100 calls to the procedure,
90 were from the recursive call site
and 10 were from a call site in the parent SCC.
Then we would calculate
that each non-recursive call
(from the parent SCC)
would on average yield nine recursive calls
(from within the SCC).
We call this the average deepest recursion:

\begin{equation*}
AvgMaxDepth = Calls_{RecCallSites} / Calls_{ParentCallSite}
\end{equation*}

The deepest call executes only the non-recursive path,
and incurs only its costs.
The next deepest would take the recursive path,
and incur one copy of the costs of the non-recursive calls along that path,
plus the cost of the last call.
The third last would incur two copies of the costs
of the non-recursive calls along the recursive path,
plus the cost of the last call.
By induction,
the cost of a recursive call at depth $D$ (0 being the deepest)
is:

\begin{equation*}
cost(D) = CostNonRec + D(CostRec + 1)
\end{equation*}

We can now calculate the average cost of a call at any level of the
recursion.
Simply recursive procedures have a uniform number of calls at each depth of
the recursion.
The depth representing the typical use of such a procedure is half of
$AvgMaxDepth$.
This allows us to calculate the typical cost of a recursive call from this
call site.
For example, if \mapfoldl's non-recursive path cost is 10 call sequences
counts (a unit defined in section \ref{sec:backgnd_deep}),
its recursive path cost is 100csc,
and its typical depth is $9/2 = 4.5$.
Then its typical cost is $10 + 4.5(100 + 1) = 464.5$ call sequence counts.

\begin{figure}[tb]
\begin{center}
\begin{minipage}[b][1.9in]{0.49\textwidth}
\subfigure[Accumulator quicksort]{%
\label{fig:quicksort_acc}
\begin{tabular}{l}
\code{quicksort([], Acc, Acc).} \\
\code{quicksort([Pivot $|$ Xs], Acc0, Acc) :-} \\
\code{~~~~partition(Pivot, Xs, Lows, Highs),} \\
\code{~~~~quicksort(Lows, Acc0, Acc1),} \\
\code{~~~~quicksort(Highs, [Pivot $|$ Acc1], Acc).} \\
% Add whitespace to shift the table upwards without also moving the caption.
\\
\\
\\
\end{tabular}
}
\hfill
\end{minipage}
\begin{minipage}[b][1.9in]{0.49\textwidth}
\subfigure[Call graph]{%
\includegraphics[width=0.98\textwidth]{pics/call_tree_dc}
\label{fig:quicksort_acc_callgraph}
}
\hfill
\end{minipage}
\end{center}
\vspace{-2ex}
\caption{Accumulator quicksort, definition and call graph}
\end{figure}

{\bf Pattern 3: Divide-and-conquer procedures.}
The third pattern consists of procedures whose bodies
that also have just two types of execution path through them:
base cases, and recursive cases containing \emph{two} recursive call sites.
Our example for this category is an accumulator version of \quicksortacc,
whose code is shown in figure~\ref{fig:quicksort_acc}.

Calculating the recursion depth of \quicksortacc can be more
problematic.
We know that if a good value for \code{Pivot} is chosen \quicksortacc runs
optimally,
dividing the list in half with each recursion.
Figure \ref{fig:quicksort_acc_callgraph} shows the call graph for such an
invocation of \quicksortacc.
This graph has 15 nodes excluding ``main'',
each node has exactly one call leading to it;
therefore there is one call from outside \quicksortacc's SCC
(the call from ``main''),
and 14 calls within the SCC
(the calls from ``qs'' nodes).
By inspection, there are four complete levels of recursion.
There are always $\log_2(N+1)$ levels in a divide and conquer call graph
with $N$ nodes,
and since there are always $N-1$ calls from within the SCC for a graph with
$N$ nodes then
it follows that,
there are $\log_2(C+2)$ levels for a divide and conquer call graph with $C$
recursive calls.
In this example, $C$ is 14 and therefore there are four levels of recursion
as we noted above by inspection.
If there were two invocations of \quicksortacc from \code{main/2} then there
would be two calls from outside the SCC and 28 calls from within,
in this case the depth is still four.
The average maximum recursion depth in terms of call counts for divide and
conquer code is therefore:

\begin{equation*}
AvgMaxDepth = \log_2 
	\left(\frac{Calls_{RecCallSites}}{Calls_{ParentCallSite}} + 2\right)
\end{equation*}

If consistently worst-case pivots are chosen then \quicksortacc fails to
divide the list at each recursion passing an empty list to one recursive call
and a list whose length is one item shorter than the input list to the other
call.
Pathologically bad cases are rare and are considered bugs;
programmers will usually want to remove such bugs in order to improve
sequential execution performance.
Slight imbalance in a call graph is a more common occurrence,
such situations fall into the same class as those where the profiling data
is not quite representative of the optimised program's future use,
or our estimates of parallel execution overheads do not match the hardware's
actual performance.
Such variations in our calculations' inputs rarely change a ``should
parallelise'' decision into a ``should not parallelise'' decision and
\emph{vice-versa}.
Therefore, we assume that all divide and conquer code is, on average,
evenly balanced.

As before, the deepest call executes only the non-recursive path,
and incurs only its costs.
The next deepest takes the recursive path,
it incurs the costs of the goals along that path,
plus twice the base case's cost, plus the costs of calls to the base case.
By induction,
the cost of a recursive call in a divide and conquer procedure at depth $D$ is:

\begin{equation*}
cost(D) = 2^D{CostNonRec} + 2^{D-1}(CostRec + 1)
\end{equation*}

\plan{interesting depth of d\&c}
Most of the execution of a divide and conquer algorithm occurs at deep
recursion levels as there are many more calls made at these levels than higher
levels.
However, for parallelism the high recursion levels are more interesting:
we know that parallelising the top of the algorithm's call graph can provide
ample coarse-grained parallelism.
\plan{quicksort example}
For example, lets compute the costs of the recursive calls at the top of
\quicksortacc's call graph.
In this example, we gathered data using Mercury's deep profiler on 10
executions of \quicksortacc sorting a list of 32,768 elements.
the profiler reports that there are 655,370 calls into this SCC,
10 of which come from the parent SCC, leaving 655,360 from the two call sites
within \quicksortacc's SCC.
With each recursive call the size of the list is roughly halved,
thus on average there are $\log_{2}(655,360/10) = 16$ levels of recursion.
This is not quite accurate for the case of \quicksortacc;
there are two reasons why:
it is possible to choose a bad value for \code{Pivot} and,
at each level \code{Pivot} is removed from the list before partitioning it.
It is more accurate for mergesort for example.
The total per-call cost of the call site to partition reported by the profiler
is approximately 35.5csc,
it is the only other goal in either the base case or recursive case with a
non-zero cost.
The call at the 16\textsuperscript{th} level of recursion is the call from the
parent SCC into \quicksortacc.
We want to calculate the cost of the calls in the next level down from the
parent,
the 15\textsuperscript{th} level, which is:
$0 + 2^{15 - 1}(35.5 + 1) \approx 591,462$.
This is the average cost of both of the two recursive calls;
depending on how the list was partitioned,
either sub-list could be larger, making the other one
smaller and affecting the costs of the recursive calls similarly.
However the deep profiler does not provide this type of information and we
believe that on average the costs computed here will be good enough.
Since the deep profiler reports that the total per-call cost of the
\quicksortacc SCC is 1,229,106csc, we can see that the above figure makes
sense.

%\begin{algorithm}
%\begin{algorithmic}
%\Procedure{classify}{$Goal$}
%    \Switch{$Goal$}
%        \Case unify or builtin call
%            \State $Res \gets [\reccallres{0}{1}{0}]$
%        \EndCase
%        \Case non-recursive call
%            \State $Res \gets [\reccallres{0}{1}{\cost{Goal}}]$
%        \EndCase
%        \Case recursive call
%            \State $Res \gets [\reccallres{1}{1}{0}]$
%        \EndCase
%        \Case conjunction
%            \State $Res \gets $classify\_conj$($conjuncts$(Goal))$
%        \EndCase
%        
%    \EndSwitch
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}

\begin{figure}
\begin{center}
\begin{tabular}{rlrr}
\C{Line} & \C{Code}         & \C{Coverage}  & \C{Cost}  \\
 1  & \code{p(X, Y, ...) :-}&         100\% &           \\
 2  & \code{~~~~(}          &               &           \\
 3  & \code{~~~~~~~~X = a,} &          20\% &         0 \\
 4  & \code{~~~~~~~~p(...)} &          20\% &           \\
 5  & \code{~~~~;}          &               &           \\
 6  & \code{~~~~~~~~X = b,} &          60\% &         0 \\
 7  & \code{~~~~~~~~q(...)} &          60\% &     1,000 \\
 8  & \code{~~~~;}          &               &           \\
 9  & \code{~~~~~~~~X = c,} &          20\& &         0 \\
10  & \code{~~~~~~~~r(...)} &          20\% &     2,000 \\
11  & \code{~~~~),}         &               &           \\
12  & \code{~~~~(}          &               &           \\
13  & \code{~~~~~~~~Y = d,} &          10\% &         0 \\
14  & \code{~~~~~~~~p(...)} &          10\% &           \\
15  & \code{~~~~;}          &               &           \\
16  & \code{~~~~~~~~Y = e,} &          90\% &         0 \\
17  & \code{~~~~~~~~s(...)} &          90\% &    10,000 \\
18  & \code{~~~~).}         &               &           \\
\end{tabular}
\end{center}
\caption{Two recursive calls and six code paths.}
\label{fig:2_reccalls_4_paths}
\end{figure}

\plan{How we classify recursion type}
We classify recursion types with an algorithm that walks over the structure
of a procedure.
As it traverses the procedure,
it counts the number of recursive calls along each path,
the path's cost and the number of times the path is executed.
When a branching structure like an if-then-else or switch is found,
the algorithm processes each branch independently and then merges its
results at the end of the branch.
If several branches have the same number of recursive calls (including zero)
they can be merged.
If several branches have different numbers of recursive calls they are all
added to the result set.
This means that the result of traversing a goal might include data for
several different recursion counts.
Consider the example in figure \ref{fig:2_reccalls_4_paths}.
The example code has been annotated with coverage information (in the
third column) and with cost information where it is available (forth
column). 
The conjunction on lines six and seven does not contain a recursive call.
The result of processing it is a list containing a single tuple:
\code{[(reccalls: 0, coverage: 60\%, cost: 1,000)]}.
The other conjunction in the same switch (lines three and four) does contain
a recursive call.
The result of processing it is a list containing the tuple:
\code{[(reccalls: 1, coverage: 20\%, cost: 0)]}.
The result for the remaining switch arm (lines 9 and 10) is:
\code{[(reccalls: 0, coverage: 20\%, cost: 2,000)]}.
When the algorithm is finished processing all the cases in the switch it
adds them together;
When adding tuples, we can add tuples together with the same number of
recursive calls by adding their coverage and adding their costs weighted by
coverage (these are per-call costs).
This simplifies multiple code paths with the same number of recursive calls
into a single ``code path''.
The result of processing the switch from line 2--11 is:

\noindent
\begin{center}
\begin{tabular}{l}
\code{[(reccalls: 0, coverage: 80\%, cost: 1,250),}  \\
\code{~(reccalls: 1, coverage: 20\%, cost: ~~~~0)]}. \\
\end{tabular}
\end{center}

\noindent
In this way, the result of processing a goal represents all the possible
code paths through that goal.  In this case there are three code paths
through the switch,
and the result has two entries, one represents the two base case code paths,
the other represents the single recursive case.

The result of processing the other switch in the example,
lines 12--18, is:

\noindent
\begin{center}
\begin{tabular}{l}
\code{[(reccalls: 0, coverage: 90\%, cost: 10,000),}  \\
\code{~(reccalls: 1, coverage: 10\%, cost: ~~~~~0)]}. \\
\end{tabular}
\end{center}

\noindent
In order to compute the result for the whole procedure, we must compute the
product of these two results;
this computes all the possible paths through the two switches.
We do this by constructing pairs of tuples from the two lists.
Since each list has two entries there are four pairs:

\noindent
\begin{center}
\begin{tabular}{rcl}
\code{[   (rc: 0, cvg: 80\%, cost: ~1,250)} &
    $\times$&
    \code{(rc: 0, cvg: 90\%, cost: 10,000),}
    \\
\code{   ~(rc: 0, cvg: 80\%, cost: ~1,250)} &
    $\times$&
    \code{(rc: 1, cvg: 10\%, cost: ~~~~~0),}
    \\
\code{   ~(rc: 1, cvg: 20\%, cost: ~~~~~0)} &
    $\times$&
    \code{(rc: 0, cvg: 90\%, cost: 10,000),}
    \\
\code{   ~(rc: 1, cvg: 20\%, cost: ~~~~~0)} &
    $\times$&
    \code{(rc: 1, cvg: 10\%, cost: ~~~~~0)]}
    \\
\end{tabular}
\end{center}

\noindent
For each pair we compute a new tuple by adding the number of recursive
calls, averaging the coverage counts, and adding the costs.

\noindent
\begin{center}
\begin{tabular}{l}
\code{[   (rc: 0, cvg: 72\%, cost: 11,250),} \\
\code{   ~(rc: 1, cvg: ~8\%, cost: ~1,250),} \\ 
\code{   ~(rc: 1, cvg: 18\%, cost: 10,000),} \\
\code{   ~(rc: 2, cvg: ~2\%, cost: ~~~~~0),} \\
\end{tabular}
\end{center}

\noindent
Again, we can merge the cases with the sane numbers of recursive calls.

\noindent
\begin{center}
\begin{tabular}{l}
\code{[   (rc: 0, cvg: 72\%, cost: 11,250),} \\
\code{   ~(rc: 1, cvg: 26\%, cost: ~1,319),} \\ 
\code{   ~(rc: 2, cvg: ~2\%, cost: ~~~~~0)]} \\
\end{tabular}
\end{center}

\noindent
There are six paths through this procedure,
and two recursive calls.
However, we can represent all six paths using just three tuples,
one for each path type.
This allows us to conveniently handle procedures of different forms as
rather simple recursion types such as ``simply recursion'' we saw above:
a procedure with two recursive paths with one call each can be handled as
if it has just one recursive path and one base case.

We can determine the type of recursion for any list of recursion path
information.
If there is a single path entry with zero recursive calls
then the procedure is not recursive.
If there is an entry for a path with zero recursive calls,
and a path with one recursive call
then the procedure is ``simply recursive''.
Finally if there are two paths, one with zero recursive calls and one with
two recursive calls, then we know that the procedure
uses ``divide and conquer'' recursion.
It is possible to generalise further, if a procedure has two entries,
one with zero recursive calls and the other with some $N$ recursive calls.
Then the recursion pattern is similar to divide and conquer except that the
base in the formulas shown above is $N$ rather than 2.
We have not found it necessary to handle these cases.

\begin{table}
\begin{center}
\begin{tabular}{l|rrr}
Recursion Type & No.\ of SCCs       & Percent & Total cost \\
\hline
Not recursive  &            292,893 & 78.07\% & 2,320,270,385 \\
Simple recursion&            48,458 & 12.92\% &   402,430,967 \\
Divide and conquer&           1,917 &  0.51\% &     5,337,293 \\
Mutual recursion: 2 procs &   4,066 &  1.08\% &    27,099,504 \\
Mutual recursion: 3 procs &   9,393 &  2.50\% &    14,846,076 \\
Mutual recursion: 4 procs &   1,092 &  0.29\% &    12,542,308 \\
Mutual recursion: 5 procs &   1,035 &  0.28\% &     3,863,295 \\
Mutual recursion: other   &   3,707 &  0.99\% &   139,975.394 \\

Other recursion with rec-branches: 1, 2
                           &     44 &  0.01\% &       580,994 \\
Other recursion with rec-branches: 2, 3
                           &    281 &  0.07\% &       189,377 \\
Other recursion with rec-branches: 2, 3, 4
                           &    564 &  0.15\% &     3,902,188 \\
Other multi recursive-branch:
                           &    200 &  0.05\% &         5,908 \\
Unknown (built-in \& foreign language code) 
						   & 10,623 &  2.83\% &           364 \\
Unknown (error)            &  1,180 &  0.32\% &     8,838,291 \\
\end{tabular}
\end{center}
\caption{Survey of recursion types in an execution of the Mercury compiler}
\label{tab:recursion_types}
\end{table}

\plan{Recursion type survey}
There are many possible types of recursion,
and we wanted to limit our development effort to just those recursion types
that would occur often enough to be important.
Therefore,
we ran our analysis across all the SCCs in the Mercury compiler,
the largest open source Mercury program,
to determine which types of recursion are most common.
Table \ref{tab:recursion_types} summarises the results.
We can see that most SCCs are not recursive,
and the next biggest group is the simply recursive SCCs,
accounting for nearly 13\% of the profile's SCCs.
If our analysis finds an SCC with more than one procedure,
it counts the number of procedures and marks the whole SCC as
mutually recursive and does no further analysis.
It may be possible to perform further analysis on mutually recursive SCCs,
but we have not found it important to do this yet.
Mutual recursion as a whole accounts for a larger proportion of procedures
than divide and conquer.
The ``Other'' recursion types refer to cases where there are multiple
recursive paths through the SCC's procedure with different numbers of
recursive calls plus a base case,
we have not classified these cases further.
The table row labelled ``Unknown (error)'' refers to cases that our algorithm
could not handle.
These are primarily procedures that may backtrack because they are either
\dnondet or \dmulti.

The profiler represents each procedure in the program's source code as a \PS
structure (section \ref{sec:backgnd_deep}).
Each procedure may be used multiple times and therefore have multiple \PD
structures appearing in different SCCs.
Each SCC may have a different recursion type depending on how it was called.
For example, calling \code{length/2} on an empty list will not execute the base
case and therefore this \emph{use} of \code{length/2} is non-recursive.

Some of the ``Other'' recursion type cases are due to the implementation of the
\code{map} ADT and code in Mercury's standard library,
which uses a 2--3--4 tree implementation, and hence many of the
multi-recursive path cases with 2, 3 or 4 recursive calls on a path
are often 2--3--4 tree traversals.
These traversals also account for some of the other multi recursive path
cases,
such as those with 2 or 3 recursive calls on a path.
these may be due to the same code running on a tree without any
4-nodes.

\begin{figure}
\begin{center}
\subfigure[One loop]{%
\label{fig:mutrec1}
\includegraphics[scale=0.75]{pics/mutrec1}
}
%
\hspace{0.09\textwidth}
%
\subfigure[Two loops]{%
\label{fig:mutrec2}
\includegraphics[scale=0.75]{pics/mutrec2}
}
%
\hspace{0.09\textwidth}
%
\subfigure[Three loops]{%
\label{fig:mutrec3}
\includegraphics[scale=0.75]{pics/mutrec3}
}
\end{center}
\caption{Mutual recursion}
\label{fig:mutrec}
\end{figure}

\plan{Mutual recursion}
We can handle simple cases of mutual recursion by a process of hypothetical
inlining.
Consider the call graph in figure \ref{fig:mutrec1}.
In this example $f$, $g$ and $h$ represent an SCC,
$f$ calls $g$, which calls $h$, which calls $f$.
These calls are recursive, they create a loop between all three procedures.
Prior to doing the recursion path analysis we could inline each of these
procedure's representations inside one another as follows:
inline $h$ into $g$ and then inline $g$ into $f$.
This creates a new pseudo-procedure that is equivalent to the loop between the
three procedures.
We can then run the recursion path analysis on this procedure and apply the
results to the three original procedures.
We have not yet needed to implement call site cost analysis for mutually
recursive procedures;
therefore this discussion is a thought experiment and not part of our
analysis tool.

We believe that we can handle some other forms of mutually recursive code.
One example of this is  the graph in figure \ref{fig:mutrec2},
which has two loops, one mutually recursive loop and one loop within $g$.
We cannot inline $g$ into $f$ because $g$ has a recursive call whose
entry point would disappear after inlining.
But we can duplicate $f$, creating a new $f\prime$, and re-write $g$'s call
to $f$ as a call to $f\prime$.
This allows us to inline $f\prime$ into $g$ without an issue, and $f$'s call
to $g$ is no longer recursive.

The graph in figure \ref{fig:mutrec3} is more complicated,
it has three loops.
In this case we cannot inline $g$ into $f$ because of the loop within $g$,
and we cannot inline $f$ into $g$ because of the loop within $f$.
Simple duplication of either $f$ or $g$ does not help us.

\plan{What is unimplemented}
Our current implementation handles non-recursive and simply recursive
loops within a single procedure.
We have shown how to calculate the cost of recursive calls in divide and
conquer code.
However, generating efficient parallel code is slightly more complicated,
as Granularity Control is needed to prevent embarrassingly parallel
workloads (Section \ref{sec:rts_work_stealing2}).
This is less true with simply recursive code as a non-recursive call that is
parallelised against will usually have a non-trivial cost in all levels of
the recursion.

\section{Deep coverage information}
\label{sec:overlap_coverage}

\status{This section is ready for review.}

\paul{This section and the previous one have a circular dependency.
However it is useful to put this section after the previous one as the
previous one provides motivation for this section and this section has an
example that relies on understanding the previous section.}
    
Our auto-parallelism feedback tool is an extension of the Mercury deep
profiler,
which is able to provide ancestor call chain specific (\emph{deep})
profiling data.
Deep profiling data is extremely valuable if not necessary for profiling
programs in declarative languages.
This also applies to profiler directed optimisations such as
auto-parallelisation.
This means that our algorithms should therefore use deep profiling data to
compute auto-parallelisation feedback specific to ancestor call chains;
this includes the variable use time algorithm (section
\ref{sec:backgnd_var_use_analysis})
and the recursive call cost calculation (above).
These two analyses use coverage data to compute how often different branches
in procedures are taken.
Coverage data is collected by the profiler's coverage profiling feature
(section \ref{sec:backgnd_coverage}),
which records how many times execution reaches each point in the program,
the \emph{coverage} of that program point.
It is possible to calculate the coverage of \emph{most} program points using
only the call counts associated with call sites.
Coverage profiling introduces some extra profiling instrumentation and
allows coverage to be inferred for \emph{all} program points.

\plan{Explain the shallow coverage information.}
Until now,
the profiler provided deep cost data,
but only \emph{static} coverage data.
The static coverage data is collected and stored in the profiler's \PS
structures where it cannot be associated with a specific ancestor context of
a procedure.
The data is represented by using two arrays and an integer describing the
arrays' lengths.
Each corresponding pair of slots in the arrays refers to a single coverage
point in the procedure.
The first array gives static information such as the type of coverage point and
its location in the compiler's representation of the procedure.
The second array contains the current value of the coverage point.
Each time execution reaches the coverage point this value is incremented.

\begin{figure}
\begin{center}
\begin{tabular}{r|rr|rr|l}
\C{\textbf{Line}} &
\multicolumn{2}{c|}{\textbf{Static}} & \multicolumn{2}{c|}{\textbf{Deep}} &
    \C{\textbf{Code}} \\
&
\C{\textbf{Coverage}} & \Cbr{\textbf{Cost}} & 
\C{\textbf{Coverage}} & \Cbr{\textbf{Cost}} & \\ 
\hline
 1 & 7,851 & 3,358 & 2,689 & 3,898,634 & \code{foldl3(P, Xs0, !Acc1, ...) :-} \\
 2 &     - &     - &     - &         - & \code{~~~~(} \\
 3 & 2,142 &     0 &     1 &         0 & \code{~~~~~~~~Xs0 = []} \\
 4 &     - &     - &     - &         - & \code{~~~~;} \\
 5 & 5,709 &     0 & 2,688 &         0 & \code{~~~~~~~~Xs0 = [X $|$ Xs],} \\
 6 & 5,709 & 1,259 & 2,688 &     1,449 & \code{~~~~~~~~P(X, !Acc1, ...),} \\
 7 & 5,709 & 2,097 & 2,688 & 3,897,182 & \code{~~~~~~~~foldl3(P, Xs, !Acc1, ...)} \\
 8 &     - &     - &     - &         - & \code{~~~~).} \\
\end{tabular}
\end{center}
\caption{Static and Deep coverage and cost data for \foldlthree}
\label{fig:static_and_deep_coverage}
\end{figure}

\plan{Show a problem, use an example.}
Our automatic parallelisation tool uses deep profiling data throughout.
However, using deep profiling data and static coverage data creates
inconsistencies that can result in erroneous results.
Figure \ref{fig:static_and_deep_coverage} shows \foldlthree from
Mercury's standard library;
it is annotated with data collected from a profiling run of the Mercury
compiler.
Each line of code is shown with static and deep profiling data to its left.
The static data is shown in the first two columns and the dynamic data in
the next two.
Within each group
the first of column shows the code coverage, the number of times this code was
executed,
and the second shows the average per-call cost of executing this code
measured in call sequence counts.
Using the static coverage data we can see that \foldlthree is called 7,851 times
(line one)
throughout the execution of the compiler,
this includes recursive calls.
There are 5,709 directly recursive calls (the coverage on line seven).
Using these figures we can calculate that the average deepest recursion is 
2.66 levels.
The static cost data provided by the profiler includes the average per-call
cost for the procedure as a whole on line one,
and the average per-call cost for the call to P on line six.
We calculated the average cost of the recursive call (line seven) using the
methods in the previous section for the depth of 1.66,
which is the first level down from the top of \foldlthree's call tree
(the first recursive call).
We can conclude that on average across the whole program,
\foldlthree does not make very deep recursions
(it is called on short lists),
and that the cost of the higher order call is cheap.

When we consider the \emph{deep} profiling information specific to a
particular ancestor call chain,
indecently a use of \foldlthree in the compiler's mode-checker,
we see a case where there is only a single non-recursive call and 2,688
recursive calls.
This recursion is rather deep having a maximum depth of 2,689 levels.
Without the deep coverage information in the third column of the table we
would not be able to compute this recursion depth accurately.
Using only static information we would have computed the depth of 2.66
as above.
This incorrect recursion depth could cause problems if we tried to analyse
this loop.
Additionally, coverage data is necessary to accurately compute the cost of
the recursive call.
For example,
the true cost of the recursive call at the second level of the recursion is 
3,897,182
(calculated using the deep coverage data).
Had we computed this using the erroneous depth from the static coverage
data,
the result would be 4,464.
It is easy to understand how this discrepancy could affect a later analysis
such as the variable use time analysis.

\plan{Explain how we gather deep coverage info, and why it fixes the
problem.}
Each procedure in the program is associated with a single \PS structure
and any number of \PD structures.
Each \PD structure links the procedure to one of its uses in the program's
call graph (modulo recursive calls).
Therefore we can move the array containing the coverage points' execution
counts into the \PD structure.
There are more \PD structures than \PS structures in a program's
profile;
this will make the profile larger, consuming more memory and potentially
affecting performance.
It is important to minimise a profiler's impact on performance.
Poor performance both affects the user's experience and 
increases the risk of distorting the program's profile.
However, we do not have to worry about distortion as our unit of time,
call sequence counts,
cannot be distorted.

\begin{table}
\begin{center}
\begin{tabular}{l|rrr|rrr}
\Cbr{\textbf{Profiling type}} &
\multicolumn{3}{c|}{\textbf{Without optimisations}} &
\multicolumn{3}{c}{\textbf{With optimisations}} \\
 & Time & Heap growth & Profile size &
   Time & Heap growth & Profile size \\
\hline
no profiling     & -     &     - &    - & 10.8s & 182MB & - \\
no coverage      & 55.1s & 410MB & 36MB & 35.9s & 312MB & 24MB \\
static coverage  & 56.9s & 410MB & 36MB & 37.3s & 312MB & 25MB \\
dynamic coverage & 58.0s & 466MB & 42MB & 38.0s & 362MB & 31MB \\ 
\end{tabular}
\end{center}
\caption{Coverage profiling overheads}
\label{tab:coverage_prof_overheads}
\end{table}

\plan{Benchmark coverage profiling.}
Table \ref{tab:coverage_prof_overheads} shows the overheads of profiling 
the Mercury compiler, including coverage profiling.
We compiled the Mercury compiler with seven different sets of options for
profiling.
The rows of the table give results for no profiling support,
profiling support without coverage profiling,
static coverage profiling support,
and dynamic coverage profiling support.
The two groups of result columns show results with certain optimisations
disabled and enabled.
the non-profiling version of the compiler was not compiled or tested with
optimisations disabled.
The three result columns in each of the two column groups show:
the user time, an average of 20 executions while compiling the eight largest
modules of the compiler itself;
the \emph{heap growth}, the amount by which the program break\footnote{
    The program break marks the end of the heap area.
    See the \code{sbrk(2)} Unix system call}
moved during the execution;
and the size of the resulting profiling file.

Certain optimisations, such as inlining,
transform a program in ways that would make it hard for a programmer to
recognise their program's profile.
We normally disable such optimisations when compiling with profiling
support.
However automatic parallelisation like other optimisations is used to
speedup a program, and therefore normally used with other optimisations.
The compiler performs parallelisation after most other optimisations,
and therefore it operates on a version of the program that has already had
these optimisations applied.
The feedback tool must also operate on a similar version of the program;
so that its feedback can be matched against the compiler's representation of
the program.
We must enable optimisations when compiling profiling versions of programs
for the purpose of automatic parallelisation (or other types of profiler
feedback)
Therefore we have presented profiling results with optimsations both
disabled (as one would do when using the profiler by hand)
and enabled (as one would do when using profile feedback directed
optimisations).

We can see that enabling profiling slows the program down by at least a
factor of three,
or a factor of five if optimisations are disabled.
While this is a considerable slow down,
we would like to remind the reader that the profile cannot be distorted by
such a slowdown as we use call sequence counts as our unit of time.
The difference between the results with and without optimisations is
unsurprising.
It is well established that simple optimisations such as inlining can make a
significant difference to performance.
As optimisations make the program simpler, the call graph also becomes
simpler.
For example, inlining will reduce the number of procedures in the program,
which means that fewer data structures are used to represent them in the
program's memory and in the data file.
This is why the results show that the optimised programs use less memory
and generate smaller profiling data files.
It can also have a secondary effect on performance, the fewer procedures
there are the less instrumentation there will be which improves the
program's performance.

Enabling static coverage profiling does not significantly impact the memory
usage or profile size of the program.
It does slightly affect the program's performance.
Associating coverage data with \PD structures rather than \PS structures
measurably affects memory usage,
but the amount of memory usage (an additional 56MB or 50MB
(without and with optimisations respectivly)
is acceptable.
Collecting deep coverage information does have a slight performance impact over
collecting static coverage information,
but the difference is minimal.
The small cost of collecting deep coverage data is small compared to the
benefit that this data provides,
especially when optimisations are enabled during a profiling build.


\section{Calculating the overlap between dependent conjuncts}
\label{sec:overlap_overlap_alg}

\status{Currently revising this section}

The previous two sections provide us with the information necessary to
estimate a candidate parallelisation's performance,
which is the topic of this section.
As stated earlier,
our goal is to recognise how dependencies affect the amount of parallelism
in parallel conjunctions.
The amount of elapsed time that can be saved by parallel execution is
represented by the amount that the boxes overlap in diagrams such as
figure~\ref{fig:overlap_compare}.
Figuring out this \emph{overlap}
in the parallel executions of two dependent conjuncts
requires knowing, for each of the variables they share,
when that variable is generated by the first conjunct and
when it is first consumed by the second conjunct.
The algorithms for calculating when variables are produced and consumed
(variable use times)
are described in section \ref{sec:backgnd_var_use_analysis}.
The variable use analysis implementation has been updated since my honours
candidate to make use of the deep coverage information described in the
previous section.

% Show formulas for calculating the overlap in a simple case, one
% shared variable between two conjuncts.

Suppose a candidate parallel conjunction has two conjuncts $p$ and $q$,
and their execution times in the original, sequential conjunction $p, q$,
are ${SeqTime}_p$ and ${SeqTime}_q$.
Suppose ${SV}_i$ are the variables shared between them,
and for each ${SV}_i$,
the time at which $p$ produces it is ${ProdTime}_{pi}$, and
the time at which $q$ consumes it is ${ConsTime}_{qi}$.

If we denote the execution times of the conjuncts
in the parallel conjunction $p~\&~q$
as ${ParTime}_p$ and ${ParTime}_q$,
then the expected speedup
from parallelising the original sequential conjunction
is ${Speedup} = {SeqTime} / {ParTime}$,
where ${SeqTime} = {SeqTime}_p + {SeqTime}_q$,
and ${ParTime} = {SpawnOverhead} + {max}({ParTime}_p, {ParTime}_q)$.
The profile gives us ${SeqTime}_p$ and ${SeqTime}_q$,
and if we ignore overheads for now (we will come back to them later),
then ${ParTime}_p$ will always be equal to ${SeqTime}_p$.
The main task of computing the speedup
therefore consists of computing ${ParTime}_q$;
as we saw in figure \ref{fig:overlap_compare},
this will differ from ${SeqTime}_q$
whenever $q$ needs to wait for $p$ to produce a shared variable.

\iclp{
\begin{algorithm}[tb]
\begin{algorithmic}[1]
\Procedure{find\_par\_time}{$Conjs$}
    \State $ProdTimeMap \gets empty$
    \State $TotalParTime \gets 0$
    \For{$i = 1$ to length($Conjs$)}
        \State $CurSeqTime \gets 0$
        \State $CurParTime \gets 0$
        \State $VarUsesList_i \gets$ get\_variable\_uses($Conj_i$)
        \State sort $VarUsesList_i$ by time
        \For{$(Var_{i, j}, Time_{i, j}) \in VarUsesList_i$}
            \State $Duration_{i, j} \gets Time_{i, j} - CurSeqTime$
            \State $CurSeqTime \gets CurSeqTime + Duration_{i, j}$
            \If{$Conj_i$ produces $Var_{i, j}$}
                \State $CurParTime \gets CurParTime + Duration_{i, j}$
                \State $ProdTimeMap$[$Var_{i, j}$]~$ \gets CurParTime$
            \Else
                \Comment $Conj_i$ must consume $Var_{i, j}$
                \State $ParWantTime \gets CurParTime + Duration_{i, j}$
                \State $CurParTime \gets$
                    max($ParWantTime, ProdTimeMap$[$Var_{i, j}$])
            \EndIf
        \EndFor
        \State $DurationRest_i \gets SeqTime_i - CurSeqTime$
        \State $CurParTime \gets CurParTime + DurationRest_i$
        \State $TotalParTime \gets$ max($TotalParTime, CurParTime$)
    \EndFor
    \State \Return $TotalParTime$
\EndProcedure
\end{algorithmic}
\caption{Dependent parallel conjunction algorithm}
\label{alg:dep_par_conj_overlap_middle}
%\vspace{-2\baselineskip}
\end{algorithm}

Algorithm \ref{alg:dep_par_conj_overlap_middle} shows
a simplified version of the algorithm we use to compute
the expected execution time of a conjunction
when its conjuncts are executed in parallel,
assuming an unlimited number of CPUs.
The input of the algorithm is $Conjs$,
the conjuncts themselves.
%$ProdConsList$,
%which gives, for each conjunct,
%the list of its input and output variables,
%together with the times at which,
%% (during the profiling run, which executes the conjuncts in sequence)
%in a sequential execution,
%they are respectively first consumed or produced.
%The times are relative to the start of the execution of the relevant conjunct.
The main task of the algorithm is
to divide the execution times of all the conjuncts into chunks
and keep track of when those chunks can execute.
The execution time of $Conj_i$
has one chunk ($Duration_{i, j}$) for each of $Conj_i$'s shared variables
that ends at the time at which that variable is produced or first consumed,
and there is one chunk $DurationRest_{i}$ at the end,
during which the call may produce non-shared variables.
Figure \ref{fig:overlap_compare} shows that
the production of $A$ divides $p$ into two chunks, ${pA}$ and ${pR}$,
while the consumption of $A$ divides $q$ into ${qA}$ and ${qR}$.

The algorithm processes the chunks in order, and keeps track
of the sequential and parallel execution times of the chunks so far.
When a chunk of $Conj_i$ ends with the production of a variable,
we record when that variable is produced (line 14),
and the next chunk can start executing immediately.
When a chunk ends with the consumption of a variable,
then in the \emph{sequential} version of $Conj_i$
the next chunk can also execute immediately,
since the values of all the input variables will be available when it starts,
but in the \emph{parallel} version,
the variable may not have been produced yet.
If it has, then $Conj_i$ does not need to wait for it;
the left side of figure \ref{fig:overlap_compare} shows this case.
However, it is also possible that it has not.
In that case, $Conj_i$ will suspend on the variable,
and will resume only when its producer signals that it is available;
the right side of figure \ref{fig:overlap_compare} shows this case.
These two alternatives are handled by line 17.
Note that $Var_{i, j}$
will always be in $ProdTimeMap$ when we look for it,
because it must have been produced by an earlier conjunct
and therefore processed by an earlier iteration of the loop over the
conjuncts (the loop starting on line 4).
In the simple case of a two-conjunct conjunction,
the left conjunct,
which must produce all the shared variables,
will be handled by the first iteration;
and the right conjunct,
which must consume all the shared variables,
will be handled by the second (and last) iteration.
In the general case of longer conjunctions,
conjuncts that are neither the left and right most conjuncts
can both consume variables produced by conjuncts on their left
and produce variables consumed by conjuncts on their right.
This is why our algorithm must include both productions and consumptions
in each $VarUsesList_i$:
the list of shared variables that $Conj_i$ either produces or consumes,
together with their times of production and first consumption respectively.

The version of this algorithm we have actually implemented is
a bit longer than the one in algorithm \ref{alg:dep_par_conj_overlap_middle},
because it also accounts for several forms of overhead:

\begin{itemize}
\item
Creating a spark and adding it to a work queue has a cost.
Every conjunct but the last conjunct incurs this cost
to create the spark for the rest of the conjunction.
\item
It takes some time to take a spark off a spark queue,
create or reuse a context for it, and start its execution.
Every parallel conjunct that is not the first incurs this delay
before it starts running.
\item
The signal and wait operations on shared variables' futures have a cost.
\item
It takes some time to wake up a context that was waiting on a future.
\item
It takes time for each conjunct to synchronise on the barrier
when it has finished its job.
\end{itemize}

\noindent
We can account for every one of these overheads
by adding the estimated cost of the relevant operation to $CurParTime$
at the correct point in the algorithm.

In many cases,
the conjunction given to algorithm \ref{alg:dep_par_conj_overlap_middle}
will contain a recursive call.
In these cases the recursive call's cost at its average recursion depth is
used to determine if the parallelisation is worth-while.
This assumes that the recursive call
calls the \emph{original, sequential} version of the procedure.
When the recursive call calls the parallelised version,
we can expect a similar saving (absolute time, not ratio)
on \emph{every} recursive invocation.
How this affects the expected speedup of the top level call
depends on the structure of the recursion.
We do not currently do any further analysis when parallelising recursive
code.
However, our current approach handles the 78\% majority of cases correctly,
and a further 13\% of cases (single recursion) reasonably well.

So far, we have assumed an unlimited number of CPUs,
which is of course unrealistic.
If the machine has e.g.\ four CPUs,
then the prediction of any speedup higher than four is obviously invalid.
Less obviously,
even a predicted overall speedup of less than four may depend
on more than four conjuncts executing all at once at \emph{some} point.
We have not found this to be a problem yet.
If and when we do,
we intend to extend our algorithm to keep track
of the number of active conjuncts in all active time periods.
Then if a chunk of a conjunct wants to run in a time period
when all CPUs are predicted to be already busy executing previous conjuncts,
we assume that the start of that chunk is delayed until a CPU becomes free.

% \paul{XXX: Not implemented.  I think I vaguly remember this}
% The limited number of CPUs also means that
% there is a limit to how much parallelism we actually \emph{want}.
% The spawning off of every conjunct incurs overhead,
% but these overheads do not buy us anything if all CPUs are already busy.
% % If the machine has e.g.\ four CPUs,
% % then we do not actually want to spawn off
% % hundreds of iterations for parallel execution,
% % since parallel execution actually has several forms of overhead:
% That is why our system supports \emph{throttling}.
% If a conjunction being parallelised contains a recursive call,
% then the compiler can be asked to replace the original sequential conjunction
% not with the parallel form of the conjunction,
% but with an if-then-else.
% The condition of this if-then-else
% will test at runtime
% whether spawning off a new job is a good idea or not.
% If it is, we execute the parallelised conjunction, but
% if it is not, we execute the original sequential conjunction.
% The condition is obviously a heuristic.
% If the heuristic allows the list of runnable jobs to become empty,
% then we will not have any work to give to a CPU
% that finishes its task and becomes available.
% On the other hand,
% if the heuristic allows the list of runnable jobs to become too long,
% then we incur the overheads of spawning off some jobs unnecessarily.
% Currently, on machines with $N$ CPUs,
% we prefer to have a total of $M$ running and runnable jobs where $M > N$,
% so our heuristic stops spawning attempts
% if and only if the queue already has $M$ entries.
% Our current system by default sets $M$ to be $32$ for $N = 4$,
% though users can easily override this.

The overheads of parallel execution can also affect conjunctions
that do not contain recursive calls:
a conjunction that looks worth parallelising if you ignore overheads
may look not worth parallelising if you take them into account.
This is why our system actually uses
a version of algorithm~\ref{alg:dep_par_conj_overlap_middle}
that accounts for overheads.

% Algorithm~\ref{alg:dep_par_conj_overlap_complete}
% can also handle $n$-way conjunctions for $n>2$.
% Since the Mercury mode system reorders conjunctions
% to ensure that data flows only to the right,
% in a two-conjunct conjunction,
% the left conjunct can only produce
% the variables it shares with the right conjunct
% and the right conjunct can only consume
% those variables.
% However, in longer conjunctions,
% the conjuncts in the middle
% can both consume variables produced by conjuncts on their left
% and produce variables consumed by conjuncts on their right.
% This is why our algorithm associates with \code{Conj\_i}, the $i$th conjunct,
% \code{ProdConsList\_i}:
% the list of shared variables that \code{Conj\_i} either produces or consumes,
% together with their times of production and first consumption respectively.
% This is a generalisation of \code{ConsList\_q} in
% algorithm~\ref{alg:dep_par_conj_overlap_simple}.
% We also need to generalise \code{Prod\_pi},
% because the time at which a non-first conjunct produces a variable
% can and usually will be affected
% by the overheads and/or synchronisation delays suffered by that conjunct.
% This is why we use \code{ProdTimeMap},
% which maps each shared variable to its time of production.

% The main body of the algorithm consists of two nested loops.
% The outer loop loops over all the conjuncts from left to right,
% because the execution of a conjunct can be affected
% by the conjuncts to its left
% (through the time at which they produce the shared variables it consumes),
% but not by the conjuncts to its right.
% The first few lines of the outer loop body
% (the first two assignments to \code{CurParTime})
% compute for each conjunct
% the time at which that conjunct can start execution.

% The inner loop loops over all the components of the current conjunct,
% such as \code{pA} and \code{pR}
% from figure~\ref{fig:overlap_compare}.
% Just as our previous algorithm did,
% this loop updates the simulated current time
% in both the original sequential execution of the conjunct (\code{CurSeqTime})
% and in its modified parallelised execution (\code{CurParTime}).
% For time components that end in the consumption of a variable,
% we do what we did before,
% but also reflect the cost of the wait operation needed for the consumption.
% For time components that end in the production of a variable,
% we record the time at which
% that variable would be available in the parallel execution;
% this will be when the producer finishes executing the signal operation on it.

% We use \code{TotalParTime} to keep track of the ending time
% of the parallel conjunct that ends last.
% We also remember, in \code{FirstConjTime},
% the time at which the first conjunct finishes.
% The reason we do this is because
% our runtime system requires that
% when the parallel conjunction finishes,
% execution must continue in the context
% that entered the parallel conjunction in the first place.
% In our implementation, this context will execute the first conjunct.
% If the last conjunct to finish is the first conjunct,
% it can continue on without delay;
% if the last conjunct to finish is some other conjunct,
% then we need to free its context,
% and switch to executing the original context,
% which became idle when the first conjunct finished.
% The last two lines reflect this cost.

%XXX end of iclp
}
\tr{
\paul{Currently here.}

\begin{algorithm}
\begin{algorithmic}
\State $CurSeqTime \gets 0$
\State $CurParTime \gets 0$
\State sort $VarUseList_q$ on $Time_{q, i}$
\For{$(Var_i, Time_{q, i}) \in VarUseList_q$}
    \State $Duration_{q, i} \gets ConsTime_{q, i} - CurSeqTime$
    \State $CurSeqTime \gets CurSeqTime + Duration_{q, i}$
    \State $ParWantTime_{q, i} \gets CurParTime + Duration_{q, i}$
    \State $CurParTime \gets$ max($ParWantTime_q, ProdTime_{p, Var_i}$)
\EndFor
\State $DurationRest_q \gets SeqTime_q - CurSeqTime$
%\State $SeqTime_q \gets CurSeqTime_q + DurationRest_q$
\State $ParTime_q \gets CurParTime + DurationRest_q$
\end{algorithmic}
\caption{Dependent parallel conjunction overlap calculation}
\label{alg:dep_par_conj_overlap_simple}
\end{algorithm}

Algorithm~\ref{alg:dep_par_conj_overlap_simple} shows
a simple version of the algorithm we use to compute ${ParTime}_q$.
Its main input is ${ConsList}_q$,
a list of the variables shared by $p$ and $q$,
together with their times of consumption by $q$.

The main task of the algorithm is to divide up ${SeqTime}_q$ into chunks,
and keep track of when those chunks can execute.
There is one chunk (${Duration}_{qi})$ for each shared variable
that ends at the time at which that variable is first consumed,
and there is one chunk ${DurationRest}$
after the consumption of the last shared variable.
(Figure~\ref{fig:overlap_compare}
shows the former as ${qA}$ and the latter as ${qR}$.)
The algorithm keeps track of the sequential and parallel execution times of $q$
up to the consumption of the current shared variable.
In the sequential version,
each chunk can execute immediately after the previous chunk,
since the values of the shared variables are all available when $q$ starts.
In the parallel version,
$p$ is producing the shared variables while $q$ is running.
If $p$ has produced the value of ${SV}_i$ by the time $q$ needs it,
there $q$ does not need to wait for it;
the left side of figure~\ref{fig:overlap_compare} shows this case.
However, it is also possible that $p$ will produce ${SV}_i$
only after the time at which $q$ would like to use it.
In that case, $q$ will suspend on ${SV}_i$,
and will resume only when $p$ signals that it is available;
the right side of figure~\ref{fig:overlap_compare} shows this case.

On both sides figure~\ref{fig:overlap_compare}
${SeqTime}_p = 5$ and ${SeqTime}_q = 4$.
On the left side, ${ConsTime}_{qA} = 2$,
and therefore ${Duration}_{qA} = 2$ and ${DurationRest}_{qA} = 2$,
Since ${ProdTime}_{pA} = 1$,
the first update of ${CurParTime}_q$ sets it to $2$,
and the second sets it to $4$, so ${ParTime}_q = 4$.
On the right side, ${ConsTime}_{qA} = 1$,
and therefore ${Duration}_{qA} = 1$ and ${DurationRest}_{qA} = 3$,
Since ${ProdTime}_{pA} = 4$,
the first update of ${CurParTime}_q$ sets it to ${max}(1, 4) = 4$,
and the second sets it to $4+3 = 7$, so ${ParTime}_q$ is 7.

\begin{table}
\begin{center}
\begin{tabular}{l|rr}
 & \multicolumn{1}{|c}{Cost}
 & \multicolumn{1}{|c}{Local use of \code{Acc1}} \\
\hline
\code{M}  &   1,625,050 & none \\
\code{F}  &           3 & ${Prod}_{Acc1}$ =         3 \\
\mapfoldl &   1,625,054 & ${Cons}_{Acc1}$ = 1,625,051 \\
% Note: The cost of the recursive call assumes that there is one
% recursive case and one base case remaining in the recursion.
\end{tabular}
\end{center}
\caption{Rounded profiling data for \mapfoldl}
\label{tab:prof_data_map_foldl}
\end{table}

% \begin{figure}[tb]
% \begin{verbatim}
% map_foldl_par(_, _, [], Acc, Acc).
% map_foldl_par(M, F, [X | Xs], Acc0, Acc) :-
%     (
%         M(X, Y),
%         F(Y, Acc0, Acc1)
%     ) &
%     map_foldl_par(M, Xs, Acc1, Acc).
% \end{verbatim}
% \caption{Parallel \mapfoldl}
% % the recursive call is less dependent
% % on the conjunction of the first two calls.
% \label{fig:map_foldl_par}
% \end{figure}

To see how the algorithm works on realistic data,
consider the \mapfoldl example in figure~\ref{fig:map_foldl}.
Table \ref{tab:prof_data_map_foldl} gives
the approximate costs of the calls in the recursive clause of \mapfoldl
when used in a Mandelbrot image generator.
Each call to $M$ draws a row,
while $F$ appends the new row
onto the list of the rows already drawn.
The table also shows when $F$ produces ${Acc1}$
and when the recursive call consumes ${Acc1}$.
The costs were collected from a real execution using Mercury's deep profiler
and then rounded to make mental arithmetic easier.

Figure~\ref{fig:map_foldl_par} shows the best parallelisation of
\mapfoldl.
When evaluating the speedup for this parallelisation,
${Cons}_{{F} {Acc1}} = 1,625,000 + 2 = 1,625,002$, and
${Cons}_{{map\_foldl} {Acc1}} = 1,625,000 + 2 + 1,620,000 = 3,245,002$.
\zoltan{Show the rest of the algorithm's execution when this question is answered.}
\paul{I would have fixed this but I do not know what these formulas mean,
Maybe I am confused by notation}

% \zoltan{Show real data, if we can,
% that shows that quicksort has very little overlap.}

The speedup computed by algorithm~\ref{alg:dep_par_conj_overlap_simple}
applies only when the recursive call
calls the original sequential version of the predicate.
When the recursive call calls the parallelised version,
the maximum speedup available (assuming an unlimited number of CPUs)
depends on the structure of the recursion.
The profiling data gives us \tr{$E$,}
the number of entry calls to the procedure from the higher clique,
and \tr{$R_i$,} the number of recursive calls at each recursive call site.
From these, and the structure of the procedure's code,
we can calculate the average depth of recursion in most cases.

For singly recursive predicates like \mapfoldl,
there is only one recursive call site,
and the depth of recursion is simply $R_1/E$.
For example, if $E = 2$ and $R_1 = 20$,
then the average call sequence to the procedure
has one entry call followed by the recursive calls.
It also means that the average call sequence
has ten calls that cause the procedure to execute its recursive clause,
the clause containing the conjunction being parallelised,
followed by one call that executes the base clause.
From this, we can deduce that if ${SeqSaving} = {SeqTime} - {ParTime}$
is the time saving we get from parallelising the top conjunction
if the recursive call calls the original sequential version of the procedure,
then making it call the parallelised version of the procedure
would yield a time saving of ${ParSaving} = {SeqSaving} * R_1/E$
if we have enough CPUs to execute all the $R_1/E$ iterations in parallel.
\peter{I think this needs more explanation.  It does not look right to me.
I would expect it to be ${ParSaving} = {SeqTime} - {ParTime} * R_1/E$.}
This assumes that we get the same savings at each iteration,
\peter{I find this sentence weakens the claim, rather than strengthing it.
Can we just add ``, assuming we get the same savings at each iteration'' at
the end of the previous sentence?}
but this is reasonable,
since what we are doing is essentially executing
the different iterations of a loop in parallel,
and we have no reason to believe that the savings
from executing iteration $k$ in parallel with iteration $k+1$
would vary systematically based on the value of $k$.

Some singly recursive predicates have more than one recursive clause,
each with one recursive call site.
Suppose there are $n$ call sites, with execution counts $R_1 \ldots R_n$.
The overall time saving from parallelising the conjunct
that contains the call site associated $R_i$
has to be multiplied by the fraction of recursive calls
that execute the conjunction being parallelised:
${ParSaving} = {SeqSaving} * R_i/E * R_i/\sum_{j=1}^n R_j$.

For doubly recursive predicates like \code{quicksort}, $R_1 = R_2$,
and just under half of their calls invoke the recursive clause,
so for them, ${ParSaving} = {SeqSaving} * R_1/(2 * E)$.
\zoltan{check the math}
\peter{I think it is a bit confusing to use $R_1$ and $R_2$ to name the
recursive calls to quicksort, when above they have named the sole recursive
call from different clauses.  Maybe use $R^i_j$ to indicate the $i^{th}$
recursive call from the $j^{th}$ clause.  Then you can use $R^1_i$ in the
previous paragraph, and $R_1^1$ and $R_1^2$ in this one.}

All these calculations show that the available parallelism
can be greater than the number of CPUs.
If the machine has e.g.\ four CPUs,
then we do not actually want to spawn off
hundreds of iterations for parallel execution,
since parallel execution actually has several forms of overhead:

\begin{description}
\item[SparkCost]
is the cost of creating a spark and adding it to the local spark stack.
In a parallel conjunction,
every conjunct that is not the last conjunct incurs this cost
to create the spark for the rest of the conjunction.
\item[SparkDelay]
is the estimated length of time between the creation of a spark
and the beginning of its execution on another engine.
Every parallel conjunct that is not the first incurs this delay
before it starts running.
\item[SignalCost]
is the cost of signalling a future.
\item[WaitCost]
is the cost of waiting on a future.
\item[ContextWakeupDelay]
is the estimated time that it takes for a context to resume execution
after being placed on the runnable queue,
assuming that the queue is empty and there is an idle engine.
\item[BarrierCost]
is the cost of executing the operation
that synchronises all the conjuncts at the barrier
at the end of the conjunction.
\end{description}

Because of these overheads, our system uses \emph{throttling}.
If a conjunction being parallelised contains a recursive call,
then the compiler will replace the original sequential conjunction
not with the parallel form of the conjunction,
but with an if-then-else.
The condition of this if-then-else
will test at runtime
whether spawning off a new job is a good idea or not.
If it is, we execute the parallelised conjunction,
if it is not, we execute the original sequential conjunction.
The condition is obviously a heuristic.
If the heuristic allows the list of runnable jobs to become empty,
then we will not have any work to give to a CPU
that finishes its task and becomes available.
On the other hand,
if the heuristic allows the list of runnable jobs to become too long,
then we incur the overheads of spawning off some jobs unnecessarily.
Currently, on machines with $N$ CPUs,
we prefer to have a total of $M$ running and runnable jobs where $M > N$,
so our heuristic stops spawning attempts
if and only if the queue already has $M$ entries.
Our current system by default sets $M$ to be $32$ for $N = 4$,
though users can easily override this.

The overheads of parallel execution can also affect conjunctions
that do not contain recursive calls:
a conjunction that looks worth parallelising if you ignore overheads
may look not worth parallelising if you take them into account.
This is why our system actually uses
algorithm~\ref{alg:dep_par_conj_overlap_complete},
a version of algorithm~\ref{alg:dep_par_conj_overlap_simple}
that accounts for overheads.

\begin{algorithm}
\begin{algorithmic}
\Procedure{find\_par\_time}{$Conjs, VarUseLists, SeqTimes$}
    \State $N \gets$ length($Conjs$)
    \State $ProdTimeMap \gets$ empty
    \State $FirstConjTime \gets 0$
    \State $TotalParTime \gets 0$
    \For{$i \gets 1$ to $N$}
        \State $CurSeqTime \gets 0$
        \State $CurParTime \gets (SparkCost + SparkDelay) \times (i-1)$
        \If{$i \neq N$}
            \State $CurParTime \gets CurParTime + SparkCost$
        \EndIf
        \State sort $VarUseList_i$ on $Time_{i, j}$
        \For{$(Var_{i, j}, Time_{i, j}) \in VarUseList_i$}
            \State $Duration_{i, j} \gets Time_{i, j} - CurSeqTime$
            \State $CurSeqTime \gets CurSeqTime + Duration_{i, j}$
            \If{$Conj_i$ produces $Var_{i, j}$}
                \State $CurParTime \gets
                    CurParTime + Duration_{i, j} + SignalCost$
                \State $ProdTimeMap$[$Var_{i, j}$]$ \gets CurParTime$
            \Else \Comment $Conj_i$ must consume $Var_{i, j}$
                \State $ParWantTime \gets CurParTime + Duration_{i, j}$
                \State $CurParTime \gets$
                    max($ParWantTime, ProdTimeMap$[$Var_{i, j}$]$) + WaitCost$
                \If{$ParWantTime < ProdTimeMap$[$Var_{i, j}$]}
                    \State $CurParTime \gets CurParTime + ContextWakeupDelay$
                \EndIf
            \EndIf
        \EndFor
        \State $DurationRest \gets SeqTime_i - CurSeqTime$
        \State $CurParTime \gets CurParTime + DurationRest + BarrierCost$
        \If{$i = 1$}
            \State $FirstConjTime = CurParTime$
        \EndIf
        \State $TotalParTime \gets$ max($TotalParTime, CurParTime$)
    \EndFor
    \If{$TotalParTime > FirstConjTime$}
        \State $TotalParTime \gets TotalParTime + ContextWakeupDelay$
    \EndIf
    \State \Return $TotalParTime$
\EndProcedure
\end{algorithmic}
\caption{Dependent parallel conjunction complete algorithm}
\label{alg:dep_par_conj_overlap_complete}
\end{algorithm}

Algorithm~\ref{alg:dep_par_conj_overlap_complete}
can also handle $n$-way conjunctions for $n>2$.
Since the Mercury mode system reorders conjunctions
to ensure that data flows only to the right,
in a two-conjunct conjunction,
the left conjunct can only produce
the variables it shares with the right conjunct
and the right conjunct can only consume
those variables.
However, in longer conjunctions,
the conjuncts in the middle
can both consume variables produced by conjuncts on their left
and produce variables consumed by conjuncts on their right.
This is why our algorithm associates with \code{Conj\_i}, the $i$th conjunct,
\code{ProdConsList\_i}:
the list of shared variables that \code{Conj\_i} either produces or consumes,
together with their times of production and first consumption respectively.
This is a generalisation of \code{ConsList\_q} in
algorithm~\ref{alg:dep_par_conj_overlap_simple}.
We also need to generalise \code{Prod\_pi},
because the time at which a non-first conjunct produces a variable
can and usually will be affected
by the overheads and/or synchronisation delays suffered by that conjunct.
This is why we use \code{ProdTimeMap},
which maps each shared variable to its time of production.

The main body of the algorithm consists of two nested loops.
The outer loop loops over all the conjuncts from left to right,
because the execution of a conjunct can be affected
by the conjuncts to its left
(through the time at which they produce the shared variables it consumes),
but not by the conjuncts to its right.
The first few lines of the outer loop body
(the first two assignments to \code{CurParTime})
compute for each conjunct
the time at which that conjunct can start execution.

The inner loop loops over all the components of the current conjunct,
such as \code{pA} and \code{pR}
from figure~\ref{fig:overlap_compare}.
Just as our previous algorithm did,
this loop updates the simulated current time
in both the original sequential execution of the conjunct (\code{CurSeqTime})
and in its modified parallelised execution (\code{CurParTime}).
For time components that end in the consumption of a variable,
we do what we did before,
but also reflect the cost of the wait operation needed for the consumption.
For time components that end in the production of a variable,
we record the time at which
that variable would be available in the parallel execution;
this will be when the producer finishes executing the signal operation on it.

We use \code{TotalParTime} to keep track of the ending time
of the parallel conjunct that ends last.
We also remember, in \code{FirstConjTime},
the time at which the first conjunct finishes.
The reason we do this is because
our runtime system requires that
when the parallel conjunction finishes,
execution must continue in the context
that entered the parallel conjunction in the first place.
In our implementation, this context will execute the first conjunct.
If the last conjunct to finish is the first conjunct,
it can continue on without delay;
if the last conjunct to finish is some other conjunct,
then we need to free its context,
and switch to executing the original context,
which became idle when the first conjunct finished.
The last two lines reflect this cost.

% Each conjunct's execution depends on
% when the variables it consumes are produced by other conjuncts.
% These must be conjuncts to its left,
% since the compiler reorders conjunctions as needed
% to ensure that data flows only from left to right.
% Therefore, we can calculate the execution time of
% $G_1 \& \ldots \& G_n$
% by computing the execution time
% first of $G_1$,
% then of $G_1 \& G_2$,
% then of $(G_1 \& G_2) \& G_3$,
% % then of $((G_1 \& G_2) \& G_3) \& G_4$,
% and so on.

% \zoltan{Discuss how the synchronisation at the end of the algorithm is
% similar to the synchronisation while waiting for another variable.}
}

\section{Choosing how to parallelise a conjunction}
\label{sec:overlap_howto}

A conjunction with $n > 2$ conjuncts
can be converted into several different parallel conjunctions.
Converting all the commas into ampersands
(e.g.\ \code{c1, c2, c3} into \code{c1 \& c2 \& c3})
yields the most parallelism.
Unfortunately, this will often be \emph{too} much parallelism,
because in practice many conjuncts are unifications
and arithmetic operations whose execution takes very few instructions.
Executing such conjuncts in their own threads
costs far more in overheads than they save by running in parallel.
Therefore in most cases,
we want to create parallel conjunctions with $k < n$ conjuncts,
each consisting of a contiguous sequence
of one or more of the original sequential conjuncts,
effectively partitioning the original conjuncts into groups.

\begin{figure}
\begin{center}
\begin{verbatim}
global NumEvals := 0
find_best_partition(InitPartition, InitTime, LaterConjs)
    returns <FinalTime, FinalPartitionSet>:
  switch on LaterConjs:
  when LaterConjs = []:
    return <InitTime, {InitPartition}>
  when LaterConjs = [Head | Tail]:
    Extend := all_but_last(InitPartition) ++ [last(InitPartition) ++ [Head]]
    AddNew := InitPartition ++ [Head]
    ExtendTime := find_par_time(Extend)
    AddNewTime := find_par_time(AddNew)
    NumEvals := NumEvals + 2
    if ExtendTime < AddNewTime:
      BestExtendSoln := find_best_partition(Extend, ExtendTime, Tail)
      let BestExtendSoln be <BextExTime, BestExPartSet>
      if NumEvals < PreferLinearEvals:
        BestAddNewSoln := find_best_partition(AddNew, AddNewTime, Tail)
        let BestAddNewSoln be <BestANTime, BestANPartSet>
        if BestExTime < BestANTime:
          return BestExtendSoln
        else if BestExTime = BestANTime:
          return <BextExTime, BestExPartSet union BestANPartSet>
        else:
          return BestAddNewSoln
      else:
        return BestExtendSoln
    else:
      <symmetric with the then case>
\end{verbatim}
\end{center}
\caption{Search for the best parallelisation}
\label{fig:best_par_search}
%\vspace{-2\baselineskip}
\end{figure}

For any conjunction to be worth parallelising,
it should contain two or more expensive goals.
Our main algorithm (figure \ref{fig:best_par_search} works on the list
of conjuncts
from the first expensive goal to the last.
This will be the middle of original conjunction,
with (possibly empty) lists of cheap goals before it and after it.
Our initial search assumes that
the set of conjuncts in the parallel conjunction we want to create
is exactly the set of conjuncts in the middle.
A post-processing step then removes that assumption.

% into \code{(c1 \& c2), c3},
% into \code{c1, (c2 \& c3)},
% into \code{c1 \& (c2, c3)},
% into \code{(c1, c2) \& c3}, or
% into \code{c1 \& c2 \& c3}.

% In the usual case where $n >> 2$,
% there will be a huge number ways to do this.
% Our parallelisation algorithm,
% algorithm~\ref{alg:best_par_search},
% therefore tries to find the partition
% that yields the lowest overall execution time.

% A middle sequence with $n > 2$ conjuncts
% can be converted into several different parallel conjunctions;
% for example, \code{c1, c2, c3} can be converted
% into \code{(c1 \& c2), c3},
% into \code{c1, (c2 \& c3)},
% into \code{c1 \& (c2, c3)},
% into \code{(c1, c2) \& c3}, or
% into \code{c1 \& c2 \& c3}.
% The first two do not make sense if \code{c1} and {c3} are expensive goals,
% so we consider only conjunctions in which all conjuncts in the middle sequence
% are part of one parallel conjunct or another.
% The last of these gives the finest grain parallelism.
% Unfortunately, this will often be \emph{too} fine-grained,
% because in practice many conjuncts are unifications
% or builtin operations such as arithmetic
% whose execution takes very few instructions.
% Executing such conjuncts in their own threads
% can cost far more in overheads than they save by running in parallel.
% Therefore in most cases,
% we want to create parallel conjunctions with $k < n$ conjuncts,
% each consisting of a contiguous sequence
% of one or more of the original sequential conjuncts,
% effectively partitioning the original conjuncts into groups.
% % In the usual case where $n >> 2$,
% % there will be a huge number ways to do this.
% % Our parallelisation algorithm,
% % algorithm~\ref{alg:best_par_search},
% % therefore tries to find the partition
% % that yields the lowest overall execution time.

% Therefore, it is best to break sequential conjunctions into a number of
% smaller sequential conjunctions that are conjuncts of a larger
% parallel conjunction, however there are a multiple possible ways to do
% this.

If the middle sequence has $n$ conjuncts,
then there are $n-1$ AND operations between them,
each of which can be either sequential or parallel.
There are then $2^{n-1}$ combinations,
all but one of which are parallelisations.
That is a large space to search for the \emph{best} parallelisation,
and it would be larger still if we allowed code reordering,
that is, parallel conjuncts consisting of
a \emph{non}contiguous sequence of the original conjuncts.
We explore this space with a search algorithm,
\code{find\-\_\-best\-\_\-par\-ti\-tion}, which
we invoke with the empty list as \code{InitPartition},
zero as \code{InitTime}, and the list of middle conjuncts as \code{LaterConj}.
\code{InitPartition} expresses a partition of an initial sequence
of the middle goals into parallel conjuncts
whose estimated execution time is \code{InitTime},
and considers whether it is better to add the next middle goal
to the last existing parallel conjunct (\code{Extend}),
or to put it into a new parallel conjunct (\code{AddNew}).
It explores extensions of the better of the resulting partitions first.
If the search is still under the limit on the number of evaluations,
it explores the worse partition as well,
which is an exponential search.
When it hits the limit,
it switches to a linear search;
we explore the more promising partition first
to make this search more effective.
(This limit ensures that the algorithm runs in reasonable time.)
The algorithm returns a set of equal best parallelisations so far,
``best'' being measured by
\iclp{a version of the algorithm in figure~\ref{fig:dep_par_conj_overlap_middle} that
computes the estimated parallel execution time \emph{including} overheads.}
\tr{algorithm \ref{alg:dep_par_conj_overlap_complete},
that is, the estimated parallel execution time including overheads.}

There are some simple ways to improve this algorithm.
%\vspace{-2mm}
\begin{itemize}
\item
Most invocations of \verb|find_par_time| specify a partition
that is an extension of a partition processed in the recent past.
In such cases, \verb|find_part_time| should do its task
incrementally, not from scratch.
\item
If the expected execution time
for the candidate partition currently being considered
is already greater than the fastest existing complete partition,
we can stop exploring that branch;
it cannot lead to a better solution.
\tr{
(This is the idea of branch-and-bound algorithms.)
}
\item
Sometimes consecutive conjuncts do things that are
obviously a bad idea to do in parallel, such as building a ground term.
The algorithm should treat these as a single conjunct.
% XXX we could make the third item tr only if we need space
\tr{
\item
graph of dependencies
\item
take total CPU utilisation into account,
at least by using it to break ties on overall CPU time
}
\end{itemize}
%\vspace{-2mm}

\noindent
At the completion of the search,
we select one of the equal best parallelisations,
and post-process it to adjust both edges.
Suppose the best parallel form of the middle goals is $P_1~\&~\ldots~\&~P_p$,
where each $P_i$ is a sequential conjunction.
We compare the execution time of $P_1~\&~\ldots~\&~P_p$
with that of $P_1,~(P_2~\&~\ldots~\&~P_p)$.
If the former is slower,
which can happen if $P_1$ produces its outputs at its very end
and the other $P_i$ consume those outputs at their start,
then we conceptually move $P_1$ out of the parallel conjunction
(from the ``middle'' part of the conjunction to the ``before'' part).
We keep doing this for $P_2$, $P_3$ etc until either
we find a goal worth keeping in the parallel conjunction,
or we run out of conjuncts.
We also do the same thing at the other end of the middle part.
This process can shrink the middle part.

In cases where we do not shrink an edge, we can consider expanding that edge.
Normally, we want to keep cheap goals out of parallel conjunctions,
since more conjuncts tends to mean
more shared variables and thus more synchronisation overhead,
but sometimes this consideration is overruled by others.
Suppose the goals before the conjuncts in $P_1~\&~\ldots~\&~P_p$
in the original conjunction were $B_1,~\ldots,~B_b$
and the goals after it $A_1,~\ldots,~A_a$,
and consider $A_1$ after $P_p$.
If $P_p$ finishes before the other parallel conjuncts,
then executing $A_1$ just after $P_p$ in $P_p$'s context
may be effectively free:
the last context could still arrive at the barrier at the same time,
but this way, $A_1$ would have been done by then.
Now consider $B_b$ before $P_1$.
If $P_1$ finishes before the other parallel conjuncts,
\emph{and} if none of the other conjuncts wait for variables produced by $P_1$,
then executing $B_b$ in the same context as $P_1$ can be similarly free.

We loop from $i=b$ down towards $i=1$, and check whether
including $B_i,~\ldots,~B_b$ at the start of $P_1$ is improvement.
If not, we stop; if it is, we keep going.
We do the same from the other end.
% If we end up with moving
% The second search loops from $j=1$ up towards $j=a$
% and checks whether including $A_1, \ldots, A_j$ at the end of $P_p$
% is improvement.
% Each loop stops when the answer becomes ``no'',
The stopping points of the loops of the contraction and expansion phases
dictate our preferred parallel form of the conjunction, which
(if we shrunk the middle at the left edge and expanded it at the right)
will look something like
$B_1,$ $\ldots,$ $B_{b},$ $P_1,$ $\ldots~P_k,$
$(P_{k+1}$ $\&$ $\ldots$ $\&$ $P_{p-1}$ $\&$ $(P_p,$ $A_1,$ $\ldots,$ $A_j)),
A_{j+1},$ $\ldots,$ $A_a$.
% $B_1, \ldots, B_{i-1},
% ((B_i, \ldots, B_b, P_1) \& P_2, \ldots \& P_{p-1} \& (P_p, A_1, \ldots, A_j)),
% A_{j+1}, \ldots, A_a$.
If this preferred parallelisation is better than
the original sequential version of the conjunction by at least 1% (a configurable threshold),
then we include a recommendation for its conversion to this form
in the feedback file we create for the compiler.

% These two loops are specifically designed
% to allow the inclusion of cheap goals in the parallel conjunction.
% Note that this algorithm always tries to arrange
% \emph{all} the conjuncts in the conjunction,
% not just the conjuncts from the first costly goal to the last.
% Normally, we want to keep cheap goals out of parallel conjunctions,
% since more conjuncts usually means more shared variables,
% which means more synchronisation overhead.
% The reason why we expanding the scope of the parallel conjunction
% is that sometimes this consideration is overruled by others.
% Consider $A_1$ after $P_p$.
% If $P_p$ finishes before the other parallel conjuncts,
% then executing $A_1$ just after it in $P_p$'s context may be effectively free:
% the last context could still arrive at the barrier at the same time,
% but this way, $A_1$ would have completed by then.
% Now consider $B_b$ before $P_1$ where $P_1$ is still in a parallel conjunct.
% If $P_1$ finishes before the other parallel conjuncts,
% \emph{and} if none of the other conjuncts
% wait for variables produced by $P_1$,
% then executing $B_b$ in the same context as $P_1$ can be similarly free.

% \begin{itemize}
% \item
% Currently no tie breaking is done and we have not explored
% using other formulas for the search's objective function.
% % \item
% % Also, the current implementation does not make an estimate of the
% % minimum cost of the work that could be scheduled after the current point.
% % This affects the amount of pruning that the branch and bound code
% % is able to achieve.
% \end{itemize}

% When explaining the algorithm, tell readers to first assume that
% GoalsBefore and GoalsAfter are the empty list,
% i.e. MaxBefore = 0 and MinAfter = N+1.
% Only after explaining the algorithm in that case

% Consider changing the loop structure so that instead of
% loop on Before;
%     loop on After,
%         loop on Arrangement,
% it is
% loop on Arrangement,
%     loop on Before (assuming a given After) find the best and commit to it,
%     loop on After, find the best and commit to it.

\section{Pragmatic issues}
\label{sec:overlap_pragmatic}

% \subsection{The effects of module boundaries}
% \label{sec:pragmamoduleboundary}

% pushing waits and signals into calles stops at module boundaries

% \subsection{Cliques vs procedures}
% \label{sec:pragmacliqueproc}

\emph{Dynamic context}
The algorithms in sections \ref{sec:overlap_overlap_alg}
and \ref{sec:overlap_howto}
work on profiling data that shows the behaviour of a procedure
in the context given by a particular chain of ancestors.
Many procedures are of course called from multiple ancestor contexts.
What happens when our analysis of the behaviour of the same procedure
yields different results for different ancestor contexts?

At the moment, for any procedure
that our analysis indicates is worth parallelising in any context,
we pick one particular parallelisation (usually there is only one anyway),
and transform the procedure accordingly.
This gets the benefit of parallelisation when it is worthwhile,
but incurs its costs even in contexts when it is not.
In the future, we plan to fix this using multi-version specialisation.
For every procedure with different parallelisation recommendations,
we intend to create a specialised version for each recommendation,
leaving the original sequential version.
This will of course require the creation of specialised versions
of its parent, grandparent etc procedures,
until we get to an ancestor procedure
which occurs in the common prefix of all the conflicting ancestor contexts.

% \subsection{Searching for parallelism opportunities}
% \label{sec:pragmabestfirst}
%
% Our candidates list actually contains
% both cliques and conjunctions within cliques.
%
% The candidates list should contain cliques, since
% \begin{itemize}
% \item
% the entry points of some child cliques are not in conjunctions
% (e.g.\ they can be switch arms), and
% \item
% we want to delay breaking a clique down into its constituent conjunctions,
% since this way if our traversal stops before getting to a clique,
% then we never have to break it down.
% \end{itemize}
%
% The candidates list should also contain conjunctions,
% since a clique can contain both cheap and expensive conjunctions,
% and we do not want to evaluate the cheap ones
% until we have processed all the more expensive conjunctions
% not just in this clique but in all other cliques;
% again, we expect that this way,
% our traversal will stop before it gets to the cheapest conjunctions.

% \subsection{Parallelising children vs ancestors}
% \label{sec:pragmachildancestor}

\emph{Parallelising children vs ancestors}
What happens when we decide that a conjunction that should be parallelised
has an ancestor that we decided should also be parallelised?
We can
(1) parallelise only the ancestor,
(2) parallelise only this conjunction, or
(3) parallelise both

% The first alternative (parallelise neither) has already been rejected twice,
% since we concluded that (2) was better (1)
% when we decided to parallelise the ancestor,
% and we concluded that (3) was better (1)
% when we decided to parallelise this conjunction.

\zoltan{Does the implementation actually do this now?}
We choose among the other three alternatives
by evaluating the speedup you get from each of them, and just pick the best.
This reevaluation must take into account
the fact that for each invocation of the ancestor conjunction,
we will invoke the current conjunction many times,
and that therefore we will incur both the overheads and the benefits
of parallelising the current conjunction many times.
The profile will give the actual number.

% \subsection{Disagreement among children}
% \label{sec:pragmachildchild}

% what if for some ``current'' clique,
% you want to parallelise the ancestor,
% but for some other current clique,
% you do not want to parallelise the same ancestor?

\emph{Parallelising branched goals}
Many programs have code that looks like this:
\begin{verbatim}
( if ... then
    ... expensive call 1 ...
else
    ... cheap goal ...
),
expensive call 2
\end{verbatim}
If the condition of the if-then-else succeeds only rarely,
then the average cost of the if-then-else
may be below the threshold of what we consider to be an expensive goal.
We therefore would not even consider
parallelising the top-level conjunction,
rightly considering that its overheads would probably outweigh its benefits.

What we want to do in such cases
is execute just the two expensive calls in parallel,
which would be equivalent to parallelising the conjunction
in the then part of this transformed goal:
\begin{verbatim}
( if ... then
    ... expensive call 1 ...
    expensive call 2
else
    ... cheap goal ...
    expensive call 2
)
\end{verbatim}
We intend to change our feedback tool to detect such situations,
and if found, to recommend
some equivalence-preserving transformations for the compiler to apply
before parallelising some of the resulting conjunctions.

% \subsection{Garbage collector issues}
% \label{sec:pragmagc}

\emph{Garbage collector issues}
The Mercury implementation uses the Boehm-Demers-Weiser
conservative collector for C \zoltan{add cite} to manage memory.
This system has worse overheads
for parallel programs than for sequential programs.
First, even though this collector uses
a separate memory pool for each mutator thread
(and hence, in our system, for each Mercury engine),
you still need synchronisation to access the global pool
when a local pool runs out.
Second, this collector
does not support incremental collection for parallel programs,
and a full collection stops all threads,
and thrashes the caches of their CPUs.
We therefore ran our benchmarks with the collector tuned
to use large local pool sizes
and to grow the size of the global pool more quickly than usual.
These settings significantly improved
the performance of the sequential programs as well.

% GC In our case, the worry is that
% GC the Boehm collector may scale significantly worse than
% GC the Mercury code that we choose to parallelise.

% GC From the perspective of a GC implementor,
% GC a program's runtime has two components:
% GC the execution time of the main part of the program (the \emph{mutator}),
% GC and the execution time of the collector itself.
% GC When the mutator is a Mercury program,
% GC there is a fixed (and usually small) limit
% GC on the number of instructions it can execute
% GC between increments of the call sequence count
% GC (though the limit is program-dependent).
% GC There is no such limit on the collector.
% GC This is can be a problem.
% GC Since a construction unification does not involve a call,
% GC our profiler considers its CSC cost to be zero.
% GC \peter{Haven't defined ``CSC \emph{cost}''.}
% GC Yet if the memory allocation required by a construction
% GC triggers a collection,
% GC then this nominally zero-cost action can actually take as much time
% GC as many calls in the mutator.
% GC
% GC The normal way to view the time taken by gc
% GC is to simply distribute it among the allocations,
% GC so that one CSC represents the average time taken
% GC by the mutator between two calls
% GC plus the average amortized cost of the collections
% GC triggered by the unifications between those calls.
% GC For a sequential program, this view works very well.
% GC For a parallel program, it works less well,
% GC because the performance of the mutator and the collector may scale differently.
% GC In our case, the worry is that
% GC the Boehm collector may scale significantly worse than
% GC the Mercury code that we choose to parallelise.
% GC
% GC For Mercury code,
% GC the limits on speedups from parallelism fall into two categories:
% GC those that our analysis takes into account, and those it does not.
% GC The former include the cost of spawning new contexts,
% GC the cost of the barrier synchronisation at the end of the conjunction,
% GC the cost of creating the synchronisation terms,
% GC the cost of the wait and signal operations on those terms,
% GC and idle time of the consumer while waiting on the producer.
% GC The latter include interference

% lock at allocation vs stop the world at collection

% memory
% heat
%
% is to simply consider the amortized cost
% In sequential programs,
% one can consider that the cost of the
%
% In sequential programs,
% it is not really feasible to separate
% the time cost of the collector from the cost of the mutator.
% On
%
% When parallelising the program, there is unfortunately
% a natural way to separate
%
% and CSCs can be considered to measure both parts of the program.
%
% CSCs not a true representation of time
%
% take heap allocation intensity (allocations per CSC) into account
% in the algorithms above:
% consider two high-intensity goals being executed in parallel
% to be another source of overhead.
%
% convert allocs to CSCs, add them to both sequential and parallel times
%
% involve the alloc ratios somehow?
%
% There is also the consideration that with Boehm gc,
% a collection stops the world,
% and the overheads of this stopping scale with the number of CPUs being used.
% The overheads of stopping include
% not just the direct costs of the interruption,
% but also indirect costs,
% such as having to refill the cache after the collector trashes it.

\section{Performance results}
\label{sec:overlap_perf}

% \begin{table*}
% \begin{center}
% \begin{tabular}{l|rrrrrrrrrr}
%  ~ & \multicolumn{1}{|c|}{Seq RT} &
%   \multicolumn{1}{|c|}{Par RT} &
%   \multicolumn{2}{|c|}{No Deps} &
%   \multicolumn{2}{|c|}{Na\"ive} &
%   \multicolumn{2}{|c|}{Num Vars} &
%   \multicolumn{2}{|c}{Overlap} \\
% \multicolumn{1}{c|}{Program} & \multicolumn{1}{|c|}{345Time} &
%   \multicolumn{1}{|c|}{Time} &
%   \multicolumn{1}{|c|}{Conjs} & \multicolumn{1}{|c|}{Time} &
%   \multicolumn{1}{|c|}{Conjs} & \multicolumn{1}{|c|}{Time} &
%   \multicolumn{1}{|c|}{Conjs} & \multicolumn{1}{|c|}{Time} &
%   \multicolumn{1}{|c|}{Conjs} & \multicolumn{1}{|c}{Time} \\ \hline
% quicksort acc &   &   & 0 &   & 1 &   &   &   & 0 &   \\
% quicksort app &   &   & 1 &   & 1 &   &   &   & 1 &   \\
% fibs & 1 & a & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
% icfp2000 & 1 & a & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
% mandelbrot & 1 & a & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
% mmc & 1 & a & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9
% \end{tabular}
% \end{center}
% \caption{Results}
% \label{tab:results_temp}
% \end{table*}

% Report analysis times as well as sequential and parallel execution times
% and CPU usage (as integral if possible, as well as peack CPU usage).

We tested our system on three benchmark programs:
matrix multiplication, a mandelbrot image generator and a raytracer.
Matrixmult has abundant independent AND-parallelism.
Mandelbrot uses the actual \code{map\_foldl} predicate
from figure~\ref{fig:map_foldl}
to iterate over rows of pixels.
Raytracer does not use \code{map\_foldl},
but does use a similar code structure to perform a similar task.
This is not an accident:
\emph{many} predicates use this kind of code structure,
partly because programmers in declarative languages
often use accumulators to make their loops tail recursive.
% All three programs do lots of floating point arithmetic,
% and the mandelbrot program does a lot of integer arithmetic as well.
% The Mercury backend we are using always boxes floating point numbers,
% so each floating point operation requires the creation of a new cell on the heap.
% This makes matrixmult and raytracer very memory allocation intensive.
% Since the garbage collector accounts for a large fraction of their runtimes,
% Amdahl's law dictates the maximum speedup we can get by speeding up their mutators
% will be correspondingly limited.

We ran all three programs
with one set of input parameters to collect profiling data,
and with a \emph{different} set of input parameters to produce
the timing results in the following table.
All tests were run on
% taura
a Dell Optiplex 755 PC with a 2.4~GHz Intel Core 2 Quad Q6600 CPU
running Linux 2.6.31.
Each test was run ten times;
we discarded the highest and lowest times, and averaged the rest.

% \begin{table*}[h]
% \begin{center}
% \begin{tabular}{l||r|r|r|r|r|r}
% Program     & Seq   & No autopar   & 1 CPU        & 2 CPUs      & 3 CPUs      &
% 4 CPUs \\
% \hline
% mandelbrot  & 33.4  &  35.3 (0.95) &  35.4 (0.94) & 18.0 (1.85) & 12.2 (2.74) &
 % 9.4 (3.55) \\
% raytracer   & 12.33 & 14.01 (0.88) & 14.77 (0.83) & 9.40 (1.31) & 7.59 (1.62) &
% 6.70 (1.84) \\
% \end{tabular}
% \end{center}
% % \caption{Results}
% % \label{tab:results_temp}
% \end{table*}

% \vspace{-2mm}
% \begin{table*}[h]
% \begin{center}
% \begin{tabular}{|l|l||r|r|r|r|r|}
% %\hhline{|-|-||-|-|-|-|-|}
% \hline
% \multicolumn{1}{|c|}{\textbf{Program}} &
% \multicolumn{1}{ c||}{\textbf{Par}}    &
% \multicolumn{1}{ c|}{\textbf{1 CPU}}   &
% \multicolumn{1}{ c|}{\textbf{2 CPUs}}  &
% \multicolumn{1}{ c|}{\textbf{3 CPUs}}  &
% \multicolumn{1}{ c|}{\textbf{4 CPUs}}  \\
% %\hhline{|-|-||-|-|-|-|-|}
% \hline
% matrixmult & indep & 14.60 (0.75) &  7.55 (1.46) &  6.07 (1.81) &  5.21 (2.11) \\
% seq 11.00  & naive & 14.61 (0.75) &  7.53 (1.46) &  6.75 (1.63) &  5.17 (2.12) \\
% par 14.60  & overlap  & 14.59 (0.75) &  7.57 (1.45) &  5.26 (2.09) &  5.37 (2.05) \\
% %\hhline{|-|-||-|-|-|-|-|}
% \hline
% mandelbrot & indep & 35.27 (0.95) & 35.31 (0.95) & 35.15 (0.95) & 35.31 (0.95) \\
% seq 33.47  & naive & 35.33 (0.95) & 17.87 (1.87) & 12.07 (2.77) &  9.17 (3.65) \\
% par 35.27  & overlap  & 35.16 (0.95) & 17.91 (1.87) & 12.02 (2.78) &  9.15 (3.65) \\
% %\hhline{|-|-||-|-|-|-|-|}
% \hline
% raytracer  & indep & 11.33 (0.87) & 11.37 (0.87) & 11.36 (0.87) & 11.36 (0.87) \\
% seq  9.85  & naive & 11.20 (0.88) &  7.48 (1.32) &  5.91 (1.66) &  5.39 (1.83) \\
% par 11.29  & overlap  & 11.28 (0.87) &  7.56 (1.30) &  5.94 (1.66) &  5.38 (1.83) \\
% %\hhline{|-|-||-|-|-|-|-|}
% \hline
% \end{tabular}
% \end{center}
% % \caption{Results}
% % \label{tab:results_temp}
% \end{table*}
% \vspace{-2mm}

% \vspace{-2mm}
\begin{table}[tb]
\begin{center}
\begin{tabular}{llrrrrr}
\hline \hline
\multicolumn{1}{c}{\textbf{Program}} &
\multicolumn{1}{c}{\textbf{Par}}    &
\multicolumn{1}{c}{\textbf{1 CPU}}   &
\multicolumn{1}{c}{\textbf{2 CPUs}}  &
\multicolumn{1}{c}{\textbf{3 CPUs}}  &
\multicolumn{1}{c}{\textbf{4 CPUs}}  \\
\hline
%\hhline{|-|-||-|-|-|-|-|}
% \hline
% matrixmult & indep & 14.7 (0.76) &  7.6 (1.47) &  5.2 (2.15) &  5.2 (2.15) \\
% seq 11.2   & naive & 14.7 (0.76) &  8.0 (1.40) &  5.7 (1.96) &  4.7 (2.38) \\
% par 14.6   & overlap  & 14.7 (0.76) &  7.6 (1.47) &  6.7 (1.67) &  5.2 (2.15) \\
matrixmult & indep    & 14.6 (0.75) &  7.5 (1.47) &  7.0 (1.66) &  5.2 (2.12) \\
seq 11.0   & naive    & 14.6 (0.75) &  7.6 (1.45) &  5.2 (2.12) &  5.2 (2.12) \\
par 14.6   & overlap  & 14.6 (0.75) &  7.5 (1.47) &  6.2 (1.83) &  5.2 (2.12) \\
%\hhline{|-|-||-|-|-|-|-|}
\hline
mandelbrot & indep    & 35.2 (0.95) & 35.1 (0.95) & 35.2 (0.95) & 35.3 (0.95) \\
seq 33.4   & naive    & 35.4 (0.94) & 18.0 (1.86) & 12.1 (2.76) &  9.1 (3.67) \\
par 35.2   & overlap  & 35.6 (0.94) & 17.9 (1.87) & 12.1 (2.76) &  9.1 (3.67) \\
%\hhline{|-|-||-|-|-|-|-|}
\hline
raytracer  & indep    & 26.2 (0.87) & 26.3 (0.86) & 26.1 (0.87) & 26.2 (0.87) \\
seq 22.7   & naive    & 25.3 (0.90) & 16.0 (1.42) & 11.2 (2.03) &  9.4 (2.42) \\
par 26.5   & overlap  & 25.1 (0.90) & 16.0 (1.42) & 11.2 (2.03) &  9.4 (2.42) \\
%\hhline{|-|-||-|-|-|-|-|}
\hline \hline
\end{tabular}
\end{center}
%\vspace{-2\baselineskip}
\end{table}

Each group of three rows reports the results for one benchmark.
The first column shows the benchmark name,
the runtime of the program when compiled for sequential execution, and
its runtime when compiled for parallel execution
but without enabling auto-parallelisation.
This shows the overhead of support for parallel execution
when it does not buy any benefits.
We auto-parallelised each program three different ways:
executing expensive goals in parallel
only when they are independent (``indep'');
even if they are dependent, regardless of overlap (``naive'');  and
even if they are dependent, but only if they have good overlap (``overlap'').
The last four columns give the runtime in seconds
of each of these versions of the program
on 1, 2, 3 and 4 CPUs,
with speedups compared to the sequential version.

The parallel version of the Mercury system
needs to use a real machine register
to point to thread-specific data,
such as each engine's abstract machine registers.
On x86s, this leaves only one real register for the Mercury abstract machine,
so compiling for parallelism but not using it
yields a slowdown ranging from 5\% on mandelbrot to 25\% on matrixmult.
(We observe such slowdowns for other programs as well.)
On one CPU, autoparallelisation gets only this slowdown,
plus the (small) additional overheads of all the parallel conjunctions
that cannot get any parallelism.
% However, when we move to 2, 3 or 4 CPUs,
% some of the autoparallelised programs do get speedups.

The parallelism in the main predicate of matrixmult is independent,
Overlap parallelises the program the same way as indep,
so it gets the same speedup.
The numbers look different for 3 CPUs,
but all the runs for both versions actually took either 5.2 or 7.5 seconds,
depending (we think) on which way
the OS arranged the engines across the two CPU die of the Q6600;
the indep version just happened to get the 7.5s arrangement fewer times.
For naive, all the runs just happened to take 5.2 seconds,
even though naive creates a worse parallelisation than either indep or overlap:
during the expansion phase we described in section~\ref{sec:overlap_howto},
it includes an extra goal in the first of the parallel conjuncts;
this makes the conjunction dependent, which adds some overhead.
Naive also executes the code that does the matrix multiplication
in parallel with the goals that create its inputs,
which also adds overhead without speedup.
These overheads are too small to affect the results.

In mandelbrot and raytracer, all the parallelism is dependent,
which is why indep gets no speedup for them.
For mandelbrot, naive and overlap get speedups
that are as good as one can reasonably expect:
$35.2/9.1 = 3.87$ on four CPUs over the one CPU case.
% (Perfect speedups of 4.0 on 4 CPUs are not attainable in practice
% due to bottlenecks such as CPU-memory buses and stop-the-world garbage
% collection.)
For matrixmult and raytracer, the speedups they get,
2.12 and 2.42 on four CPUs,
also turn out to be pretty good when one takes a closer look.

For matrixmult, the bottleneck is almost certainly CPU-memory bandwidth.
Each step in this program does only one multiply and one add (both integer)
before creating a new cell on the heap and filling it in.
On current CPUs, the arithmetic takes much less time than the memory writes,
and since the new cells are never accessed again, caches do not help,
which makes it easy to saturate the memory bus, even when using only three CPUs.

The raytracer is very memory-allocation-intensive,
because it does lots of FP arithmetic,
and the Mercury backend we are using always boxes floating point numbers,
so each floating point operation requires
the creation of a new cell on the heap.
Because of this, memory bandwidth may also be an issue for it,
but its bigger problem is GC;
while GC takes only about 5\% of the runtime when run on one CPU,
it takes almost 40\% of the runtime when run on four CPUs,
even though we used four marker threads.
(For fairness, we used four marker threads
regardless of how many CPUs the Mercury code used.)
Given this fact, the best speedup we can hope for is
$(4 \times 0.6 + 0.4)/(0.6 + 0.4) = 2.8$,
and we do come pretty close to that.

GC becomes more expensive with more CPUs
not only because of increased contention,
but also because the GC has more work to do:
with more contexts being spawned, there are more stacks for it to scan.
We have tested versions of the raytracer in which
each spawned-off goal computed the pixels for several rows, not just one,
and these versions yield speedups of about 3.3 on four CPUs.
These versions spawn many fewer contexts, thus putting much less load
on the GC.
This shows that
program transformations that cause more work to be done in each context
are likely to be a promising area for future work.
% We thus expect that applying throttling
% (as described in section~\ref{sec:overlap})
% should significantly improve these results.

Most small programs like these benchmarks
have only one loop that dominates their runtime.
In all three of these benchmarks, and in many others,
the naive and overlap methods will parallelise the same loops,
and usually the same way;
they tend to differ only in how they parallelise code
that executes much less often (typically only once)
whose effect is lost in the noise.
The raw timings show a great deal of variability:
we have seen two consecutive runs of the same program on the same data
differ in their runtime by as much as 15\%.
% (One possible cause of this is differences
% in whether the OS puts frequently-communicating engines
% on cores on the same die, or cores on two different dies.)
% As the table shows,
Some of this variability remains even after filtering and averaging.
% However, the raw times showed significant variability,
% and this process does not entirely eliminate that variability.

To see the difference between naive and overlap,
we need to look at larger programs.
Our standard large test program is the Mercury compiler, which contains
53 conjunctions with two or more expensive goals.
Of these, 52 are dependent,
and only 31 have an overlap
that leads to a predicted local speedup of more than 1\%,
our default threshold.
Our algorithms can thus prevent
the unproductive parallelisation of $53-31=22$ of these conjunctions.
Unfortunately, programs that are large and complex enough
to show a performance effect from this saving
also tend to have large components
that cannot be profitably parallelised with existing techniques,
which means that (due to Amdahl's law)
our autoparallelisation system cannot yield overall speedups for them yet.

On the bright side,
our feedback tool generates feedback files
in less than a second from the profiles of small programs like these benchmarks,
and in only a minute or two even from much larger profiles.
The extra time taken by the Mercury compiler
when it follows the recommendations in feedback files
is so small that it is not noticeable.

% Currently, the Mercury runtime system
% often continues execution, on completion of a parallel conjunction,
% on a CPU different from the one being used before that parallel conjunction.
% When our system finds a smattering of parallel conjunctions
% through a mostly sequential program,
% these switches from a CPU with a warm cache to a CPU with a cold cache
% severely degrade the program's performance.
% Right now, for most programs,
% this effect yields a slowdown significantly bigger
% than the speedups yielded by automatic parallelisation.
% Once this defect is fixed, we hope to report significantly better results
% for more and bigger programs.

% \footnote{
% For referees
% who read this paper together
% with the submission by Wang and Somogyi,
% the version of the raytracer used in that paper
% had manual granularity control;
% the version we are using in this paper is the original version of the program,
% which was not written for parallelism and has no manual granularity control.}

% \begin{itemize}
% \item
% ICFP2000 --- Raytracer
% \item
% Mandelbrot Image generator.
% \item
% Variations on the above two programs including varying degrees
% of dependence and a \mapfoldl version.
% \item
% ICFP2001 --- SGML optimizer
% \item
% Compiler --- We do not expect this to speed up.
% However it will be useful to ensure that it does not slow down too much.
% \item
% pic --- a NuFib benchmark by ported by Peter
% \item
% SWRL --- An inference engine provided by Mission Critical.
% % The mission critical benchmark is at:
% %
% %     taura:/home/taura/pbone/mcdemo/swrl-snapshot.tar.gz
% %
% % SWRL is an inference engine.  Building and running it is covered by:
% %
% %     taura:/home/taura/pbone/mcdemo/swrl.txt
% %
% % These files are owned by a new group, mcdemo, since they are MC's
% % closed source project.  Peter Ross is easy going and has let us use
% % them without any formal non-disclosure agreement.  That said, I am
% % respecting this IP as much as I would anything where I had signed a
% % formal NDA.
% \end{itemize}

% Two programs, a raytracer and a mandelbrot image generator showed
% strong speedups, see figure \ref{tab:results}.
% This confirms that our analysis is correctly identifying parallelism
% available within the main loops of these programs.
% The two programs have a similar structure, they both have a loop with
% an accumulator that contains the rows of the images already rendered.
% We believe that a lot of dependent parallelism has this form as
% programmers in declarative languages are trained to use accumulators
% in their loops to ensure that the loop is tail-recursive.
% The mandelbrot program's loop uses \mapfoldl example above.
% \paul{Should I include a back-reference for the mapfoldl figure?}
%
% We used different inputs for the profiling and benchmarking
% executions to ensure that the profile analysis would not over-fit the
% parallelisation to a particular input.

% Other programs tested included the Mercury compiler and pic, a program
% ported to Mercury from the nofib~\cite{nofib} benchmark suite.
% Our analysis found exploitable parallelism within these benchmarks,
% however,
% it appears to be too fine-grained for Mercury's runtime to handle.
% We hope to correct these problems fix before the camera ready deadline.

% I will need to test a na\"ive approach, for instance: assuming maximum
% overlap, or factoring in some fixed cost for each shared variable.

% \section{Related work}
% \label{sec:related_work}

\section{Related work and conclusion}
\label{sec:overlap_conc}

% Mercury's strong mode system
% greatly simplifies the parallel execution of logic programs,
% making the comparison of parallel Mercury with parallel Prolog difficult.
% For example, \cite{Hermenegildo1995} defines non-strict
% goal independence such that goals that are non-strictly independent can be
% run in parallel without leading to incorrect results.
% Because Mercury
% statically determines a single goal in a conjunction to bind each variable,
% and because Mercury does not permit variables to be aliased,
% the conditions of non-strict goal independence
% are not necessary for Mercury to guarantee correctness.
% Similarly, other existing work on AND-parallelism in Prolog
% is not closely related to the present work,
% because Mercury sidesteps the
% problems that work seeks to overcome.
% \peter{Is that too hand-wavey and dismissive?}

Mercury's strong mode and determinism systems
greatly simplify the parallel execution of logic programs.
The information gathered by semantic analysis in Mercury
makes it easy to solve most of the problems faced by the
designers of parallel versions of Prolog and Prolog-like languages.
These include testing the independence of goals
in systems that support only independent AND-parallelism
and discovering producer-consumer relationships
in systems that also support dependent AND-parallelism,
such as \cite{DBLP:journals/tcs/GrasH09}.
They also make it possible to \emph{avoid} having to solve some tough problems,
the main example being how to execute nondeterministic conjuncts in parallel
without excessive overhead.

% That is what they were \emph{designed} to do.
% The information gathered by semantic analysis in Mercury
% Many problems in the parallel execution of Prolog and Prolog-like languages,
% like testing the independence of goals
% in systems that support only independent AND-parallelism,
% discovering producer-consumer relationships at runtime
% in systems that also support dependent AND-parallelism,
% and having to handle nondeterministic conjuncts,
% disappear completely,
% with the answers to the problem being presented on a silver platter
% Our group designed Mercury specifically to ensure this.

Most research in parallel logic programming so far
has focused on trying to solve these problems
of getting parallel execution to \emph{work} well,
with only a small fraction trying to find
when parallel execution would actually be \emph{worthwhile}.
Almost all previous work on automatic parallelisation
has focused on granularity control:
parallelising only computations that are expensive enough
to make parallel execution
worthwhile \cite{harris_07_feedback_imp_par,lopez96:distance_granularity},
and properly accounting for the overheads
of parallelism itself \cite{shen_98_granularity-control}.
Most of the rest has tried to find opportunities
to exploit independent AND-parallelism
during the execution of otherwise-dependent conjunctions
\cite{DBLP:journals/jlp/MuthukumarBBH99,DBLP:conf/lopstr/CasasCH07}.

Our experience with our feedback tool shows that
for Mercury programs, this is far from enough.
For most programs,
it finds enough conjunctions with two or more expensive conjuncts,
but almost all are dependent,
and, as we mention in section~\ref{sec:perf},
many of these have too little overlap to be worth parallelising.
% For example, the Mercury compiler contains
% 50 conjunctions with two or more expensive goals.
% 49 of these are dependent.
% Of these, only 38 of these have any overlap,
% and only for 31 does the overlap
% lead to a predicted local speedup of more than 1\%.

We know of only three attempts to estimate the overlap
between parallel computations.
One was in the context of speculative execution in imperative programs.
Given two successive blocks of instructions,
\cite{von_Praun:2007:implicit_parallelism_with_ordered_transactions}
% estimates the likely speedup
% from executing the two blocks in parallel
% by using the difference between the addresses of two instructions
decides whether the second block should be executed speculatively
based on the difference between the addresses of two instructions,
one that writes a value to a register and one that reads from that register.
% This is effectively a binary metric.
This works if instructions take a bounded time to execute,
but in the presence of call instructions
this heuristic will not be at all accurate.

Another attempt was a previous auto-parallelisation project for
Mercury \cite{tannier:2007:parallel_mercury}.
% This did not use profiling data,
% and instead used the number of shared variables between conjuncts
This used the number of shared variables between conjuncts
as a measure of the dependency between goals,
and as a predictor of the likely overlap.
While two conjuncts are indeed less likely
to have useful parallel overlap if they have more shared variables,
we have found this heuristic too inaccurate to be useful.

The most closely related work to ours
generated parallelism annotations for the ACE and/or-parallel system
\cite{Pontelli97automaticcompile-time}.
This system used, much as we do,
estimates of the costs of calls
and of the times at which variables are produced and consumed.
However, it produced its estimates through static analysis of the program.
This can work for small programs,
where the call trees of the relevant calls can be quite small and regular.
In large programs, the call trees of the expensive calls
are almost certain to be both tall and wide,
with a huge gulf between best-case and worst-case behaviour.
Using profiling data is the only way
for an automatic parallelisation system to find out
what the \emph{typical} behaviour of such calls is.

% There is a risk that the program could have changed between the
% profiling build and the parallelised build,
% this makes it more difficult for the compiler to apply the profiling
% advice.
% To reduce this risk the profiling build should be built with the same
% optimizations that the parallelised build will be built with.
% In usual circumstances inlining should be disabled during profiling so
% that a programmer can more easily understand their program's profile.
% Our implementation re-enables inlining in profiling builds if a
% suitable optimization level is selected and
% \code{--profile-for-implicit-parallelism} is passed to the compiler.
% % XXX: These details may be unimportant, especially the name of this
% % compiler option,  But this is (for now) an easy way to describe
% % this.

Our system's predictions of the likely speedup from parallelising a conjunction
are also fallible, since they currently ignore several relevant issues,
including cache effects
and the effects of bottlenecks
such as CPU-memory buses and stop-the-world garbage collection.
However, our system seems to be a sound basis for such further refinements.
% However, they come much closer
% to predicting actual overlaps than previous attempts,
% and our system seems to be a sound basis for further refinements.
% \begin{itemize}
% \item
% It is hard to define what a typical workload is,
% and we do not yet implement profile merging.
% \item
% The feedback framework is general purpose
% and can be used for other optimizations.
% \item
% \zoltan{I haven't covered any technical details about the feedback framework.
% I guess there's not much to say.}
% \end{itemize}
In the future, we plan to support parallelisation as a specialisation:
applying a specific parallelisation only when a predicate is called
from a specific parent, grandparent or other ancestor.
% we will look at how best to resolve cases
% where our tool gives different parallelisation advice for the same conjunction
% due to the different behaviour of that conjunction in different contexts.
We also plan to modify our feedback tool
to accept several profiling data files,
with a priority scheme to resolve any conflicts.
% between their advice.

% \paragraph{Acknowledgements}
We thank the rest of the Mercury team,
and Tom Conway and Peter Wang in particular,
for creating the infrastructure we build upon,
and the anonymous referees for their suggestions.

\section{Related work}

\paul{Say how this work differs from my honours work.}

The previous version of the tool did not use the call graph.
Instead it looked at the most expensive procedures,
considering only those whose cost was above its own threshold.
The call graph traversal is better because it allows us to avoid parallelising
a callee when the caller already provides enough parallelism.

\paul{Throttle recursive parallelism based on the cost of each iteration.}
\paul{Handle other recursion times intelligently.}

% TODO Items.


% \begin{algorithm}
% \begin{verbatim}
% MaxBefore := 0
% N := num_conjuncts(Conjs)
% for i in 1 to N:
%     if conjunct i in Conjs is below threshold then
%         MaxBefore := i
%     else
%         break
%
% MinAfter := N+1
% for i in N downto 1:
%     if conjunct i in Conjs is below threshold then
%         MinAfter := i
%     else
%         break
%
% BestTime := infinity
% Arrangements := [[[conjunct MaxBefore+1]]]
% # each element in Arrangements is
% #   a list of parallel conjuncts
% # each parallel conjunct consists of
% #   a list of consecutive conjuncts
% for i in MaxBefore+2 to MinAfter-1:
%     NewArrangements := []
%     for Arrangement in Arrangements:
%         ExtendLast := all_but_last(Arrangement)
%             ++ [last(Arrangement) ++ conjunct i]
%         AddNewLast := Arrangement ++ [conjunct i]
%         NewArrangements := NewArrangements ++
%             [ExtendLast, AddNewLast]
%     Arrangements := NewArrangements
%
%     for Before in 0 to MaxBefore:
%         for After in MinAfter to N+1:
%             GoalsBefore := conjuncts 1 .. Before in Conjs
%             GoalsAfter  := conjuncts After .. N in Conjs
%             # GoalsBefore and/or GoalsAfter may be empty
%             ExtraGoalsBefore := conjuncts (Before+1) .. MaxBefore in
%                Conjs
%             ExtraGoalsAfter := conjuncts MinAfter .. (After-1) in
%                Conjs
%
%             for each Arrangement in Arrangements
%                 Arrangement := [ExtraGoalsBefore ++ first(Arrangement)] ++
%                    all_but_first_and_last(Arrangement) ++
%                     [last(Arrangement) ++ ExtraGoalsAfter]
%                 ParConj := par_conj(Arrangement)
%                 OverallGoal :=
%                     seq_conj(GoalsBefore ++ [ParConj] ++ GoalsAfter)
%                 Time := compute_par_exec_time(OverallGoal)
%                 if Time < BestTime:
%                     BestTime := Time
%                     BestGoal := OverallGoal
% \end{verbatim}
% \caption{Search for best parallelisation}
% \label{alg:branch_and_bound_search}
% \end{algorithm}

