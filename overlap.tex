% vim: ts=4 sw=4 et ft=tex

\status{This has not been revised since the paper. I have only removed parts
of it to place elsewhere, for example, in the background section.}

\begin{itemize}
\item \paul{
    Add discussion about how our data-flow analysis does not recognise loops
    since they need to be treated specially.}

\item \paul{Explain the limitations of deep profiling,
      for example, that levels within recursive code do not have separate
      data recorded for them.}
\item \paul{Describe Callgraph search}
\item \paul{Minor changes to var use algorithm.}
\item \paul{Coverage (and var use) now work on deep profiling data.}
\end{itemize}

Introducing parallelism into a Mercury program seems easy.
A programmer can use the parallel conjunction operator (an ampersand)
instead of the plain conjunction operator (a comma) to tell the compiler
that the conjuncts of the conjunction should be executed in parallel with
one another.
However,
introducing parallelism efficiently is much more difficult.
It is easy to make common mistakes.
One such mistake is creating too much parallelism: 
making a task available to another CPU 
may take thousands of instructions,
spawning off a task that takes only a hundred instructions is clearly a loss.
Even spawning off a tack of a few thousand instructions is not a win;
it should only be done for computations
that take long enough to benefit from parallelism.
Researchers have therefore worked towards automatic parallelisation.
Autoparallelising compilers have long tried to use granularity analysis to
ensure that they only spawn off computations whose cost will probably exceed
the spawn-off cost by a comfortable margin.
However, this is not enough to yield good results,
because data dependencies may \emph{also} limit
the usefulness of running computations in parallel.
If one computation blocks almost immediately
and can resume only after another has completed its work,
then the cost of parallelisation again exceeds the benefit.

This chapter is based on and extends our paper:

\begin{quote}
\pubauthor{Paul Bone, Zoltan Somogyi and Peter Schachte.}
\pubtitle{Estimating the overlap between dependent computations for automatic
parallelization}
\pubhow{Theory and Practice of Logic Programming,}{11(4--5):575--591, 2011.}
\end{quote}

\noindent
It presents a set of algorithms for recognising places in a program
where it is worthwhile to execute two or more computations in parallel
that pay attention to the second of these issues as well as the first.
Our system uses profiling information to compute
the times at which a procedure call consumes the values of its input arguments
and the times at which it produces the values of its output arguments.
Given two calls that may be executed in parallel,
our system uses the times of production and consumption
of the variables they share
to determine how much their executions are likly to overlap
when run in parallel,
and therefore whether executing them in parallel is a good idea or not.

We have implemented this technique for Mercury
in the form of a tool\footnote{
    This tool was derrived from the initial implementation described in
    Section \ref{honours_autopar}, Page \pageref{honours_autopar} and by
    \citet{bone:2008:hons}.
    However many parts of the tool have changed and it has only very few
    parts in common with the original version.
    Everything described in this chapter is new unless otherwise specified.}
that uses profiling data
to generate recommendations about what to parallelise,
for the Mercury compiler to apply on the next compilation of the program.
We present preliminary results that show that
this technique can yield useful parallelisation speedups,
while requiring nothing more from the programmer
than representative input data for the profiling run.

\section{Introduction}
\label{sec:intro}

\picfigure{small_overlap}{Ample vs smaller parallel overlap between \code{p} and \code{q}}

% Multicore is ubiquitous.

% Parallel programming is hard.

% Pure Declarative languages make it safe, and Automatic
% parallelisation makes it easier.

When parallelising Mercury~\cite{mercury_jlp} programs,
the best parallelisation opportunities occur
where two goals take a significant and roughly similar time to execute.
Their execution time should be as large as possible
so that the relative costs of parallel execution are small,
and they should be independent to minimise synchronisation costs.
Unfortunately, goals expensive enough to be worth executing in parallel
are rarely independent.
For example, in the Mercury compiler itself,
there are 53 conjunctions containing two or more expensive goals,
but in only one of those conjunctions are the expensive goals independent.
This is why Mercury supports the parallel execution of dependent conjunctions.
The Mercury compiler wraps shared variables within a
\emph{future}~\cite{wang:2011:dep-par,wang:2006:hons}, to
ensure that the \emph{consumer} of the variable is blocked
until the \emph{producer} makes the variable available.

Dependent parallel conjunctions may differ
in the amount of parallelism they have available.
Consider a parallel conjunction with two similarly-sized conjuncts,
\code{p} and \code{q}, that share a single variable \code{A}.
If \code{p} produces \code{A} late but \code{q} consumes it early,
as shown on the right side of figure \ref{fig:dep_conj_overlap1},
there will be little parallelism,
since \code{q} will be blocked soon after it starts,
and will be unblocked only when \code{p} is about to finish.
Alternatively, if \code{p} produces \code{A} early
and \code{q} consumes it late,
as shown on the left side of in figure~\ref{fig:dep_conj_overlap1},
we would get much more parallelism.
The top part of each scenario
shows the execution of the sequential form of the conjunction.

% \paul{It would be nice to have some data to backup this argument,
% although the argument is easy to believe so it will not be necessary.}
% \peter{Data would be good.  Or you could drop the claim.}
% \zoltan{Everyone who has written real programs knows the claim to be true.}
Unfortunately, in real Mercury programs,
almost all conjunctions are dependent conjunctions,
and in most of them,
shared variables are produced very late and consumed very early.
Parallelising them would therefore yield slowdowns instead of speedups,
because the overheads of parallel execution would far outweigh the benefits.
We want to parallelise only conjunctions
in which any shared variables are produced early, consumed late,
or (preferably) both.
The first purpose of this paper is to show how one can find these conjunctions.

\begin{figure}[b]
\begin{verbatim}
map_foldl(_, _, [], Acc, Acc).
map_foldl(M, F, [X | Xs], Acc0, Acc) :-
    M(X, Y),
    F(Y, Acc0, Acc1),
    map_foldl(M, F, Xs, Acc1, Acc).
\end{verbatim}
\caption{\mapfoldl}
% the recursive call is less dependent
% on the conjunction of the first two calls.
\label{fig:map_foldl}
%\vspace{-2\baselineskip}
\end{figure}

The second purpose is to find the best way to parallelise these conjunctions.
Consider the \mapfoldl predicate in figure~\ref{fig:map_foldl}.
The body of the recursive clause has three conjuncts.
We could make each conjunct execute in parallel,
or we could execute two conjuncts in sequence
(either the first and second, or the second and the third),
and execute that sequential conjunction in parallel with the remaining conjunct.
In this case, there is little point in executing
the higher order calls to the map and fold predicates
in parallel with one another,
since in virtually all cases,
the map predicate will generate \code{Y} very late and
the fold predicate will consume \code{Y} very early.
However, executing the sequential conjunction of the map and fold predicates
in parallel with the recursive call \emph{will} be worthwhile
if the map predicate is time-consuming,
because this implies that
a typical recursive call will consume its fourth argument late;
the recursive call processing the second element of the list
will have significant execution overlap
with its parent processing the first element of the list
even if (as is typical) the fold predicate generates \code{Acc1} very late.
(This is the kind of computation that
Reform Prolog \cite{bevemyr:reform} was designed to parallelise.)

The structure of this paper is as follows.
Section~\ref{sec:background} gives
the background needed for the rest of the paper.
Section~\ref{sec:approach} outlines our general approach,
which the later sections fill in.
Section~\ref{sec:overlap} describes our algorithm for calculating
the execution overlap between two or more dependent conjuncts.
A conjunction with more than two conjuncts can be parallelised
in several different ways;
section~\ref{sec:howto} shows how we choose the best way.
\tr{Section~\ref{sec:pragmatic} discusses some pragmatic issues.}
Section~\ref{sec:perf} evaluates
how our system works in practice on some example programs, and
section~\ref{sec:conc} concludes
with comparisons to related work.

% \paul{Reference the conclusion and other final sections}

\section{Background}
\label{sec:background}


\section{Our general approach}
\label{sec:approach}

% \begin{figure}
% \begin{center}
% \include{prof_fb}
% \centerline{\raise 1em\box\graph}
% \end{center}
% \caption{Automatic parallelisation workflow}
% \label{fig:profiler_feedback_workflow}
% \end{figure}

% During automatic parallelisation,
% a compiler may be able to estimate some cost information from static
% analysis.
% However, this will not be accurate;
% static analysis cannot take into account sizes of data terms or other
% values that are only available at runtime.
% Therefore, we use profiler feedback information in our implementation.
% Figure \ref{fig:profiler_feedback_workflow} shows a workflow where the
% program is first compiled for profiling, executed, the profile is then
% analyzed by an analysis tool before the results are fed back into the
% compiler to build the parallel version of the program.

We want to find the conjunctions in the program
whose parallelisation would be the most profitable.
This means finding the conjunctions with conjuncts
whose execution cost exceeds the spawning-off cost by the highest margin,
and whose interdependencies, if any,
allow their executions to overlap the most.
% \paul{Is the next sentence needed?}
% \peter{No, but does not hurt, either.}
% It is better to spawn off a medium-sized computation
% whose execution can overlap almost completely
% with the execution of another medium-sized computation,
% than it is to spawn off a big computation
% whose execution can overlap only slightly
% with the execution of another big computation,
% but it is better still to spawn off a big computation
% whose execution can overlap almost completely
% with the execution of another big computation.
Essentially, the greater the margin by which
the likely runtime of the parallel version of a conjunction beats
the likely runtime of the sequential version,
the more beneficial parallelising that conjunction will be.

To compute this likely benefit,
we need information
both about the likely cost of calls
and the execution overlap allowed by their dependencies.
Our system therefore asks programmers
to follow this sequence of actions
after they have tested and debugged the program.

\begin{enumerate}
\item
Compile the program
with options asking for profiling.
% for automatic parallelisation.
\item
Run the program on a representative set of input data.
This will generate a profiling data file.
\item
Invoke our feedback tool on the profiling data file.
This will generate a parallelisation advice file.
\item
Compile the program for parallel execution,
specifying the parallelisation advice file.
The advice file tells the compiler
\emph{which} sequential conjunctions to convert to parallel conjunctions,
and exactly \emph{how}.
For example, \code{c1, c2, c3} can be converted
into \code{c1 \& (c2, c3)},
into \code{(c1, c2) \& c3}, or
into \code{c1 \& c2 \& c3},
and as the \code{map\_foldl} example shows,
the speedups you get from them can be strikingly different.
\end{enumerate}

\noindent
It is up to the programmer using our system
to select training input for the profiling run in step 2.
Obviously, programmers should pick input that is as representative as possible,
but the recommended parallelisation can be useful
even for input data that is quite different from the training input.
The main focus of this paper is on step 3;
we give the main algorithms used by the feedback tool.
\tr{
However, we will also touch on steps 1 and 4.
}

% \paul{The coverage work should really be published separately.
% For this paper we should just say 'we have a way of obtaining
% coverage data'.  This is probably acceptable for the workshop.}
Our feedback tool is an extension of the Mercury deep profiler.
% \paul{make the rest of this paragraph an itemize?}
% \peter{I wouldn't.}
One of our modifications gives the deep profiler access
to the relevant parts of the compiler's representation of the program.
This includes a representation of each procedure body,
and for each atomic subgoal (call or unification) within each body,
the set of variables bound by that subgoal.
Another modification records
how many times execution reaches each point in the program.
\tr{
Since even the unmodified deep profiler could figure this out
for \emph{most} program points from the call counts associated with call sites,
we need to gather execution counts at only a few additional sites
to allow us to figure it out for \emph{all} program points.
}
As we will see in section~\ref{sec:overlap},
we need this information
to calculate the likely speedup from parallelising a conjunction.

% \paul{This is not yet implemented and will not be for this version of the paper.}
% \peter{Then you need to say that.}

% GREEDY_SEARCH The top level algorithm of the feedback tool
% GREEDY_SEARCH is a traversal of the tree of cliques
% GREEDY_SEARCH recorded in the deep profiling data file.
% GREEDY_SEARCH Each clique has its own unique entry point,
% GREEDY_SEARCH which will be a call site in a higher clique;
% GREEDY_SEARCH this higher clique is the parent node of this clique.
% GREEDY_SEARCH Likewise, every call site
% GREEDY_SEARCH in every procedure in the clique
% GREEDY_SEARCH will be the entry point of another clique,
% GREEDY_SEARCH provided that
% GREEDY_SEARCH (a) it is actually executed and (b) the callee is not in this clique.
% GREEDY_SEARCH These lower cliques are the children of this clique.

% GREEDY_SEARCH % We will describe our traversal algorithm in detail
% GREEDY_SEARCH % in section \ref{sec:bestfirst},
% GREEDY_SEARCH % but for now, consider this traversal
% GREEDY_SEARCH % as operating on a \emph{candidates list},
% GREEDY_SEARCH % a list of cliques sorted on total cost.
% GREEDY_SEARCH Our traversal algorithm operates on a \emph{candidates list},
% GREEDY_SEARCH which contains a list of cliques sorted on total cost.
% GREEDY_SEARCH We start with the list containing only
% GREEDY_SEARCH the clique of the top level call to \code{main},
% GREEDY_SEARCH the predicate where every Mercury program starts execution.
% GREEDY_SEARCH Then, at each step,
% GREEDY_SEARCH \begin{itemize}
% GREEDY_SEARCH \item
% GREEDY_SEARCH we remove the clique at the start of the candidates list;
% GREEDY_SEARCH \item
% GREEDY_SEARCH we process this clique
% GREEDY_SEARCH by looking at the conjunctions in the clique's procedures
% GREEDY_SEARCH to see whether they should be parallelised; and then
% GREEDY_SEARCH \item
% GREEDY_SEARCH we insert the child cliques (if any) of this clique into the candidates list.
% GREEDY_SEARCH \end{itemize}
% GREEDY_SEARCH We stop when either even the highest cost candidate
% GREEDY_SEARCH is too cheap to be worth parallelising,
% GREEDY_SEARCH or we have achieved our target CPU utilisation
% GREEDY_SEARCH for all phases of the program's execution.

% GREEDY_SEARCH This is only an outline of our traversal algorithm.
% GREEDY_SEARCH In section \ref{sec:pragmatic}, we will describe it in detail,
% GREEDY_SEARCH together with our solutions to several issues that come up in practice.

Our feedback tool looks for parallelisation opportunities
by doing a depth-first search
of the call tree recorded in the profiling data file.
It explores the subtree below a node in the call tree
only if the overall cost of the call
is greater than a configurable threshold,
and if the amount of parallelism it has found at and above that node
is below another configurable threshold.
The first test lets us avoid looking at code
that would take more work to spawn off than to execute,
while the second test lets us avoid creating
more parallel work than the target machine can handle.

For each procedure in the call tree,
we search its body for conjunctions that contain
two or more calls with execution times above a configurable threshold.
To parallelise the conjunction,
its conjuncts have to be partitioned,
each partition being one conjunct in the parallel conjunction.
In most cases, this can be done in several different ways.
% Parallelising a conjunction
% requires partitioning the set of the original conjuncts
% into two or more groups,
% with the conjuncts in each group being executed sequentially
% but different groups being executed in parallel.
% This can usually be done in several different ways.
We can use the algorithms of section \ref{sec:overlap}
to compute the expected parallel execution time of each partition;
these algorithms take into account the runtime overheads of parallel execution.
We use the algorithms of section \ref{sec:howto} to generate
the set of partitions whose performance we want to evaluate.
If the best-performing parallelisation we find
shows a nontrivial speedup over sequential execution,
we remember that we want to perform that parallelisation on this conjunction.
% TODO: This feature is not yet implemented.
If the depth first search later finds
some of the conjuncts to have parallelisable code inside them,
we revisit this conjunction,
this time using updated data about the cost of those conjuncts.
Otherwise,
we add a recommendation to perform the selected parallelisation
to the feedback advice we generate for the compiler.

An important benefit of profile-directed parallelisation is that
since programmers do not annotate the source program,
it can be re-parallelised easily after a change to the program
obsoletes some old parallelisation opportunities and creates others.
Nevertheless, if programmers want to parallelise some conjunctions manually,
they can do so: our system will not override the programmer.

% \zoltan{this is wrong: the overheads should be PART OF the parallel time}
% \begin{equation*}
% Speedup = \frac{Time_{Seq}}{Time_{Par} + ParOverheads}
% \end{equation*}

% For journal version
% \section{The cost of recursive calls}
% \label{sec:reccalls}
%
% % leave discussion of granularity estimation by static analysis
% % for the related work section;
% % mention that this work has not extended to large programs.
%
% The Mercury deep profiler gives us directly
% the costs of all non-recursive call sites in a clique.
% For recursive calls,
% the costs of the callee are mingled together
% with the costs of the caller,
% which is either the same procedure as the callee,
% or is mutually recursive with it.
% Therefore if we want to know the cost of a recursive call site (and we do),
% we have to infer this
% from the cost of the clique as a whole,
% the cost of each call site within the procedures of the clique,
% the structures of the bodies of those procedures,
% and the frequency of execution of each path through those bodies.
%
% For now, we will restrict our attention to cliques
% that contain only a single procedure,
% since this inference process is much simpler for them.
% Later, we will discuss how this restriction can be lifted at least partially.
% We will further restrict our attention to procedures
% whose bodies match one of a small set of patterns.
%
% {\bf Pattern 1: simply recursive procedures.}
% The first pattern consists of procedures whose bodies
% that have just two execution paths through them:
% a base case, and a recursive case containing a single recursive call site.
% Our example for this category is \code{map\_foldl},
% a predicate in the Mercury standard library
% that does first a map and then a fold over a list,
% whose code is shown in figure~\ref{fig:map_foldl}.
%
% Let us say that of the 100 calls to the procedure,
% 90 were from the recursive call site
% and 10 were from a call site in the parent clique.
% Then we would calculate
% that each non-recursive call
% would on average yield nine recursive calls.
% The last of these would take the nonrecursive path, and incur only its costs.
% The second last would take the recursive path,
% and incur one copy of the costs of the nonrecursive calls along that path,
% plus the cost of the last call.
% The third last would incur two copies of the costs
% of the nonrecursive calls along the recursive path,
% plus the cost of the last call.
%
% XXX should we discuss conditionals?
%
% more than once base case can be folded into one;
% more than once recursive case can be folded into one.
%
% {\bf Pattern 2: Divide-and-conquer procedures.}
% The second pattern consists of procedures whose bodies
% that also have just two execution paths through them:
% a base case, and a recursive case containing \emph{two} recursive call sites.
% Our example for this category is an accumulator version of \code{quicksort},
% whose code is shown in figure~\ref{fig:quicksort_acc}.

\section{Calculating the overlap between dependent conjuncts}
\label{sec:overlap}

As we can see from the difference between the two sides of
figure~\ref{fig:dep_conj_overlap1},
figuring out the overlap
in the parallel executions of two dependent conjuncts
requires knowing, for each of the variables they share,
when that variable is generated by the first conjunct and
when it is first consumed by the second conjunct.
Our algorithms for computing these times are considerably simplified
by the Mercury mode system
and by the fact that we only parallelise deterministic goals.

The profiling data gives us both
the total execution time of each conjunct
and its number of invocations;
the ratio of the two is the expected execution time for each invocation.


Suppose the first appearance of the variable (call it $X$)
in a conjunction $G_1, \ldots, G_n$ is in $G_k$, and $G_k$ is a switch.
If $X$ is consumed by some switch arms and not others,
then on some execution paths,
the first consumption of the variable may be in $G_k$ (a),
on some others it may be in $G_{k_1}, \ldots, G_n$ (b),
% XXX: We do not handle c - but we should.
and on some others it may not be consumed at all (c).
For case (a),
we compute the average time of first consumption by the consuming arms,
and then compute the weighted average of these times,
with the weights being the probability of entry into each arm, as before.
For case (b), we compute the probability of entry
into arms which do \emph{not} consume the variable,
and multiply the sum of those probabilities
by the weighted average of those arms' execution time
\emph{plus} the expected consumption time of the variable
in $G_{k+1},~\ldots,~G_n$.
For case (c)
we pretend $X$ is consumed at the very end of the goal,
and then handle it in the same way as (b).
This is because for our overlap calculations,
a goal that does not consume a variable is equivalent to
a goal that consumes it at the end of its execution.

% Show formulas for calculating the overlap in a simple case, one
% shared variable between two conjuncts.

Suppose a candidate parallel conjunction has two conjuncts $p$ and $q$,
and their execution times in the original, sequential conjunction $p, q$,
are ${SeqTime}_p$ and ${SeqTime}_q$.
Suppose ${SV}_i$ are the variables shared between them,
and for each ${SV}_i$,
the time at which $p$ produces it is ${ProdTime}_{pi}$, and
the time at which $q$ consumes it is ${ConsTime}_{qi}$.

If we denote the execution times of the conjuncts
in the parallel conjunction $p~\&~q$
as ${ParTime}_p$ and ${ParTime}_q$,
then the expected speedup
from parallelising the original sequential conjunction
is ${Speedup} = {SeqTime} / {ParTime}$,
where ${SeqTime} = {SeqTime}_p + {SeqTime}_q$,
and ${ParTime} = {SpawnOverhead} + {max}({ParTime}_p, {ParTime}_q)$.
The profile gives us ${SeqTime}_p$ and ${SeqTime}_q$,
and if we ignore overheads for now (we will come back to them later),
then ${ParTime}_p$ will always be equal to ${SeqTime}_p$.
The main task of computing the speedup
therefore consists of computing ${ParTime}_q$;
as we saw in figure~\ref{fig:dep_conj_overlap1},
this will differ from ${SeqTime}_q$
whenever $q$ needs to wait for $p$ to produce a shared variable.

\begin{figure}[tb]
\begin{center}
\begin{verbatim}
find_par_time(Conjs) returns TotalParTime:
  N := length(Conjs)
  ProdTimeMap := empty
  TotalParTime := 0
  for i in 1 to N:
    CurSeqTime := 0
    CurParTime := 0
    sort ProdConsList_i on Time_ij
    forall (Var_ij, Time_ij) in ProdConsList_i:
      Duration_ij := Time_ij - CurSeqTime
      CurSeqTime := CurSeqTime + Duration_ij
      if Conj_i produces Var_ij:
        CurParTime := CurParTime + Duration_ij
        ProdTimeMap[Var_ij] := CurParTime
      else Conj_i must consume Var_ij:
        ParWantTime := CurParTime + Duration_ij
        CurParTime := max(ParWantTime, ProdTimeMap[Var])
    DurationRest_i := SeqTime_i - CurSeqTime
    CurParTime := CurParTime + DurationRest_i
    TotalParTime := max(TotalParTime, CurParTime)
\end{verbatim}
\end{center}
\caption{Dependent parallel conjunction algorithm}
\label{fig:dep_par_conj_overlap_middle}
%\vspace{-2\baselineskip}
\end{figure}

% XXX \iclp{
Figure~\ref{fig:dep_par_conj_overlap_middle} shows
a simplified version of the algorithm we use to compute
the expected execution time of a conjunction
when its conjuncts are executed in parallel,
assuming an unlimited number of CPUs.
The inputs of the algorithm are \verb|Conjs|, the conjuncts themselves,
and \verb|ProdConsList|,
which gives, for each conjunct,
the list of its input and output variables,
together with the times at which,
% (during the profiling run, which executes the conjuncts in sequence)
in a sequential execution,
they are respectively first consumed or produced.
The times are relative to the start of the execution of the relevant conjunct.

The main task of the algorithm is
to divide the execution times of all the conjuncts into chunks
and keep track of when those chunks can execute.
The execution time of \verb|Conj_i|
has one chunk (\verb|Duration_ij|) for each of \verb|Conj_i|'s shared variables
that ends at the time at which that variable is produced or first consumed,
and there is one chunk (\verb|DurationRest_i|) at the end,
during which the call may produce non-shared variables.
Figure~\ref{fig:dep_conj_overlap1} shows that
the production of $A$ divides $p$ into two chunks, ${pA}$ and ${pR}$,
while the consumption of $A$ divides $q$ into ${qA}$ and ${qR}$.

The algorithm processes the chunks in order, and keeps track
of the sequential and parallel execution times of the chunks so far.
When a chunk of \verb|Conj_i| ends with the production of a variable,
we record when that variable is produced,
and the next chunk can start executing immediately.
When a chunk ends with the consumption of a variable,
then in the \emph{sequential} version of \verb|Conj_i|
the next chunk can also execute immediately,
since the values of all the input variables will be available when it starts,
but in the \emph{parallel} version,
the variable may not have been produced yet.
If it has, then \verb|Conj_i| does not need to wait for it;
the left side of figure~\ref{fig:dep_conj_overlap1} shows this case.
However, it is also possible that it has not.
In that case, \verb|Conj_i| will suspend on the variable,
and will resume only when its producer signals that it is available;
the right side of figure~\ref{fig:dep_conj_overlap1} shows this case.
Note that \verb|Var_ij|
will always be in \verb|ProdTimeMap| when we look for it,
because the Mercury mode system reorders conjunctions
to put the producer of each variable before all its consumers.
% in a two-conjunct conjunction,
% the left conjunct can only produce
% the variables it shares with the right conjunct
% and the right conjunct can only consume
% those variables.
% However, in longer conjunctions,
% the conjuncts in the middle
% can both consume variables produced by conjuncts on their left
% and produce variables consumed by conjuncts on their right.
% This is why our algorithm associates with \code{Conj\_i}, the $i$th conjunct,
% \code{ProdConsList\_i}:
% the list of shared variables that \code{Conj\_i} either produces or consumes,
% together with their times of production and first consumption respectively.

The version of this algorithm we have actually implemented is 
a bit longer than the one in figure~\ref{fig:dep_par_conj_overlap_middle},
because it also accounts for several forms of overhead:

\begin{itemize}
\item
Creating a spark and adding it to a work queue has a cost.
Every conjunct but the last conjunct incurs this cost
to create the spark for the rest of the conjunction.
\item
It takes some time to take a spark off a spark queue,
create or reuse a context for it, and start its execution.
Every parallel conjunct that is not the first incurs this delay
before it starts running.
\item
The signal and wait operations have a cost.
\item
It takes some time to wake up a context when its wait operation succeeds.
\item
It takes time for each conjunct to synchronise on the barrier
when it has finished its job.
\end{itemize}

\noindent
We can account for every one of these overheads
by adding the estimated cost of the relevant operation to \verb|CurParTime|
at the right point in the algorithm.

In many cases,
the conjunction given to the algorithm shown in figure~\ref{fig:dep_par_conj_overlap_middle}
will contain a recursive call.
In such cases, the speedup computed by the algorithm
reflects the speedup we can expect to get when the recursive call
calls the \emph{original, sequential} version of the predicate.
When the recursive call calls the parallelised version,
we can expect a similar saving (absolute time, not ratio)
on \emph{every} recursive invocation.
How this affects the expected speedup of the top level call
depends on the structure of the recursion.
For the most common recursion structure,
singly recursive predicates like \verb|map_foldl|,
calculating the expected speedup of the top level call is easy,
since we can compute the average depth of recursion
from the relative execution counts of the base and recursive cases.
For some less common structures,
such as doubly recursive predicates like \verb|quicksort|, it is a bit harder,
and for irregular structures in which different execution paths
contain different numbers of recursive calls,
the profiling data gathered by the current version of the Mercury profiler
contains insufficient information to allow our system to determine the
expected speedup.
However, an automated survey of the programs handled by our feedback tool
shows that such predicates are rare;
our system can compute
the expected recursion depth and therefore the expected speedup
for virtually all candidates for parallelisation.

So far, we have assumed an unlimited number of CPUs,
which is of course unrealistic.
If the machine has e.g.\ four CPUs,
then the prediction of any speedup higher than four is obviously invalid.
Less obviously,
even a predicted overall speedup of less than four may depend
on more than four conjuncts executing all at once at \emph{some} point.
We have not found this to be a problem yet.
If and when we do,
we intend to extend our algorithm to keep track
of the number of active conjuncts in all active time periods.
Then if a chunk of a conjunct wants to run in a time period
when all CPUs are predicted to be already busy executing previous conjuncts,
we assume that the start of that chunk is delayed until a CPU becomes free.

The limited number of CPUs also means that
there is a limit to how much parallelism we actually \emph{want}.
The spawning off of every conjunct incurs overhead,
but these overheads do not buy us anything if all CPUs are already busy.
% If the machine has e.g.\ four CPUs,
% then we do not actually want to spawn off
% hundreds of iterations for parallel execution,
% since parallel execution actually has several forms of overhead:
That is why our system supports \emph{throttling}.
If a conjunction being parallelised contains a recursive call,
then the compiler can be asked to replace the original sequential conjunction
not with the parallel form of the conjunction,
but with an if-then-else.
The condition of this if-then-else
will test at runtime
whether spawning off a new job is a good idea or not.
If it is, we execute the parallelised conjunction, but
if it is not, we execute the original sequential conjunction.
The condition is obviously a heuristic.
If the heuristic allows the list of runnable jobs to become empty,
then we will not have any work to give to a CPU
that finishes its task and becomes available.
On the other hand,
if the heuristic allows the list of runnable jobs to become too long,
then we incur the overheads of spawning off some jobs unnecessarily.
Currently, on machines with $N$ CPUs,
we prefer to have a total of $M$ running and runnable jobs where $M > N$,
so our heuristic stops spawning attempts
if and only if the queue already has $M$ entries.
Our current system by default sets $M$ to be $32$ for $N = 4$,
though users can easily override this.

% The overheads of parallel execution can also affect conjunctions
% that do not contain recursive calls:
% a conjunction that looks worth parallelising if you ignore overheads
% may look not worth parallelising if you take them into account.
% This is why our system actually uses
% a version of algorithm~\ref{alg:dep_par_conj_overlap_middle}
% that accounts for overheads.

% Algorithm~\ref{alg:dep_par_conj_overlap_complete}
% can also handle $n$-way conjunctions for $n>2$.
% Since the Mercury mode system reorders conjunctions
% to ensure that data flows only to the right,
% in a two-conjunct conjunction,
% the left conjunct can only produce
% the variables it shares with the right conjunct
% and the right conjunct can only consume
% those variables.
% However, in longer conjunctions,
% the conjuncts in the middle
% can both consume variables produced by conjuncts on their left
% and produce variables consumed by conjuncts on their right.
% This is why our algorithm associates with \code{Conj\_i}, the $i$th conjunct,
% \code{ProdConsList\_i}:
% the list of shared variables that \code{Conj\_i} either produces or consumes,
% together with their times of production and first consumption respectively.
% This is a generalisation of \code{ConsList\_q} in
% algorithm~\ref{alg:dep_par_conj_overlap_simple}.
% We also need to generalise \code{Prod\_pi},
% because the time at which a non-first conjunct produces a variable
% can and usually will be affected
% by the overheads and/or synchronisation delays suffered by that conjunct.
% This is why we use \code{ProdTimeMap},
% which maps each shared variable to its time of production.

% The main body of the algorithm consists of two nested loops.
% The outer loop loops over all the conjuncts from left to right,
% because the execution of a conjunct can be affected
% by the conjuncts to its left
% (through the time at which they produce the shared variables it consumes),
% but not by the conjuncts to its right.
% The first few lines of the outer loop body
% (the first two assignments to \code{CurParTime})
% compute for each conjunct
% the time at which that conjunct can start execution.

% The inner loop loops over all the components of the current conjunct,
% such as \code{pA} and \code{pR}
% from figure~\ref{fig:dep_conj_overlap1}.
% Just as our previous algorithm did,
% this loop updates the simulated current time
% in both the original sequential execution of the conjunct (\code{CurSeqTime})
% and in its modified parallelised execution (\code{CurParTime}).
% For time components that end in the consumption of a variable,
% we do what we did before,
% but also reflect the cost of the wait operation needed for the consumption.
% For time components that end in the production of a variable,
% we record the time at which
% that variable would be available in the parallel execution;
% this will be when the producer finishes executing the signal operation on it.

% We use \code{TotalParTime} to keep track of the ending time
% of the parallel conjunct that ends last.
% We also remember, in \code{FirstConjTime},
% the time at which the first conjunct finishes.
% The reason we do this is because
% our runtime system requires that
% when the parallel conjunction finishes,
% execution must continue in the context
% that entered the parallel conjunction in the first place.
% In our implementation, this context will execute the first conjunct.
% If the last conjunct to finish is the first conjunct,
% it can continue on without delay;
% if the last conjunct to finish is some other conjunct,
% then we need to free its context,
% and switch to executing the original context,
% which became idle when the first conjunct finished.
% The last two lines reflect this cost.

% XXX end of iclp }

\begin{algorithm}
\begin{verbatim}
CurSeqTime_q := 0
CurParTime_q := 0
sort ConsList_q on ConsTime_qi
forall (Var_i, ConsTime_qi) in ConsList_q:
    Duration_qi := ConsTime_qi - CurSeqTime_q
    CurSeqTime_q := CurSeqTime_q + Duration_qi
    ParWantTime_qi := CurParTime_q + Duration_qi
    CurParTime_q := max(ParWantTime_q, Prod_pi)
DurationRest_q := SeqTime_q - CurSeqTime_q
SeqTime_q := CurSeqTime_q + DurationRest_q
ParTime_q := CurParTime_q + DurationRest_q
\end{verbatim}
\caption{Dependent parallel conjunction overlap calculation}
\label{alg:dep_par_conj_overlap_simple}
\end{algorithm}

Algorithm~\ref{alg:dep_par_conj_overlap_simple} shows
a simple version of the algorithm we use to compute ${ParTime}_q$.
Its main input is ${ConsList}_q$,
a list of the variables shared by $p$ and $q$,
together with their times of consumption by $q$.

The main task of the algorithm is to divide up ${SeqTime}_q$ into chunks,
and keep track of when those chunks can execute.
There is one chunk (${Duration}_{qi})$ for each shared variable
that ends at the time at which that variable is first consumed,
and there is one chunk ${DurationRest}$
after the consumption of the last shared variable.
(Figure~\ref{fig:dep_conj_overlap1}
shows the former as ${qA}$ and the latter as ${qR}$.)
The algorithm keeps track of the sequential and parallel execution times of $q$
up to the consumption of the current shared variable.
In the sequential version,
each chunk can execute immediately after the previous chunk,
since the values of the shared variables are all available when $q$ starts.
In the parallel version,
$p$ is producing the shared variables while $q$ is running.
If $p$ has produced the value of ${SV}_i$ by the time $q$ needs it,
there $q$ does not need to wait for it;
the left side of figure~\ref{fig:dep_conj_overlap1} shows this case.
However, it is also possible that $p$ will produce ${SV}_i$
only after the time at which $q$ would like to use it.
In that case, $q$ will suspend on ${SV}_i$,
and will resume only when $p$ signals that it is available;
the right side of figure~\ref{fig:dep_conj_overlap1} shows this case.

On both sides figure~\ref{fig:dep_conj_overlap1}
${SeqTime}_p = 5$ and ${SeqTime}_q = 4$.
On the left side, ${ConsTime}_{qA} = 2$,
and therefore ${Duration}_{qA} = 2$ and ${DurationRest}_{qA} = 2$,
Since ${ProdTime}_{pA} = 1$,
the first update of ${CurParTime}_q$ sets it to $2$,
and the second sets it to $4$, so ${ParTime}_q = 4$.
On the right side, ${ConsTime}_{qA} = 1$,
and therefore ${Duration}_{qA} = 1$ and ${DurationRest}_{qA} = 3$,
Since ${ProdTime}_{pA} = 4$,
the first update of ${CurParTime}_q$ sets it to ${max}(1, 4) = 4$,
and the second sets it to $4+3 = 7$, so ${ParTime}_q$ is 7.

\begin{table}
\begin{center}
\begin{tabular}{l|rr}
 & \multicolumn{1}{|c}{Cost}
 & \multicolumn{1}{|c}{Local use of \code{Acc1}} \\
\hline
\code{M}  &   1,625,050 & none \\
\code{F}  &           3 & ${Prod}_{Acc1}$ =         3 \\
\mapfoldl &   1,625,054 & ${Cons}_{Acc1}$ = 1,625,051 \\
% Note: The cost of the recursive call assumes that there is one
% recursive case and one base case remaining in the recursion.
\end{tabular}
\end{center}
\caption{Rounded profiling data for \mapfoldl}
\label{tab:prof_data_map_foldl}
\end{table}

\begin{figure}[tb]
\begin{verbatim}
map_foldl_par(_, _, [], Acc, Acc).
map_foldl_par(M, F, [X | Xs], Acc0, Acc) :-
    (
        M(X, Y),
        F(Y, Acc0, Acc1)
    ) &
    map_foldl_par(M, Xs, Acc1, Acc).
\end{verbatim}
\caption{Parallel \mapfoldl}
% the recursive call is less dependent
% on the conjunction of the first two calls.
\label{fig:map_foldl_par}
\end{figure}

To see how the algorithm works on realistic data,
consider the \mapfoldl example in figure~\ref{fig:map_foldl}.
Table \ref{tab:prof_data_map_foldl} gives
the approximate costs of the calls in the recursive clause of \mapfoldl
when used in a Mandelbrot image generator.
Each call to $M$ draws a row,
while $F$ appends the new row
onto the list of the rows already drawn.
The table also shows when $F$ produces ${Acc1}$
and when the recursive call consumes ${Acc1}$.
The costs were collected from a real execution using Mercury's deep profiler
and then rounded to make mental arithmetic easier.

Figure~\ref{fig:map_foldl_par} shows the best parallelisation of
\mapfoldl.
When evaluating the speedup for this parallelisation,
${Cons}_{{F} {Acc1}} = 1,625,000 + 2 = 1,625,002$, and
${Cons}_{{map\_foldl} {Acc1}} = 1,625,000 + 2 + 1,620,000 = 3,245,002$.
\zoltan{Show the rest of the algorithm's execution when this question is answered.}
\paul{I would have fixed this but I do not know what these formulas mean,
Maybe I am confused by notation}

% \begin{figure}[tb]
% \begin{verbatim}
% quicksort([], Acc, Acc).
% quicksort([Pivot | Xs], Acc0, Acc) :-
%     partition(Pivot, Xs, Lows, Highs),
%     quicksort(Lows, Acc0, Acc1),
%     quicksort(Highs, [Pivot | Acc1], Acc).
% \end{verbatim}
% \caption{Accumulator quicksort \peter{This is not referenced anywhere; drop it?}}
% % the two recursive calls are highly dependent.
% \label{fig:quicksort_acc}
% \end{figure}
%
% \zoltan{Show real data, if we can,
% that shows that quicksort has very little overlap.}

The speedup computed by algorithm~\ref{alg:dep_par_conj_overlap_simple}
applies only when the recursive call
calls the original sequential version of the predicate.
When the recursive call calls the parallelised version,
the maximum speedup available (assuming an unlimited number of CPUs)
depends on the structure of the recursion.
The profiling data gives us \tr{$E$,}
the number of entry calls to the procedure from the higher clique,
and \tr{$R_i$,} the number of recursive calls at each recursive call site.
From these, and the structure of the procedure's code,
we can calculate the average depth of recursion in most cases.

For singly recursive predicates like \mapfoldl,
there is only one recursive call site,
and the depth of recursion is simply $R_1/E$.
For example, if $E = 2$ and $R_1 = 20$,
then the average call sequence to the procedure
has one entry call followed by the recursive calls.
It also means that the average call sequence
has ten calls that cause the procedure to execute its recursive clause,
the clause containing the conjunction being parallelised,
followed by one call that executes the base clause.
From this, we can deduce that if ${SeqSaving} = {SeqTime} - {ParTime}$
is the time saving we get from parallelising the top conjunction
if the recursive call calls the original sequential version of the procedure,
then making the recursive call call the parallelised version of the procedure
would yield a time saving of ${ParSaving} = {SeqSaving} * R_1/E$
if we have enough CPUs to execute all the $R_1/E$ iterations in parallel.
\peter{I think this needs more explanation.  It does not look right to me.
I would expect it to be ${ParSaving} = {SeqTime} - {ParTime} * R_1/E$.}
This assumes that we get the same savings at each iteration,
\peter{I find this sentence weakens the claim, rather than strengthing it.
Can we just add ``, assuming we get the same savings at each iteration'' at
the end of the previous sentence?}
but this is reasonable,
since what we are doing is essentially executing
the different iterations of a loop in parallel,
and we have no reason to believe that the savings
from executing iteration $k$ in parallel with iteration $k+1$
would vary systematically based on the value of $k$.

Some singly recursive predicates have more than one recursive clause,
each with one recursive call site.
Suppose there are $n$ call sites, with execution counts $R_1 \ldots R_n$.
The overall time saving from parallelising the conjunct
that contains the call site associated $R_i$
has to be multiplied by the fraction of recursive calls
that execute the conjunction being parallelised:
${ParSaving} = {SeqSaving} * R_i/E * R_i/\sum_{j=1}^n R_j$.

For doubly recursive predicates like \code{quicksort}, $R_1 = R_2$,
and just under half of their calls invoke the recursive clause,
so for them, ${ParSaving} = {SeqSaving} * R_1/(2 * E)$.
\zoltan{check the math}
\peter{I think it is a bit confusing to use $R_1$ and $R_2$ to name the
recursive calls to quicksort, when above they have named the sole recursive
call from different clauses.  Maybe use $R^i_j$ to indicate the $i^{th}$
recursive call from the $j^{th}$ clause.  Then you can use $R^1_i$ in the
previous paragraph, and $R_1^1$ and $R_1^2$ in this one.}

All these calculations show that the available parallelism
can be greater than the number of CPUs.
If the machine has e.g.\ four CPUs,
then we do not actually want to spawn off
hundreds of iterations for parallel execution,
since parallel execution actually has several forms of overhead:

\begin{description}
\item[SparkCost]
is the cost of creating a spark and adding it to the local spark stack.
In a parallel conjunction,
every conjunct that is not the last conjunct incurs this cost
to create the spark for the rest of the conjunction.
\item[SparkDelay]
is the estimated length of time between the creation of a spark
and the beginning of its execution on another engine.
Every parallel conjunct that is not the first incurs this delay
before it starts running.
\item[SignalCost]
is the cost of signalling a future.
\item[WaitCost]
is the cost of waiting on a future.
\item[ContextWakeupDelay]
is the estimated time that it takes for a context to resume execution
after being placed on the runnable queue,
assuming that the queue is empty and there is an idle engine.
\item[BarrierCost]
is the cost of executing the operation
that synchronises all the conjuncts at the barrier
at the end of the conjunction.
\end{description}

Because of these overheads, our system uses \emph{throttling}.
If a conjunction being parallelised contains a recursive call,
then the compiler will replace the original sequential conjunction
not with the parallel form of the conjunction,
but with an if-then-else.
The condition of this if-then-else
will test at runtime
whether spawning off a new job is a good idea or not.
If it is, we execute the parallelised conjunction,
if it is not, we execute the original sequential conjunction.
The condition is obviously a heuristic.
If the heuristic allows the list of runnable jobs to become empty,
then we will not have any work to give to a CPU
that finishes its task and becomes available.
On the other hand,
if the heuristic allows the list of runnable jobs to become too long,
then we incur the overheads of spawning off some jobs unnecessarily.
Currently, on machines with $N$ CPUs,
we prefer to have a total of $M$ running and runnable jobs where $M > N$,
so our heuristic stops spawning attempts
if and only if the queue already has $M$ entries.
Our current system by default sets $M$ to be $32$ for $N = 4$,
though users can easily override this.

The overheads of parallel execution can also affect conjunctions
that do not contain recursive calls:
a conjunction that looks worth parallelising if you ignore overheads
may look not worth parallelising if you take them into account.
This is why our system actually uses
algorithm~\ref{alg:dep_par_conj_overlap_complete},
a version of algorithm~\ref{alg:dep_par_conj_overlap_simple}
that accounts for overheads.

\begin{algorithm}
\begin{verbatim}
find_par_time(Conjs) returns TotalParTime:
N := length(Conjs)
ProdTimeMap := empty
FirstConjTime := 0
TotalParTime  := 0
for i in 1 to N:
  CurSeqTime := 0
  CurParTime := (SparkCost + SparkDelay) * (i-1)
  if i != N:
    CurParTime := CurParTime + SparkCost
  sort ProdConsList_i on Time_ij
  forall (Var_ij, Time_ij) in ProdConsList_i:
    Duration_ij := Time_ij - CurSeqTime
    CurSeqTime := CurSeqTime + Duration_ij
    if Conj_i produces Var_ij:
      CurParTime := CurParTime + Duration_ij + SignalCost
      ProdTimeMap[Var_ij] := CurParTime
    else Conj_i must consume Var_ij:
      ParWantTime := CurParTime + Duration_ij
      CurParTime := max(ParWantTime, ProdTimeMap[Var]) + WaitCost
      if ParWantTime < ProdTimeMap[Var_ij]:
        CurParTime := CurParTime + ContextWakeupDelay
  DurationRest := SeqTime_i - CurSeqTime
  CurParTime := CurParTime + DurationRest + BarrierCost
  if i == 1:
    FirstConjTime = CurParTime
  TotalParTime := max(TotalParTime, CurParTime)
if TotalParTime > FirstConjTime:
  TotalParTime := TotalParTime + ContextWakeupDelay
\end{verbatim}
\caption{Dependent parallel conjunction complete algorithm}
\label{alg:dep_par_conj_overlap_complete}
\end{algorithm}

Algorithm~\ref{alg:dep_par_conj_overlap_complete}
can also handle $n$-way conjunctions for $n>2$.
Since the Mercury mode system reorders conjunctions
to ensure that data flows only to the right,
in a two-conjunct conjunction,
the left conjunct can only produce
the variables it shares with the right conjunct
and the right conjunct can only consume
those variables.
However, in longer conjunctions,
the conjuncts in the middle
can both consume variables produced by conjuncts on their left
and produce variables consumed by conjuncts on their right.
This is why our algorithm associates with \code{Conj\_i}, the $i$th conjunct,
\code{ProdConsList\_i}:
the list of shared variables that \code{Conj\_i} either produces or consumes,
together with their times of production and first consumption respectively.
This is a generalisation of \code{ConsList\_q} in
algorithm~\ref{alg:dep_par_conj_overlap_simple}.
We also need to generalise \code{Prod\_pi},
because the time at which a non-first conjunct produces a variable
can and usually will be affected
by the overheads and/or synchronisation delays suffered by that conjunct.
This is why we use \code{ProdTimeMap},
which maps each shared variable to its time of production.

The main body of the algorithm consists of two nested loops.
The outer loop loops over all the conjuncts from left to right,
because the execution of a conjunct can be affected
by the conjuncts to its left
(through the time at which they produce the shared variables it consumes),
but not by the conjuncts to its right.
The first few lines of the outer loop body
(the first two assignments to \code{CurParTime})
compute for each conjunct
the time at which that conjunct can start execution.

The inner loop loops over all the components of the current conjunct,
such as \code{pA} and \code{pR}
from figure~\ref{fig:dep_conj_overlap1}.
Just as our previous algorithm did,
this loop updates the simulated current time
in both the original sequential execution of the conjunct (\code{CurSeqTime})
and in its modified parallelised execution (\code{CurParTime}).
For time components that end in the consumption of a variable,
we do what we did before,
but also reflect the cost of the wait operation needed for the consumption.
For time components that end in the production of a variable,
we record the time at which
that variable would be available in the parallel execution;
this will be when the producer finishes executing the signal operation on it.

We use \code{TotalParTime} to keep track of the ending time
of the parallel conjunct that ends last.
We also remember, in \code{FirstConjTime},
the time at which the first conjunct finishes.
The reason we do this is because
our runtime system requires that
when the parallel conjunction finishes,
execution must continue in the context
that entered the parallel conjunction in the first place.
In our implementation, this context will execute the first conjunct.
If the last conjunct to finish is the first conjunct,
it can continue on without delay;
if the last conjunct to finish is some other conjunct,
then we need to free its context,
and switch to executing the original context,
which became idle when the first conjunct finished.
The last two lines reflect this cost.

% Each conjunct's execution depends on
% when the variables it consumes are produced by other conjuncts.
% These must be conjuncts to its left,
% since the compiler reorders conjunctions as needed
% to ensure that data flows only from left to right.
% Therefore, we can calculate the execution time of
% $G_1 \& \ldots \& G_n$
% by computing the execution time
% first of $G_1$,
% then of $G_1 \& G_2$,
% then of $(G_1 \& G_2) \& G_3$,
% % then of $((G_1 \& G_2) \& G_3) \& G_4$,
% and so on.

% \zoltan{Discuss how the synchronisation at the end of the algorithm is
% similar to the synchronisation while waiting for another variable.}

\section{Choosing how to parallelise a conjunction}
\label{sec:howto}

A conjunction with $n > 2$ conjuncts
can be converted into several different parallel conjunctions.
Converting all the commas into ampersands
(e.g.\ \code{c1, c2, c3} into \code{c1 \& c2 \& c3})
yields the most parallelism.
Unfortunately, this will often be \emph{too} much parallelism,
because in practice many conjuncts are unifications
and arithmetic operations whose execution takes very few instructions.
Executing such conjuncts in their own threads
costs far more in overheads than they save by running in parallel.
Therefore in most cases,
we want to create parallel conjunctions with $k < n$ conjuncts,
each consisting of a contiguous sequence
of one or more of the original sequential conjuncts,
effectively partitioning the original conjuncts into groups.

\begin{figure}
\begin{center}
\begin{verbatim}
global NumEvals := 0
find_best_partition(InitPartition, InitTime, LaterConjs)
    returns <FinalTime, FinalPartitionSet>:
  switch on LaterConjs:
  when LaterConjs = []:
    return <InitTime, {InitPartition}>
  when LaterConjs = [Head | Tail]:
    Extend := all_but_last(InitPartition) ++ [last(InitPartition) ++ [Head]]
    AddNew := InitPartition ++ [Head]
    ExtendTime := find_par_time(Extend)
    AddNewTime := find_par_time(AddNew)
    NumEvals := NumEvals + 2
    if ExtendTime < AddNewTime:
      BestExtendSoln := find_best_partition(Extend, ExtendTime, Tail)
      let BestExtendSoln be <BextExTime, BestExPartSet>
      if NumEvals < PreferLinearEvals:
        BestAddNewSoln := find_best_partition(AddNew, AddNewTime, Tail)
        let BestAddNewSoln be <BestANTime, BestANPartSet>
        if BestExTime < BestANTime:
          return BestExtendSoln
        else if BestExTime = BestANTime:
          return <BextExTime, BestExPartSet union BestANPartSet>
        else:
          return BestAddNewSoln
      else:
        return BestExtendSoln
    else:
      <symmetric with the then case>
\end{verbatim}
\end{center}
\caption{Search for the best parallelisation}
\label{fig:best_par_search}
%\vspace{-2\baselineskip}
\end{figure}

For any conjunction to be worth parallelising,
it should contain two or more expensive goals.
Our main algorithm (figure \ref{fig:best_par_search} works on the list
of conjuncts
from the first expensive goal to the last.
This will be the middle of original conjunction,
with (possibly empty) lists of cheap goals before it and after it.
Our initial search assumes that
the set of conjuncts in the parallel conjunction we want to create
is exactly the set of conjuncts in the middle.
A post-processing step then removes that assumption.

% into \code{(c1 \& c2), c3},
% into \code{c1, (c2 \& c3)},
% into \code{c1 \& (c2, c3)},
% into \code{(c1, c2) \& c3}, or
% into \code{c1 \& c2 \& c3}.

% In the usual case where $n >> 2$,
% there will be a huge number ways to do this.
% Our parallelisation algorithm,
% algorithm~\ref{alg:best_par_search},
% therefore tries to find the partition
% that yields the lowest overall execution time.

% A middle sequence with $n > 2$ conjuncts
% can be converted into several different parallel conjunctions;
% for example, \code{c1, c2, c3} can be converted
% into \code{(c1 \& c2), c3},
% into \code{c1, (c2 \& c3)},
% into \code{c1 \& (c2, c3)},
% into \code{(c1, c2) \& c3}, or
% into \code{c1 \& c2 \& c3}.
% The first two do not make sense if \code{c1} and {c3} are expensive goals,
% so we consider only conjunctions in which all conjuncts in the middle sequence
% are part of one parallel conjunct or another.
% The last of these gives the finest grain parallelism.
% Unfortunately, this will often be \emph{too} fine-grained,
% because in practice many conjuncts are unifications
% or builtin operations such as arithmetic
% whose execution takes very few instructions.
% Executing such conjuncts in their own threads
% can cost far more in overheads than they save by running in parallel.
% Therefore in most cases,
% we want to create parallel conjunctions with $k < n$ conjuncts,
% each consisting of a contiguous sequence
% of one or more of the original sequential conjuncts,
% effectively partitioning the original conjuncts into groups.
% % In the usual case where $n >> 2$,
% % there will be a huge number ways to do this.
% % Our parallelisation algorithm,
% % algorithm~\ref{alg:best_par_search},
% % therefore tries to find the partition
% % that yields the lowest overall execution time.

% Therefore, it is best to break sequential conjunctions into a number of
% smaller sequential conjunctions that are conjuncts of a larger
% parallel conjunction, however there are a multiple possible ways to do
% this.

If the middle sequence has $n$ conjuncts,
then there are $n-1$ AND operations between them,
each of which can be either sequential or parallel.
There are then $2^{n-1}$ combinations,
all but one of which are parallelisations.
That is a large space to search for the \emph{best} parallelisation,
and it would be larger still if we allowed code reordering,
that is, parallel conjuncts consisting of
a \emph{non}contiguous sequence of the original conjuncts.
We explore this space with a search algorithm,
\code{find\-\_\-best\-\_\-par\-ti\-tion}, which
we invoke with the empty list as \code{InitPartition},
zero as \code{InitTime}, and the list of middle conjuncts as \code{LaterConj}.
\code{InitPartition} expresses a partition of an initial sequence
of the middle goals into parallel conjuncts
whose estimated execution time is \code{InitTime},
and considers whether it is better to add the next middle goal
to the last existing parallel conjunct (\code{Extend}),
or to put it into a new parallel conjunct (\code{AddNew}).
It explores extensions of the better of the resulting partitions first.
If the search is still under the limit on the number of evaluations,
it explores the worse partition as well,
which is an exponential search.
When it hits the limit, 
it switches to a linear search;
we explore the more promising partition first
to make this search more effective.
(This limit ensures that the algorithm runs in reasonable time.)
The algorithm returns a set of equal best parallelisations so far,
``best'' being measured by
\iclp{a version of the algorithm in figure~\ref{fig:dep_par_conj_overlap_middle} that
computes the estimated parallel execution time \emph{including} overheads.}
\tr{algorithm \ref{alg:dep_par_conj_overlap_complete},
that is, the estimated parallel execution time including overheads.}

There are some simple ways to improve this algorithm.
%\vspace{-2mm}
\begin{itemize}
\item
Most invocations of \verb|find_par_time| specify a partition
that is an extension of a partition processed in the recent past.
In such cases, \verb|find_part_time| should do its task
incrementally, not from scratch.
\item
If the expected execution time
for the candidate partition currently being considered
is already greater than the fastest existing complete partition,
we can stop exploring that branch;
it cannot lead to a better solution.
\tr{
(This is the idea of branch-and-bound algorithms.)
}
\item
Sometimes consecutive conjuncts do things that are
obviously a bad idea to do in parallel, such as building a ground term.
The algorithm should treat these as a single conjunct.
% XXX we could make the third item tr only if we need space
\tr{
\item
graph of dependencies
\item
take total CPU utilisation into account,
at least by using it to break ties on overall CPU time
}
\end{itemize}
%\vspace{-2mm}

\noindent
At the completion of the search,
we select one of the equal best parallelisations,
and post-process it to adjust both edges.
Suppose the best parallel form of the middle goals is $P_1~\&~\ldots~\&~P_p$,
where each $P_i$ is a sequential conjunction.
We compare the execution time of $P_1~\&~\ldots~\&~P_p$
with that of $P_1,~(P_2~\&~\ldots~\&~P_p)$.
If the former is slower,
which can happen if $P_1$ produces its outputs at its very end
and the other $P_i$ consume those outputs at their start,
then we conceptually move $P_1$ out of the parallel conjunction
(from the ``middle'' part of the conjunction to the ``before'' part).
We keep doing this for $P_2$, $P_3$ etc until either
we find a goal worth keeping in the parallel conjunction,
or we run out of conjuncts.
We also do the same thing at the other end of the middle part.
This process can shrink the middle part.

In cases where we do not shrink an edge, we can consider expanding that edge.
Normally, we want to keep cheap goals out of parallel conjunctions,
since more conjuncts tends to mean
more shared variables and thus more synchronisation overhead,
but sometimes this consideration is overruled by others.
Suppose the goals before the conjuncts in $P_1~\&~\ldots~\&~P_p$
in the original conjunction were $B_1,~\ldots,~B_b$
and the goals after it $A_1,~\ldots,~A_a$,
and consider $A_1$ after $P_p$.
If $P_p$ finishes before the other parallel conjuncts,
then executing $A_1$ just after $P_p$ in $P_p$'s context
may be effectively free:
the last context could still arrive at the barrier at the same time,
but this way, $A_1$ would have been done by then.
Now consider $B_b$ before $P_1$.
If $P_1$ finishes before the other parallel conjuncts,
\emph{and} if none of the other conjuncts wait for variables produced by $P_1$,
then executing $B_b$ in the same context as $P_1$ can be similarly free.

We loop from $i=b$ down towards $i=1$, and check whether
including $B_i,~\ldots,~B_b$ at the start of $P_1$ is improvement.
If not, we stop; if it is, we keep going.
We do the same from the other end.
% If we end up with moving
% The second search loops from $j=1$ up towards $j=a$
% and checks whether including $A_1, \ldots, A_j$ at the end of $P_p$
% is improvement.
% Each loop stops when the answer becomes ``no'',
The stopping points of the loops of the contraction and expansion phases
dictate our preferred parallel form of the conjunction, which
(if we shrunk the middle at the left edge and expanded it at the right)
will look something like
$B_1,$ $\ldots,$ $B_{b},$ $P_1,$ $\ldots~P_k,$
$(P_{k+1}$ $\&$ $\ldots$ $\&$ $P_{p-1}$ $\&$ $(P_p,$ $A_1,$ $\ldots,$ $A_j)),
A_{j+1},$ $\ldots,$ $A_a$.
% $B_1, \ldots, B_{i-1},
% ((B_i, \ldots, B_b, P_1) \& P_2, \ldots \& P_{p-1} \& (P_p, A_1, \ldots, A_j)),
% A_{j+1}, \ldots, A_a$.
If this preferred parallelisation is better than
the original sequential version of the conjunction by at least 1% (a configurable threshold),
then we include a recommendation for its conversion to this form
in the feedback file we create for the compiler.

% These two loops are specifically designed
% to allow the inclusion of cheap goals in the parallel conjunction.
% Note that this algorithm always tries to arrange
% \emph{all} the conjuncts in the conjunction,
% not just the conjuncts from the first costly goal to the last.
% Normally, we want to keep cheap goals out of parallel conjunctions,
% since more conjuncts usually means more shared variables,
% which means more synchronisation overhead.
% The reason why we expanding the scope of the parallel conjunction
% is that sometimes this consideration is overruled by others.
% Consider $A_1$ after $P_p$.
% If $P_p$ finishes before the other parallel conjuncts,
% then executing $A_1$ just after it in $P_p$'s context may be effectively free:
% the last context could still arrive at the barrier at the same time,
% but this way, $A_1$ would have completed by then.
% Now consider $B_b$ before $P_1$ where $P_1$ is still in a parallel conjunct.
% If $P_1$ finishes before the other parallel conjuncts,
% \emph{and} if none of the other conjuncts
% wait for variables produced by $P_1$,
% then executing $B_b$ in the same context as $P_1$ can be similarly free.

% \begin{itemize}
% \item
% Currently no tie breaking is done and we have not explored
% using other formulas for the search's objective function.
% % \item
% % Also, the current implementation does not make an estimate of the
% % minimum cost of the work that could be scheduled after the current point.
% % This affects the amount of pruning that the branch and bound code
% % is able to achieve.
% \end{itemize}

% When explaining the algorithm, tell readers to first assume that
% GoalsBefore and GoalsAfter are the empty list,
% i.e. MaxBefore = 0 and MinAfter = N+1.
% Only after explaining the algorithm in that case

% Consider changing the loop structure so that instead of
% loop on Before;
%     loop on After,
%         loop on Arrangement,
% it is
% loop on Arrangement,
%     loop on Before (assuming a given After) find the best and commit to it,
%     loop on After, find the best and commit to it.

\section{Pragmatic issues}
\label{sec:pragmatic}

% \subsection{The effects of module boundaries}
% \label{sec:pragmamoduleboundary}

% pushing waits and signals into calles stops at module boundaries

% \subsection{Cliques vs procedures}
% \label{sec:pragmacliqueproc}

\emph{Dynamic context}
The algorithms in sections~\ref{sec:overlap} and~\ref{sec:howto}
work on profiling data that shows the behaviour of a procedure
in the context given by a particular chain of ancestors.
Many procedures are of course called from multiple ancestor contexts.
What happens when our analysis of the behaviour of the same procedure
yields different results for different ancestor contexts?

At the moment, for any procedure
that our analysis indicates is worth parallelising in any context,
we pick one particular parallelisation (usually there is only one anyway),
and transform the procedure accordingly.
This gets the benefit of parallelisation when it is worthwhile,
but incurs its costs even in contexts when it is not.
In the future, we plan to fix this using multi-version specialisation.
For every procedure with different parallelisation recommendations,
we intend to create a specialised version for each recommendation,
leaving the original sequential version.
This will of course require the creation of specialised versions
of its parent, grandparent etc procedures,
until we get to an ancestor procedure
which occurs in the common prefix of all the conflicting ancestor contexts.

% \subsection{Searching for parallelism opportunities}
% \label{sec:pragmabestfirst}
%
% Our candidates list actually contains
% both cliques and conjunctions within cliques.
%
% The candidates list should contain cliques, since
% \begin{itemize}
% \item
% the entry points of some child cliques are not in conjunctions
% (e.g.\ they can be switch arms), and
% \item
% we want to delay breaking a clique down into its constituent conjunctions,
% since this way if our traversal stops before getting to a clique,
% then we never have to break it down.
% \end{itemize}
%
% The candidates list should also contain conjunctions,
% since a clique can contain both cheap and expensive conjunctions,
% and we do not want to evaluate the cheap ones
% until we have processed all the more expensive conjunctions
% not just in this clique but in all other cliques;
% again, we expect that this way,
% our traversal will stop before it gets to the cheapest conjunctions.

% \subsection{Parallelising children vs ancestors}
% \label{sec:pragmachildancestor}

\emph{Parallelising children vs ancestors}
What happens when we decide that a conjunction that should be parallelised
has an ancestor that we decided should also be parallelised?
We can
(1) parallelise only the ancestor,
(2) parallelise only this conjunction, or
(3) parallelise both

% The first alternative (parallelise neither) has already been rejected twice,
% since we concluded that (2) was better (1)
% when we decided to parallelise the ancestor,
% and we concluded that (3) was better (1)
% when we decided to parallelise this conjunction.

\zoltan{Does the implementation actually do this now?}
We choose among the other three alternatives
by evaluating the speedup you get from each of them, and just pick the best.
This reevaluation must take into account
the fact that for each invocation of the ancestor conjunction,
we will invoke the current conjunction many times,
and that therefore we will incur both the overheads and the benefits
of parallelising the current conjunction many times.
The profile will give the actual number.

% \subsection{Disagreement among children}
% \label{sec:pragmachildchild}

% what if for some ``current'' clique,
% you want to parallelise the ancestor,
% but for some other current clique,
% you do not want to parallelise the same ancestor?

\emph{Parallelising branched goals}
Many programs have code that looks like this:
\begin{verbatim}
( if ... then
    ... expensive call 1 ...
else
    ... cheap goal ...
),
expensive call 2
\end{verbatim}
If the condition of the if-then-else succeeds only rarely,
then the average cost of the if-then-else
may be below the threshold of what we consider to be an expensive goal.
We therefore would not even consider
parallelising the top-level conjunction,
rightly considering that its overheads would probably outweigh its benefits.

What we want to do in such cases
is execute just the two expensive calls in parallel,
which would be equivalent to parallelising the conjunction
in the then part of this transformed goal:
\begin{verbatim}
( if ... then
    ... expensive call 1 ...
    expensive call 2
else
    ... cheap goal ...
    expensive call 2
)
\end{verbatim}
We intend to change our feedback tool to detect such situations,
and if found, to recommend
some equivalence-preserving transformations for the compiler to apply
before parallelising some of the resulting conjunctions.

% \subsection{Garbage collector issues}
% \label{sec:pragmagc}

\emph{Garbage collector issues}
The Mercury implementation uses the Boehm-Demers-Weiser
conservative collector for C \zoltan{add cite} to manage memory.
This system has worse overheads
for parallel programs than for sequential programs.
First, even though this collector uses
a separate memory pool for each mutator thread
(and hence, in our system, for each Mercury engine),
you still need synchronisation to access the global pool
when a local pool runs out.
Second, this collector
does not support incremental collection for parallel programs,
and a full collection stops all threads,
and thrashes the caches of their CPUs.
We therefore ran our benchmarks with the collector tuned
to use large local pool sizes
and to grow the size of the global pool more quickly than usual.
These settings significantly improved
the performance of the sequential programs as well.

% GC In our case, the worry is that
% GC the Boehm collector may scale significantly worse than
% GC the Mercury code that we choose to parallelise.

% GC From the perspective of a GC implementor,
% GC a program's runtime has two components:
% GC the execution time of the main part of the program (the \emph{mutator}),
% GC and the execution time of the collector itself.
% GC When the mutator is a Mercury program,
% GC there is a fixed (and usually small) limit
% GC on the number of instructions it can execute
% GC between increments of the call sequence count
% GC (though the limit is program-dependent).
% GC There is no such limit on the collector.
% GC This is can be a problem.
% GC Since a construction unification does not involve a call,
% GC our profiler considers its CSC cost to be zero.
% GC \peter{Haven't defined ``CSC \emph{cost}''.}
% GC Yet if the memory allocation required by a construction
% GC triggers a collection,
% GC then this nominally zero-cost action can actually take as much time
% GC as many calls in the mutator.
% GC
% GC The normal way to view the time taken by gc
% GC is to simply distribute it among the allocations,
% GC so that one CSC represents the average time taken
% GC by the mutator between two calls
% GC plus the average amortized cost of the collections
% GC triggered by the unifications between those calls.
% GC For a sequential program, this view works very well.
% GC For a parallel program, it works less well,
% GC because the performance of the mutator and the collector may scale differently.
% GC In our case, the worry is that
% GC the Boehm collector may scale significantly worse than
% GC the Mercury code that we choose to parallelise.
% GC
% GC For Mercury code,
% GC the limits on speedups from parallelism fall into two categories:
% GC those that our analysis takes into account, and those it does not.
% GC The former include the cost of spawning new contexts,
% GC the cost of the barrier synchronisation at the end of the conjunction,
% GC the cost of creating the synchronisation terms,
% GC the cost of the wait and signal operations on those terms,
% GC and idle time of the consumer while waiting on the producer.
% GC The latter include interference

% lock at allocation vs stop the world at collection

% memory
% heat
%
% is to simply consider the amortized cost
% In sequential programs,
% one can consider that the cost of the
%
% In sequential programs,
% it is not really feasible to separate
% the time cost of the collector from the cost of the mutator.
% On
%
% When parallelising the program, there is unfortunately
% a natural way to separate
%
% and CSCs can be considered to measure both parts of the program.
%
% CSCs not a true representation of time
%
% take heap allocation intensity (allocations per CSC) into account
% in the algorithms above:
% consider two high-intensity goals being executed in parallel
% to be another source of overhead.
%
% convert allocs to CSCs, add them to both sequential and parallel times
%
% involve the alloc ratios somehow?
%
% There is also the consideration that with Boehm gc,
% a collection stops the world,
% and the overheads of this stopping scale with the number of CPUs being used.
% The overheads of stopping include
% not just the direct costs of the interruption,
% but also indirect costs,
% such as having to refill the cache after the collector trashes it.

\section{Performance results}
\label{sec:perf}

% \begin{table*}
% \begin{center}
% \begin{tabular}{l|rrrrrrrrrr}
%  ~ & \multicolumn{1}{|c|}{Seq RT} &
%   \multicolumn{1}{|c|}{Par RT} &
%   \multicolumn{2}{|c|}{No Deps} &
%   \multicolumn{2}{|c|}{Na\"ive} &
%   \multicolumn{2}{|c|}{Num Vars} &
%   \multicolumn{2}{|c}{Overlap} \\
% \multicolumn{1}{c|}{Program} & \multicolumn{1}{|c|}{345Time} &
%   \multicolumn{1}{|c|}{Time} &
%   \multicolumn{1}{|c|}{Conjs} & \multicolumn{1}{|c|}{Time} &
%   \multicolumn{1}{|c|}{Conjs} & \multicolumn{1}{|c|}{Time} &
%   \multicolumn{1}{|c|}{Conjs} & \multicolumn{1}{|c|}{Time} &
%   \multicolumn{1}{|c|}{Conjs} & \multicolumn{1}{|c}{Time} \\ \hline
% quicksort acc &   &   & 0 &   & 1 &   &   &   & 0 &   \\
% quicksort app &   &   & 1 &   & 1 &   &   &   & 1 &   \\
% fibs & 1 & a & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
% icfp2000 & 1 & a & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
% mandelbrot & 1 & a & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
% mmc & 1 & a & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9
% \end{tabular}
% \end{center}
% \caption{Results}
% \label{tab:results_temp}
% \end{table*}

% Report analysis times as well as sequential and parallel execution times
% and CPU usage (as integral if possible, as well as peack CPU usage).

We tested our system on three benchmark programs:
matrix multiplication, a mandelbrot image generator and a raytracer.
Matrixmult has abundant independent AND-parallelism.
Mandelbrot uses the actual \code{map\_foldl} predicate
from figure~\ref{fig:map_foldl}
to iterate over rows of pixels.
Raytracer does not use \code{map\_foldl},
but does use a similar code structure to perform a similar task.
This is not an accident:
\emph{many} predicates use this kind of code structure,
partly because programmers in declarative languages
often use accumulators to make their loops tail recursive.
% All three programs do lots of floating point arithmetic,
% and the mandelbrot program does a lot of integer arithmetic as well.
% The Mercury backend we are using always boxes floating point numbers,
% so each floating point operation requires the creation of a new cell on the heap.
% This makes matrixmult and raytracer very memory allocation intensive.
% Since the garbage collector accounts for a large fraction of their runtimes,
% Amdahl's law dictates the maximum speedup we can get by speeding up their mutators
% will be correspondingly limited.

We ran all three programs
with one set of input parameters to collect profiling data,
and with a \emph{different} set of input parameters to produce
the timing results in the following table.
All tests were run on
% taura
a Dell Optiplex 755 PC with a 2.4~GHz Intel Core 2 Quad Q6600 CPU
running Linux 2.6.31.
Each test was run ten times;
we discarded the highest and lowest times, and averaged the rest.

% \begin{table*}[h]
% \begin{center}
% \begin{tabular}{l||r|r|r|r|r|r}
% Program     & Seq   & No autopar   & 1 CPU        & 2 CPUs      & 3 CPUs      &
% 4 CPUs \\
% \hline
% mandelbrot  & 33.4  &  35.3 (0.95) &  35.4 (0.94) & 18.0 (1.85) & 12.2 (2.74) &
 % 9.4 (3.55) \\
% raytracer   & 12.33 & 14.01 (0.88) & 14.77 (0.83) & 9.40 (1.31) & 7.59 (1.62) &
% 6.70 (1.84) \\
% \end{tabular}
% \end{center}
% % \caption{Results}
% % \label{tab:results_temp}
% \end{table*}

% \vspace{-2mm}
% \begin{table*}[h]
% \begin{center}
% \begin{tabular}{|l|l||r|r|r|r|r|}
% %\hhline{|-|-||-|-|-|-|-|}
% \hline
% \multicolumn{1}{|c|}{\textbf{Program}} &
% \multicolumn{1}{ c||}{\textbf{Par}}    & 
% \multicolumn{1}{ c|}{\textbf{1 CPU}}   & 
% \multicolumn{1}{ c|}{\textbf{2 CPUs}}  & 
% \multicolumn{1}{ c|}{\textbf{3 CPUs}}  & 
% \multicolumn{1}{ c|}{\textbf{4 CPUs}}  \\
% %\hhline{|-|-||-|-|-|-|-|}
% \hline
% matrixmult & indep & 14.60 (0.75) &  7.55 (1.46) &  6.07 (1.81) &  5.21 (2.11) \\
% seq 11.00  & naive & 14.61 (0.75) &  7.53 (1.46) &  6.75 (1.63) &  5.17 (2.12) \\
% par 14.60  & overlap  & 14.59 (0.75) &  7.57 (1.45) &  5.26 (2.09) &  5.37 (2.05) \\
% %\hhline{|-|-||-|-|-|-|-|}
% \hline
% mandelbrot & indep & 35.27 (0.95) & 35.31 (0.95) & 35.15 (0.95) & 35.31 (0.95) \\
% seq 33.47  & naive & 35.33 (0.95) & 17.87 (1.87) & 12.07 (2.77) &  9.17 (3.65) \\
% par 35.27  & overlap  & 35.16 (0.95) & 17.91 (1.87) & 12.02 (2.78) &  9.15 (3.65) \\
% %\hhline{|-|-||-|-|-|-|-|}
% \hline
% raytracer  & indep & 11.33 (0.87) & 11.37 (0.87) & 11.36 (0.87) & 11.36 (0.87) \\
% seq  9.85  & naive & 11.20 (0.88) &  7.48 (1.32) &  5.91 (1.66) &  5.39 (1.83) \\
% par 11.29  & overlap  & 11.28 (0.87) &  7.56 (1.30) &  5.94 (1.66) &  5.38 (1.83) \\
% %\hhline{|-|-||-|-|-|-|-|}
% \hline
% \end{tabular}
% \end{center}
% % \caption{Results}
% % \label{tab:results_temp}
% \end{table*}
% \vspace{-2mm}

% \vspace{-2mm}
\begin{table}[tb]
\begin{center}
\begin{tabular}{llrrrrr}
\hline \hline
\multicolumn{1}{c}{\textbf{Program}} &
\multicolumn{1}{c}{\textbf{Par}}    & 
\multicolumn{1}{c}{\textbf{1 CPU}}   & 
\multicolumn{1}{c}{\textbf{2 CPUs}}  & 
\multicolumn{1}{c}{\textbf{3 CPUs}}  & 
\multicolumn{1}{c}{\textbf{4 CPUs}}  \\
\hline
%\hhline{|-|-||-|-|-|-|-|}
% \hline
% matrixmult & indep & 14.7 (0.76) &  7.6 (1.47) &  5.2 (2.15) &  5.2 (2.15) \\
% seq 11.2   & naive & 14.7 (0.76) &  8.0 (1.40) &  5.7 (1.96) &  4.7 (2.38) \\
% par 14.6   & overlap  & 14.7 (0.76) &  7.6 (1.47) &  6.7 (1.67) &  5.2 (2.15) \\
matrixmult & indep    & 14.6 (0.75) &  7.5 (1.47) &  7.0 (1.66) &  5.2 (2.12) \\
seq 11.0   & naive    & 14.6 (0.75) &  7.6 (1.45) &  5.2 (2.12) &  5.2 (2.12) \\
par 14.6   & overlap  & 14.6 (0.75) &  7.5 (1.47) &  6.2 (1.83) &  5.2 (2.12) \\
%\hhline{|-|-||-|-|-|-|-|}
\hline
mandelbrot & indep    & 35.2 (0.95) & 35.1 (0.95) & 35.2 (0.95) & 35.3 (0.95) \\
seq 33.4   & naive    & 35.4 (0.94) & 18.0 (1.86) & 12.1 (2.76) &  9.1 (3.67) \\
par 35.2   & overlap  & 35.6 (0.94) & 17.9 (1.87) & 12.1 (2.76) &  9.1 (3.67) \\
%\hhline{|-|-||-|-|-|-|-|}
\hline
raytracer  & indep    & 26.2 (0.87) & 26.3 (0.86) & 26.1 (0.87) & 26.2 (0.87) \\
seq 22.7   & naive    & 25.3 (0.90) & 16.0 (1.42) & 11.2 (2.03) &  9.4 (2.42) \\
par 26.5   & overlap  & 25.1 (0.90) & 16.0 (1.42) & 11.2 (2.03) &  9.4 (2.42) \\
%\hhline{|-|-||-|-|-|-|-|}
\hline \hline
\end{tabular}
\end{center}
%\vspace{-2\baselineskip}
\end{table}

Each group of three rows reports the results for one benchmark.
The first column shows the benchmark name,
the runtime of the program when compiled for sequential execution, and
its runtime when compiled for parallel execution
but without enabling auto-parallelisation.
This shows the overhead of support for parallel execution
when it does not buy any benefits.
We auto-parallelised each program three different ways:
executing expensive goals in parallel
only when they are independent (``indep'');
even if they are dependent, regardless of overlap (``naive'');  and
even if they are dependent, but only if they have good overlap (``overlap'').
The last four columns give the runtime in seconds
of each of these versions of the program
on 1, 2, 3 and 4 CPUs,
with speedups compared to the sequential version.

The parallel version of the Mercury system
needs to use a real machine register
to point to thread-specific data,
such as each engine's abstract machine registers.
On x86s, this leaves only one real register for the Mercury abstract machine,
so compiling for parallelism but not using it
yields a slowdown ranging from 5\% on mandelbrot to 25\% on matrixmult.
(We observe such slowdowns for other programs as well.)
On one CPU, autoparallelisation gets only this slowdown,
plus the (small) additional overheads of all the parallel conjunctions
that cannot get any parallelism.
% However, when we move to 2, 3 or 4 CPUs,
% some of the autoparallelised programs do get speedups.

The parallelism in the main predicate of matrixmult is independent,
Overlap parallelises the program the same way as indep,
so it gets the same speedup.
The numbers look different for 3 CPUs,
but all the runs for both versions actually took either 5.2 or 7.5 seconds,
depending (we think) on which way
the OS arranged the engines across the two CPU die of the Q6600;
the indep version just happened to get the 7.5s arrangement fewer times.
For naive, all the runs just happened to take 5.2 seconds,
even though naive creates a worse parallelisation than either indep or overlap:
during the expansion phase we described in section~\ref{sec:howto},
it includes an extra goal in the first of the parallel conjuncts;
this makes the conjunction dependent, which adds some overhead.
Naive also executes the code that does the matrix multiplication
in parallel with the goals that create its inputs,
which also adds overhead without speedup.
These overheads are too small to affect the results.

In mandelbrot and raytracer, all the parallelism is dependent,
which is why indep gets no speedup for them.
For mandelbrot, naive and overlap get speedups
that are as good as one can reasonably expect:
$35.2/9.1 = 3.87$ on four CPUs over the one CPU case.
% (Perfect speedups of 4.0 on 4 CPUs are not attainable in practice 
% due to bottlenecks such as CPU-memory buses and stop-the-world garbage
% collection.)
For matrixmult and raytracer, the speedups they get,
2.12 and 2.42 on four CPUs,
also turn out to be pretty good when one takes a closer look.

For matrixmult, the bottleneck is almost certainly CPU-memory bandwidth.
Each step in this program does only one multiply and one add (both integer)
before creating a new cell on the heap and filling it in.
On current CPUs, the arithmetic takes much less time than the memory writes,
and since the new cells are never accessed again, caches do not help,
which makes it easy to saturate the memory bus, even when using only three CPUs.

The raytracer is very memory-allocation-intensive,
because it does lots of FP arithmetic,
and the Mercury backend we are using always boxes floating point numbers,
so each floating point operation requires
the creation of a new cell on the heap.
Because of this, memory bandwidth may also be an issue for it,
but its bigger problem is GC;
while GC takes only about 5\% of the runtime when run on one CPU,
it takes almost 40\% of the runtime when run on four CPUs,
even though we used four marker threads.
(For fairness, we used four marker threads
regardless of how many CPUs the Mercury code used.)
Given this fact, the best speedup we can hope for is
$(4 \times 0.6 + 0.4)/(0.6 + 0.4) = 2.8$,
and we do come pretty close to that.

GC becomes more expensive with more CPUs
not only because of increased contention,
but also because the GC has more work to do:
with more contexts being spawned, there are more stacks for it to scan.
We have tested versions of the raytracer in which
each spawned-off goal computed the pixels for several rows, not just one,
and these versions yield speedups of about 3.3 on four CPUs.
These versions spawn many fewer contexts, thus putting much less load
on the GC.
This shows that
program transformations that cause more work to be done in each context
are likely to be a promising area for future work.
% We thus expect that applying throttling
% (as described in section~\ref{sec:overlap})
% should significantly improve these results.

Most small programs like these benchmarks
have only one loop that dominates their runtime.
In all three of these benchmarks, and in many others,
the naive and overlap methods will parallelise the same loops,
and usually the same way;
they tend to differ only in how they parallelise code
that executes much less often (typically only once)
whose effect is lost in the noise.
The raw timings show a great deal of variability:
we have seen two consecutive runs of the same program on the same data
differ in their runtime by as much as 15\%.
% (One possible cause of this is differences
% in whether the OS puts frequently-communicating engines
% on cores on the same die, or cores on two different dies.)
% As the table shows,
Some of this variability remains even after filtering and averaging.
% However, the raw times showed significant variability,
% and this process does not entirely eliminate that variability.

To see the difference between naive and overlap,
we need to look at larger programs.
Our standard large test program is the Mercury compiler, which contains
53 conjunctions with two or more expensive goals.
Of these, 52 are dependent,
and only 31 have an overlap
that leads to a predicted local speedup of more than 1\%,
our default threshold.
Our algorithms can thus prevent
the unproductive parallelisation of $53-31=22$ of these conjunctions.
Unfortunately, programs that are large and complex enough
to show a performance effect from this saving
also tend to have large components
that cannot be profitably parallelised with existing techniques,
which means that (due to Amdahl's law)
our autoparallelisation system cannot yield overall speedups for them yet.

On the bright side,
our feedback tool generates feedback files
in less than a second from the profiles of small programs like these benchmarks,
and in only a minute or two even from much larger profiles.
The extra time taken by the Mercury compiler
when it follows the recommendations in feedback files
is so small that it is not noticeable.

% Currently, the Mercury runtime system
% often continues execution, on completion of a parallel conjunction,
% on a CPU different from the one being used before that parallel conjunction.
% When our system finds a smattering of parallel conjunctions
% through a mostly sequential program,
% these switches from a CPU with a warm cache to a CPU with a cold cache
% severely degrade the program's performance.
% Right now, for most programs,
% this effect yields a slowdown significantly bigger
% than the speedups yielded by automatic parallelisation.
% Once this defect is fixed, we hope to report significantly better results
% for more and bigger programs.

% \footnote{
% For referees
% who read this paper together
% with the submission by Wang and Somogyi,
% the version of the raytracer used in that paper
% had manual granularity control;
% the version we are using in this paper is the original version of the program,
% which was not written for parallelism and has no manual granularity control.}

% \begin{itemize}
% \item
% ICFP2000 --- Raytracer
% \item
% Mandelbrot Image generator.
% \item
% Variations on the above two programs including varying degrees
% of dependence and a \mapfoldl version.
% \item
% ICFP2001 --- SGML optimizer
% \item
% Compiler --- We do not expect this to speed up.
% However it will be useful to ensure that it does not slow down too much.
% \item
% pic --- a NuFib benchmark by ported by Peter
% \item
% SWRL --- An inference engine provided by Mission Critical.
% % The mission critical benchmark is at:
% % 
% %     taura:/home/taura/pbone/mcdemo/swrl-snapshot.tar.gz
% % 
% % SWRL is an inference engine.  Building and running it is covered by:
% % 
% %     taura:/home/taura/pbone/mcdemo/swrl.txt
% % 
% % These files are owned by a new group, mcdemo, since they are MC's
% % closed source project.  Peter Ross is easy going and has let us use
% % them without any formal non-disclosure agreement.  That said, I am
% % respecting this IP as much as I would anything where I had signed a
% % formal NDA.
% \end{itemize}

% Two programs, a raytracer and a mandelbrot image generator showed
% strong speedups, see figure \ref{tab:results}.
% This confirms that our analysis is correctly identifying parallelism
% available within the main loops of these programs.
% The two programs have a similar structure, they both have a loop with
% an accumulator that contains the rows of the images already rendered.
% We believe that a lot of dependent parallelism has this form as
% programmers in declarative languages are trained to use accumulators
% in their loops to ensure that the loop is tail-recursive.
% The mandelbrot program's loop uses \mapfoldl example above.
% \paul{Should I include a back-reference for the mapfoldl figure?}
% 
% We used different inputs for the profiling and benchmarking
% executions to ensure that the profile analysis would not over-fit the
% parallelisation to a particular input.

% Other programs tested included the Mercury compiler and pic, a program
% ported to Mercury from the nofib~\cite{nofib} benchmark suite.
% Our analysis found exploitable parallelism within these benchmarks,
% however,
% it appears to be too fine-grained for Mercury's runtime to handle.
% We hope to correct these problems fix before the camera ready deadline.

% I will need to test a na\"ive approach, for instance: assuming maximum
% overlap, or factoring in some fixed cost for each shared variable.

% \section{Related work}
% \label{sec:related_work}

\section{Related work and conclusion}
\label{sec:conc}

% Mercury's strong mode system
% greatly simplifies the parallel execution of logic programs,
% making the comparison of parallel Mercury with parallel Prolog difficult.
% For example, \cite{Hermenegildo1995} defines non-strict
% goal independence such that goals that are non-strictly independent can be
% run in parallel without leading to incorrect results.
% Because Mercury
% statically determines a single goal in a conjunction to bind each variable,
% and because Mercury does not permit variables to be aliased,
% the conditions of non-strict goal independence
% are not necessary for Mercury to guarantee correctness.
% Similarly, other existing work on AND-parallelism in Prolog
% is not closely related to the present work,
% because Mercury sidesteps the
% problems that work seeks to overcome.
% \peter{Is that too hand-wavey and dismissive?}

Mercury's strong mode and determinism systems
greatly simplify the parallel execution of logic programs.
The information gathered by semantic analysis in Mercury
makes it easy to solve most of the problems faced by the
designers of parallel versions of Prolog and Prolog-like languages.
These include testing the independence of goals
in systems that support only independent AND-parallelism
and discovering producer-consumer relationships
in systems that also support dependent AND-parallelism,
such as \cite{DBLP:journals/tcs/GrasH09}.
They also make it possible to \emph{avoid} having to solve some tough problems,
the main example being how to execute nondeterministic conjuncts in parallel
without excessive overhead.

% That is what they were \emph{designed} to do.
% The information gathered by semantic analysis in Mercury
% Many problems in the parallel execution of Prolog and Prolog-like languages,
% like testing the independence of goals
% in systems that support only independent AND-parallelism,
% discovering producer-consumer relationships at runtime
% in systems that also support dependent AND-parallelism,
% and having to handle nondeterministic conjuncts,
% disappear completely,
% with the answers to the problem being presented on a silver platter
% Our group designed Mercury specifically to ensure this.

Most research in parallel logic programming so far
has focused on trying to solve these problems
of getting parallel execution to \emph{work} well,
with only a small fraction trying to find
when parallel execution would actually be \emph{worthwhile}.
Almost all previous work on automatic parallelisation 
has focused on granularity control:
parallelising only computations that are expensive enough
to make parallel execution
worthwhile \cite{harris_07_feedback_imp_par,lopez96:distance_granularity},
and properly accounting for the overheads
of parallelism itself \cite{shen_98_granularity-control}.
Most of the rest has tried to find opportunities
to exploit independent AND-parallelism
during the execution of otherwise-dependent conjunctions
\cite{DBLP:journals/jlp/MuthukumarBBH99,DBLP:conf/lopstr/CasasCH07}.

Our experience with our feedback tool shows that
for Mercury programs, this is far from enough.
For most programs,
it finds enough conjunctions with two or more expensive conjuncts,
but almost all are dependent,
and, as we mention in section~\ref{sec:perf},
many of these have too little overlap to be worth parallelising.
% For example, the Mercury compiler contains
% 50 conjunctions with two or more expensive goals.
% 49 of these are dependent.
% Of these, only 38 of these have any overlap,
% and only for 31 does the overlap
% lead to a predicted local speedup of more than 1\%.

We know of only three attempts to estimate the overlap
between parallel computations.
One was in the context of speculative execution in imperative programs.
Given two successive blocks of instructions,
\cite{von_Praun:2007:implicit_parallelism_with_ordered_transactions}
% estimates the likely speedup
% from executing the two blocks in parallel
% by using the difference between the addresses of two instructions
decides whether the second block should be executed speculatively
based on the difference between the addresses of two instructions,
one that writes a value to a register and one that reads from that register.
% This is effectively a binary metric.
This works if instructions take a bounded time to execute,
but in the presence of call instructions
this heuristic will not be at all accurate.

Another attempt was a previous auto-parallelisation project for
Mercury \cite{tannier:2007:parallel_mercury}.
% This did not use profiling data,
% and instead used the number of shared variables between conjuncts
This used the number of shared variables between conjuncts
as a measure of the dependency between goals,
and as a predictor of the likely overlap.
While two conjuncts are indeed less likely
to have useful parallel overlap if they have more shared variables,
we have found this heuristic too inaccurate to be useful.

The most closely related work to ours
generated parallelism annotations for the ACE and/or-parallel system
\cite{Pontelli97automaticcompile-time}.
This system used, much as we do,
estimates of the costs of calls
and of the times at which variables are produced and consumed.
However, it produced its estimates through static analysis of the program.
This can work for small programs,
where the call trees of the relevant calls can be quite small and regular.
In large programs, the call trees of the expensive calls
are almost certain to be both tall and wide,
with a huge gulf between best-case and worst-case behaviour.
Using profiling data is the only way
for an automatic parallelisation system to find out
what the \emph{typical} behaviour of such calls is.

% There is a risk that the program could have changed between the
% profiling build and the parallelised build,
% this makes it more difficult for the compiler to apply the profiling
% advice.
% To reduce this risk the profiling build should be built with the same
% optimizations that the parallelised build will be built with.
% In usual circumstances inlining should be disabled during profiling so
% that a programmer can more easily understand their program's profile.
% Our implementation re-enables inlining in profiling builds if a
% suitable optimization level is selected and
% \code{--profile-for-implicit-parallelism} is passed to the compiler.
% % XXX: These details may be unimportant, especially the name of this
% % compiler option,  But this is (for now) an easy way to describe
% % this.

Our system's predictions of the likely speedup from parallelising a conjunction
are also fallible, since they currently ignore several relevant issues,
including cache effects
and the effects of bottlenecks
such as CPU-memory buses and stop-the-world garbage collection.
However, our system seems to be a sound basis for such further refinements.
% However, they come much closer
% to predicting actual overlaps than previous attempts,
% and our system seems to be a sound basis for further refinements.
% \begin{itemize}
% \item
% It is hard to define what a typical workload is,
% and we do not yet implement profile merging.
% \item
% The feedback framework is general purpose
% and can be used for other optimizations.
% \item
% \zoltan{I haven't covered any technical details about the feedback framework.
% I guess there's not much to say.}
% \end{itemize}
In the future, we plan to support parallelisation as a specialisation:
applying a specific parallelisation only when a predicate is called
from a specific parent, grandparent or other ancestor.
% we will look at how best to resolve cases
% where our tool gives different parallelisation advice for the same conjunction
% due to the different behaviour of that conjunction in different contexts.
We also plan to modify our feedback tool
to accept several profiling data files,
with a priority scheme to resolve any conflicts.
% between their advice.

% \paragraph{Acknowledgements}
We thank the rest of the Mercury team,
and Tom Conway and Peter Wang in particular,
for creating the infrastructure we build upon,
and the anonymous referees for their suggestions.

% \vspace{-5mm}


% TODO Items.
\newpage

\begin{table*}
\begin{tabular}{lll}
Item & Owner & Status \\
\hline

\todoitem{
when calculating the average time of consumption of a variable
in a branched structure,
try giving ZERO weight to arms that themselves are trivial in cost.
example: by\_phases in the front end.
}{n/a}{later} \\

\todoitem{
fix the report generated by mdprof\_feedback:
it says it reports the number of parallelised conjunctions,
when what it reports is the number of procedures
that CONTAIN parallelised conjunctions.
If some procedures contain more than once,
the report is misleading.}{pbone}{done} \\

\todoitem{
implement option (default on) to respect boundaries
when calculating variable signal and wait times:}{pbone}{DONE} \\

\todoitem{
detecting parallelism opportunities involving nonatomic
conjuncts}{pbone}{DONE} \\

\todoitem{
implementation of throttling by default}{zs and pbone}{DONE} \\

\todoitem{
branch-and-bound on all conjuncts,
then peel off conjuncts at start and end}{zs and pbone}{DONE} \\

\todoitem{post message about Boehm parameters}{pbone}{DONE} \\

\todoitem{
consider keeping a non-first parallel conjunct on the original
engine}{all}{Different paper} \\

\todoitem{specialisation: parallel for some ancestors, not others}{zs, with
feedback work by pbone}{Different paper} \\

\todoitem{
grouping conjuncts that together do one thing, reduces $N$ for
B'n'B}{pbone}{low priority} \\

\todoitem{best-first, not depth-first search}{all}{low priority} \\

\todoitem{gzip Deep.data}{zs}{low priority} \\

\todoitem{include Deep.procrep in Deep.data}{zs}{low priority} \\

\todoitem{
Push costly calls into branching code, analysis part}{pbone}{DONE} \\

\todoitem{
Push costly calls into branching code, compiler part}{zs}{DONE} \\

\todoitem{
  Investigate Haskell's nofib benchmark for useful
  programs.}{peters}{DONE} \\

\todoitem{
  Investigate the Mars compiler as a benchmark.}{peters}{} \\

\todoitem{
  Introduce goal IDs to replace goal paths in many places}{zs}{DONE} \\

\todoitem{
  Fix --max-contexts-per-thread}{pbone}{} \\

\todoitem{
  Put workstealing deques in engines, not contexts}{pbone}{DONE} \\

\todoitem{
enhance work stealing and stack segment caching algorithms}{zs
  and pbone and maybe wangp}{} \\

\todoitem{
finish deep profiler paper}{zs}{} \\

\todoitem{
finish dep and parallel paper}{zs}{WIP} \\

\todoitem{
Mission Critical benchmark}{pbone and probably zs}{} \\

\todoitem{compiler as benchmark}{all}{} \\

\todoitem{description of the cost of recursive calls algorithm}{all}{}
\\

\todoitem{
divide and conquer limit:
same seq vs parallel for double recursive procs}{all}{} \\

\todoitem{consider how recursive calls in parallelised conjunctions
  affect the speedup}{pbone with help}{} \\

\todoitem{throttle only looping/D'n'C code}{pbone}{} \\

\todoitem{Commit various things, benchmarks, stats program to some
  repository}{pbone}{DONE} \\

\todoitem{promise that you do not care what random number sequence you get}{}{}

\end{tabular}
\end{table*}


% \begin{algorithm}
% \begin{verbatim}
% MaxBefore := 0
% N := num_conjuncts(Conjs)
% for i in 1 to N:
%     if conjunct i in Conjs is below threshold then
%         MaxBefore := i
%     else
%         break
%
% MinAfter := N+1
% for i in N downto 1:
%     if conjunct i in Conjs is below threshold then
%         MinAfter := i
%     else
%         break
%
% BestTime := infinity
% Arrangements := [[[conjunct MaxBefore+1]]]
% # each element in Arrangements is
% #   a list of parallel conjuncts
% # each parallel conjunct consists of
% #   a list of consecutive conjuncts
% for i in MaxBefore+2 to MinAfter-1:
%     NewArrangements := []
%     for Arrangement in Arrangements:
%         ExtendLast := all_but_last(Arrangement)
%             ++ [last(Arrangement) ++ conjunct i]
%         AddNewLast := Arrangement ++ [conjunct i]
%         NewArrangements := NewArrangements ++
%             [ExtendLast, AddNewLast]
%     Arrangements := NewArrangements
%
%     for Before in 0 to MaxBefore:
%         for After in MinAfter to N+1:
%             GoalsBefore := conjuncts 1 .. Before in Conjs
%             GoalsAfter  := conjuncts After .. N in Conjs
%             # GoalsBefore and/or GoalsAfter may be empty
%             ExtraGoalsBefore := conjuncts (Before+1) .. MaxBefore in
%                Conjs
%             ExtraGoalsAfter := conjuncts MinAfter .. (After-1) in
%                Conjs
%
%             for each Arrangement in Arrangements
%                 Arrangement := [ExtraGoalsBefore ++ first(Arrangement)] ++
%                    all_but_first_and_last(Arrangement) ++
%                     [last(Arrangement) ++ ExtraGoalsAfter]
%                 ParConj := par_conj(Arrangement)
%                 OverallGoal :=
%                     seq_conj(GoalsBefore ++ [ParConj] ++ GoalsAfter)
%                 Time := compute_par_exec_time(OverallGoal)
%                 if Time < BestTime:
%                     BestTime := Time
%                     BestGoal := OverallGoal
% \end{verbatim}
% \caption{Search for best parallelisation}
% \label{alg:branch_and_bound_search}
% \end{algorithm}

