
\status{This chapter is currently WIP}

Early in the project
we tested two manually parallelised programs:
a raytracer and a mandelbrot image generator.
Both of them have a single significant loop
whose iterations are independent of one another.
We expect that automatic parallelisation would parallelise this loop
as it is the best place to introduce parallelism.
When we parallelised this loop manually we did
we did not get the speedups that we expected.
Therefore,
we chose to address the performance problems
before we worked on automatic parallelism.

In this chapter we investigate and correct these performance problems.
We start with the garbage collector in Section \ref{sec:gc};
we analyse the collector's effects on performance and tune its parameters
to improve performance.
In Section \ref{sec:original_scheduling} we describe how the existing runtime
system schedules sparks.
The section provides background material for Section
\ref{sec:original_scheduling_performance}
which benchmarks the runtime system and describes a significant problem with
spark scheduling.
We address those problems by introducing work stealing in Section
\ref{sec:work_stealing}.
This section is separated into two sub-sections
which describe the initial and revised versions of the work stealing
implementation.
Section \ref{sec:work_stealing} also includes benchmarks that show
how work stealing fixes the spark scheduling problem.
We made a number of improvements to the way that Mercury engines are created.
This includes thread pinning and support for SMT systems;
we describe these improvements in Section \ref{sec:thread_pinning}.
Finally, Section \ref{sec:idle_loop} describes our changes to how engines
idle,
and how and when they wake up to execute parallel work.

\section{Garbage collection tweaks}
\label{sec:gc}

\input{rts_gc}

\section{Original spark scheduling algorithm}
\label{sec:original_scheduling}

\input{rts_original_scheduling}

\section{Original spark scheduling performance}
\label{sec:original_scheduling_performance}

\input{rts_original_scheduling_performance}

\section{Initial work stealing implementation}
\label{sec:work_stealing}

\input{rts_work_stealing}

\section{Reorder independent conjunctions}
\label{sec:rts_reorder}

\input{rts_reorder}

\section{Idle loop and better work stealing}
\label{sec:idle_loop}
\status{Not written}

\plan{Describe the problem with the current algorithm.}
Engines wake up and periodically check for work by attempting to
steal work,
firstly, this means that there can be up to a 2ms (average 1ms) delay before
a spark is executed.
secondly, this checks for work too often, wasting resources.

\plan{Solution, wake engines for different types of work}
We modified the RTS so that waking an engine is easy, and it can be given a
message so that it knows where to look for work.

\plan{A lot of work went into preventing deadlocks due to race conditions,
a thread that is not yet sleeping if notified must wake up immediately.}

\plan{Extra benefit: when an engine is woken it can be told directly where
to find work, or be given the work directly.}

\plan{Extra benefit: we can be selective about which engine to wake,
while not implemented fully, we can wake a `nearby' engine so that
we can avoid communication between dies or sockets.}

\plan{Show the algorithm for the new idle loop.}
Note that we execute contexts before sparks,
this is more-likely to produce futures and it may reduce memory consumption.

\subsection{Revised work stealing implementation}

\plan{Work stealing attempts}

\plan{Describe work stealing timeout.}


\plan{Describe problems with associating stacks with contexts}
The number of stacks varies,
Stealing uses a global lock to determine which stack to steal from.

\plan{Prove that even though there are N engines and M contexts and M may be
larger than N, that there will be at most N of the M contexts with work on
their queues}
Therefore:
Stealing is unnecessary complicated.
In pathological cases many attempts can be made without success,
a thief may give up even though there is parallelism.

\plan{We associate stacks with engines}
This removes the above problems how.

\plan{Show stealing algorithm there are any,}
Find out if I started tracking stealing per engine or not.

\plan{Show how this is safe.}
When a parallel conjunction's barrier is executed and a conjunct is
outstanding, if its spark is on this engine's stack it must be at the top of
the stack.
This invariant should be kept because the context can be re-used if
'compatible' work is found at the top of the spark stack.
This invariant is soft.

\input{tab_work_stealing_revised}

\plan{Benchmark}

\section{Thread pinning}
\label{sec:thread_pinning}
\status{Not written}

\plan{Explain why we want $P$ engines when there are $P$ processors}

\plan{Explain briefly how we detect how many processors there are,}
Explain that this method is good because it is (mostly) cross platform.

\plan{Explain why we want thread pinning.}
I wonder if there's any relevant literature.

\plan{Explain how we get thread pinning.}
This method is also (mostly) cross platform.

\plan{What about SMT, not all processors are equal.}
This does not matter when we are creating $P$ engines.
But it does matter when we create less than $P$ engines,
explain how bad CPU assignments are sub-optimal.

\plan{How do we handle SMT}
This uses a support library, which is cross platform, provided that it is installed.
We fall back to setcpuafinity() when it is not.

\paul{I am not going to talk about busy waiting since I have not written the
runtime system in a way that I can test or change this easily.}

\section{Proposed scheduling tweaks}
\label{sec:proposed_tweaks}
\status{Not written, May move to TS chapter}

I really think that this section will move to the \tscope chapter,
it will have more in common with that chapter and more data will be
available.
Secondly, \tscope can be used with micro-benchmarks to measure the
average costs of certain operations in the RTS.
I will not write it until at least the rest of this chapter is finished.

%\section{Proposed kernel support to manage processor resources}
%\label{sec:kernel_scheduling_help}
%
%\status{This may not be worth discussing until someone actually does it}
%
%Should I describe our proposal for OS kernel's to help
%applications with how many threads to use.
%GCD is related but does not fit into a language runtime system so
%easily~\cite{apple_gcd}.
%See also N:M threading.

%\section{Spare text}
%
%\status{This text will be moved up into one of the work stealing sections
%once those sections are ready}

%The spark is added to a global run queue of sparks, or if that queue is too
%full, because there's already enough parallelism,
%then the spark is added to a queue owned by the current context.
%\citet{wang:2006:thesis} intended to use the local queues for work stealing
%but had not completed his implementation,
%The work-stealing dequeue structure
%is described in \citet{Chase_2005_wsdeque}.
%see Chapter \ref{chap:rts} for details.
%
%and finds that spark is still at the head of its queue,
%it will pick it up and run it itself.
%This is a useful optimisation,
%% it is also really well-known.
%since it avoids using a separate context in the relatively common case
%that all the other CPUs are busy with their own work.
%This optimisation is also useful since it can avoid the creation of superfluous
%contexts and their stacks.
%

%When an engine becomes idle, it will first try
%to resume a suspended but runnable context if there is one.
%If not, it will attempt to run a spark from the global spark queue.
%If it successfully finds a spark, it will allocate a context,
%and start running the spark in that context.

% XXX: Mention global spark queue and spark sheduling above.
% XXX:

%Barrier code is placed at the end of each conjunct,
%this is named \code{join\_and\_continue} (Figure \ref{fig:par_conj}).
%This code starts by atomically decrementing the number of outstanding
%conjuncts in the conjunction's syncterm and checking the result for zero
%(the whole operation is thread-safe, not just the decrement).
%Algorithm \ref{alg:join_and_continue} shows the pseudo code for
%join\_and\_continue.


%XXX
%Parallel conjunctions are evaluated as described in Section
%\ref{sec:backgnd_merpar}.
%Sparks are added to the global spark queue if the global queue has room,
%otherwise they are pushed onto a context-local spark stack.
%Creating a lot of parallel work and using a single global work queue can be
%pesimistic,
%the queue itself can become a bottleneck:
%when many processors try to access it the same location in memory there will
%be many cache misses and delays.
%This is why \citet{wang:2006:thesis} choose to introduce local queues,
%and place work on them when there is a surplus of work on the global queue.
%
%When a engine finishes executing a context and reaches the barrier at the
%end of a parallel conjunct,
%it will check the context's local spark stack for any other work and attempt
%to execute it.
%Otherwise, it will either save the context to resume later or release the
%context before checking the global spark queue and global context queue.
%
%XXX Algorithm.
%
%Deciding too early, results.
%
