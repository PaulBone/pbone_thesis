
\status{This chapter is currently WIP}

Early in the project
we tested two manually parallelised programs:
a raytracer and a mandelbrot image generator.
Both of them have a single significant loop
whose iterations are independent of one another.
We expect that automatic parallelisation would parallelise this loop
as it is the best place to introduce parallelism.
When we parallelised this loop manually we did
we did not get the speedups that we expected.
Therefore,
we chose to address the performance problems
before we worked on automatic parallelism.

In this chapter we investigate and correct these performance problems.
We start with the garbage collector in Section \ref{sec:gc};
we analyse the collector's effects on performance and tune its parameters
to improve performance.
In Section \ref{sec:original_scheduling} we describe how the existing runtime
system schedules sparks.
The section provides background material for Section
\ref{sec:original_scheduling_performance}
which benchmarks the runtime system and describes a significant problem with
spark scheduling.
We address those problems by introducing work stealing in Section
\ref{sec:work_stealing}.
This section is separated into two sub-sections
which describe the initial and revised versions of the work stealing
implementation.
Section \ref{sec:work_stealing} also includes benchmarks that show
how work stealing fixes the spark scheduling problem.
We made a number of improvements to the way that Mercury engines are created.
This includes thread pinning and support for SMT systems;
we describe these improvements in Section \ref{sec:thread_pinning}.
Finally, Section \ref{sec:idle_loop} describes our changes to how engines
idle,
and how and when they wake up to execute parallel work.

\section{Garbage collection tweaks}
\label{sec:gc}

\input{rts_gc}


\section{Original spark scheduling algorithm}
\label{sec:original_scheduling}

\plan{Introduction}
We introduced parallelism in Mercury in Sections \ref{sec:backgnd_merpar} and
\ref{sec:backgnd_deppar};
we described how the runtime system in generic terms.
In this section we will explain how sparks were originally managed
prior to 2009,
when I begun my PhD candidature.
This will provide the background for the changes we have made to the
runtime since.

\plan{Global spark queue and Contention wrt global queue}
Mercury (before 2009) uses a global spark queue.
The runtime system \emph{schedules} sparks for parallel execution by placing
them onto the end of the queue.
In this chapter we use the word `schedule' to mean the act of making a spark
available for parallel or sequential work.
An idle engine runs a spark by taking it from the beginning of the queue.
The global spark queue must be protected by a lock;
this prevents concurrent access from corrupting the queue.
The global spark queue and its lock can easily become a bottleneck when many
engines contend for access to the global queue.

\plan{Local spark stack --- relieves contention on global queue}
\citet{wang:2006:hons} anticipated this problem and created context local spark
stacks to avoid contention on the global queue.
Furthermore, the local spark stacks do not require locking.
When a parallel conjunction spawns off a spark,
it places the spark either at the end of the global spark queue or at the
top of its local spark stack.
\plan{Spark scheduling decision.}
The runtime system appends the spark to the end of the global queue if
an engine is idle, and
the number of contexts in use plus the number of sparks on the global queue
does not exceed the maximum number of contexts permitted.
If either part of the condition is false,
the runtime system pushes the spark onto the top of the context's local
spark stack.
This algorithm has two aims.
The first is to reduce contention on the global queue,
especially in the common case that there is enough parallel work.
The second aim is to reduce the amount of memory allocated
as contexts' stacks by reducing the number of contexts allocated.
Globally scheduled sparks may be converted into contexts,
so they are also included in this limit.
We explain this limit on the number of context in more detail
in Section \ref{sec:original_scheduling_performance},
after covering the background information in the current section.
Note that sparks placed on the global queue are executed in a
first-in-first-out manner;
sparks placed on a context's local stack are executed in a
last-in-first-out manner.

\begin{figure}
\begin{tabular}{rl}
 1: & \code{~~MR\_SyncTerm ST;} \\
 2: & \code{~~MR\_init\_syncterm(\&ST, 2);} \\
 3: & \code{~~spawn\_off(\&ST, Spawn\_Label\_1);} \\
 4: & \code{~~}$G_1$ \\
 5: & \code{~~MR\_join\_and\_continue(\&ST, Cont\_Label);} \\
 6: & \code{Spawn\_Label:} \\
 7: & \code{~~}$G_2$ \\
 8: & \code{~~MR\_join\_and\_continue(\&ST, Cont\_Label);} \\
 9: & \code{Cont\_Label:} \\
\end{tabular}
\caption{Parallel conjunction implementation}
\label{fig:par_conj_impl_only}
\end{figure}

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{MR\_join\_and\_continue}{$ST, ContLabel$}
  \State acquire\_lock($ST.lock$)
  \State $ST.num\_outstanding \gets ST.num\_outstanding - 1$
  \If{$ST.num\_outstanding = 0$}
    \If{$ST.orig\_context = this\_context$}
      \State release\_lock($ST.lock$)
      \Goto{$ContLabel$}
    \Else
      \State $ST.orig\_context.resume\_label \gets ContLabel$
      \State schedule($ST.orig\_context$, $ContLabel$)
      \State release\_lock($ST.lock$)
      \Goto{MR\_get\_global\_work}
    \EndIf
  \Else
    \State $spark \gets$ pop\_spark($this\_context.spark\_stack$)
    \If{$spark$}
      \If{$spark.ST.stack\_ptr = MR\_parent\_sp$}
%        \Comment{This spark belongs to the same parallel conjunction.
%        It can be executed immediatly.}
        \State release\_lock($ST.lock$)
        \Goto{$spark.code\_label$}
      \Else
        \State push\_spark($this\_context.spark\_stack$, $spark$)
      \EndIf
    \EndIf
    \If{$ST.orig\_context = this\_context$}
       \State suspend($this\_context$)
       \State $this\_context \gets NULL$
    \EndIf
    \State release\_lock($ST.lock$)
    \Goto{MR\_get\_global\_work}
  \EndIf
\EndProcedure
\end{algorithmic}
\caption{MR\_join\_and\_continue}
\label{alg:join_and_continue_peterw}
\end{algorithm}

\plan{barrier code, this is used to explain the right recursion problem.}
In Section \ref{sec:backgnd_merpar} we described how parallel
conjunctions are compiled
(Figure \ref{fig:par_conj} shows an example).
Consider the compiled parallel conjunction in Figure
\ref{fig:par_conj_impl_only}.
The context that executes the parallel conjunction,
lets call it $C_{Orig}$,
begins by
setting up the sync term,
spawning off $G_2$,
and executing $G_1$ (lines 1--4).
Then it executes the barrier \joinandcontinue,
whose is shown in
Algorithm \ref{alg:join_and_continue_peterw}.
Depending on how full the global run queue is,
and how parallel tasks are interleaved,
there are three important scenarios:

\begin{description}

    \item[Scenario one:]~

    In this scenario $C_{Orig}$ placed the spark for $G_2$ on the top of its
    local spark stack.
    Sparks placed on a context local spark stack cannot be executed by any
    other context.
    Therefore when $C_{Orig}$ reaches the \joinandcontinue barrier
    (line 5 in the example),
    the context $G_2$ will be outstanding and $ST.num\_outstanding$ will be
    non-zero.
    $C_{Orig}$ will execute else branch on line 14 of \joinandcontinue
    where it will pop the spark for $G_2$ off the top of the spark stack.
    It is not possible for some other spark to be on the top of the stack;
    any sparks left on the stack by $G_1$ would have been poped off by 
    the \joinandcontinue barriers of the conjunctions that spawned off the
    sparks.

    The check that the sparks stack pointer is equal to the current
    parent stack pointer\footnote{
        The code generator will ensure that $MR\_parent\_sp$ is set
        before the parallel conjunction is executed,
        and that it is restored after.}
    will succeed (line 16 of \joinandcontinue),
    and the context will execute the spark.

    After executing $G_2$,
    $C_{Orig}$ will execute the second call to \joinandcontinue,
    the one on line 8 of the example.
    This time $ST.num\_outstanding$ will be zero,
    and $C_{Orig}$ will execute the then branch on line 5 of
    \joinandcontinue.
    In the condition of the nested if-then-else,
    $C_{Orig}$ will find that the current context is the original context,
    and therefore continue execution at line 6.
    This causes $C_{Orig}$ to jump to the continuation label,
    line 9 in the example,
    completing the execution of the parallel conjunction.

    \item[Scenario two:]~

    In this scenario $C_{Orig}$ placed the spark for $G_2$ on the 
    global spark queue,
    where another context, $C_{Other}$, picked it up and executed it
    in parallel.
    Also, as distinct from scenario three,
    $C_{Other}$ reaches the barrier on line 8 of the example \emph{before}
    $C_{Orig}$ reaches the barrier on line 5.
    Even if they both seem to reach the barrier at the same time,
    their barrier operations are performed in sequence because of the
    lock protecting the barrier code.

    When $C_{Other}$ executes \joinandcontinue,
    It will find that $ST.num\_outstanding$ is non-zero,
    and will execute the else branch on line 14 of \joinandcontinue.
    It then attempts to pop a spark off its stack,
    as in another scenario a spark on the stack might represent an
    outstanding conjunction
    (it cannot tell that the outstanding conjunct is $G_1$ executing in
    parallel, and not some hypothetical $G_3$).
    $C_{Other}$ took this spark from the global queue,
    and was either empty or brand new before executing this spark,
    meaning that it had no sparks on its stack before executing $G_2$.
    Therefore $C_{Other}$ will not have any sparks of its own and
    \code{pop\_spark} will fail.
    $C_{Other}$ will continue to line 21 in \joinandcontinue
    which will also fail since it ($C_{Other}$)
    is not the original context ($C_{Orig}$).
    The lock will be released and \joinandcontinue will determine what to do
    next by jumping to \getglobalwork.

    Eventually $C_{Orig}$ will execute its call to \joinandcontinue
    (line 5 of the example),
    or be executed after waiting on the barrier's lock (line 2 of
    \joinandcontinue),.
    When this happens it will find that $ST.num\_outstanding$ is zero,
    and execute the then branch beginning at line 5 of \joinandcontinue.
    $C_{Orig}$ will test if it is the original context, 
    which it is,
    and continue on line 6 of \joinandcontinue.
    It then jumps to the continuation label on line 9 of the example,
    completing the parallel execution of the conjunction.

    \item[Scenario three:]~

    As in scenario two $C_{Orig}$ pl the spark for $G_2$ on the global spark
    queue,
    where another context, $C_{Other}$, picked it up and executed it
    in parallel.
    However,
    in this scenario
    $C_{Orig}$ reaches the barrier on line 5 in the example \emph{before}
    $C_{Other}$ reaches its barrier on line 8.

    When $C_{Orig}$ executes \joinandcontinue,
    it finds that $ST.num\_outstanding$ is non-zero,
    causing it to execute the else branch on line 14 of \joinandcontinue.
    $C_{Orig}$ will try to pop a spark of its local spark stack.
    However the the spark for $G_2$ was placed on the global
    spark queue,
    the only spark it might find is one created by an outer conjunction.
    If a spark is found, the spark's parent stack pointer will not match the
    current parent stack pointer,
    and it will put the spark back on the stack.
    $C_{Orig}$ executes the then branch (line 22 of \joinandcontinue), since this
    context is the original context.
    This branch will suspend $C_{Orig}$,
    and set the engine's context pointer to \NULL
    before jumping to \getglobalwork.

    When $C_{Other}$ reaches the barrier on line 8,
    it will find that $ST.num\_outstanding = 0$,
    and will execute the then branch of the if-then-else.
    Within this branch it will test to see if it is the original
    context,
    the test will fail and the else branch of the nested if-then-else
    will be executed.
    At this point we know that $C_{Orig}$ must be suspended because
    there where no outstanding conjuncts and the current context is not
    the original context;
    this can only happen if $C_{Orig}$ is suspended.
    The code wakes $C_{Orig}$ up by
    setting its code pointer to the continuation label,
    placing it on the global run queue,
    and then jumps to \getglobalwork.

    When $C_{Orig}$ resumes execution it executes the code on line 9,
    which is the continuation label.

\end{description}

\paul{XXX}
The algorithm includes an optimisation not shown here:
if the parallel conjunction has been executed by only one context,
then a version of the algorithm without locking is used.
We have not shown the optimisation because it is equivalent and not relevant
to our discussion,
we mention it only for completeness.

\begin{algorithm}
\begin{algorithmic}
\Procedure{MR\_get\_global\_work}{}
  \State acquire\_lock($MR\_runqueue\_lock$)
  \Loop
    \If{$MR\_exit\_now$}
      \State release\_lock($MR\_runqueue\_lock$)
      \State MR\_destroy\_thread()
    \EndIf
    \State $ctxt \gets$ MR\_get\_runnable\_context()
    \If{$ctxt$}
      \State release\_lock($MR\_runqueue\_lock$)
      \If{$current\_context$}
        \State MR\_release\_context($current\_context$)
      \EndIf
      \State MR\_load\_context($ctxt$)
      \Goto $ctxt.resume$
    \EndIf
    \State $spark \gets$ MR\_dequeue\_spark($MR\_global\_spark\_queue$)
    \If{$spark$}
      \State release\_lock($MR\_runqueue\_lock$)
      \If{$\neg current\_context$}
        \State $ctxt \gets$ MR\_get\_free\_context()
        \If{$\neg ctxt$}
          \State $ctxt \gets$ MR\_create\_context()
        \EndIf
        \State MR\_load\_context($ctxt$)
      \EndIf
      \State $MR\_parent\_sp \gets spark.parent\_sp$
      \State $ctxt.thread\_local\_mutables \gets
        spark.thread\_local\_mutables$
      \Goto $spark.resume$
    \EndIf
    \State wait($MR\_runqueue\_cond, MR\_runqueue\_lock$)
  \EndLoop
\EndProcedure
\end{algorithmic}
\caption{\getglobalwork}
\label{alg:MR_get_global_work_initial}
\end{algorithm}

\plan{Explain how work begins executing, for completeness.}
When an engine cannot get any local work it must search for global work.
Newly created engines, except for the first, also search for global work.
They do this by calling \getglobalwork,
whose code is shown in Algorithm \ref{alg:MR_get_global_work_initial}.
Only one of the idle engines can execute \getglobalwork at a time.
The $MR\_runqueue\_lock$ lock protects the context run queue and the global
context queue from concurrent access.
After acquiring the lock,
engines execute a loop.
An engine exits the loop only when it finds some work to do or the
program is exiting.
Each iteration first checks if the runtime system is being shut down,
if so,
then this thread releases the lock,
and then destroys itself.
If the system is not being shut down,
the engine will search for a runnable context.
If it finds a context it releases the run queue lock, loads the context
and jumps to the resume point for the context.
If it already had a context then it first releases that context,
doing so is safe as
a precondition of an engine calling \getglobalwork is that if the engine has
a context, the context must not contain a spark or represent a
suspended computation.
If no context was found, the engine attempts to take a spark from the global
spark queue.
If it finds a spark then it will need a context to execute that spark.
It will try to get a context from the free list, if there is none it will
create a new context.
Once it has a context,
it copies the context's copies of registers into the engine.
It also initialises the engine's parent stack pointer
register and the spark's thread local mutables
(which are set by the context that created the spark)
into the context.
If the engine does not find any work,
it will wait using a condition variable and the run queue lock.
The pthreads wait function is able to unlock the lock and wait on the
condition atomically, preventing race conditions.
The condition variable is used to wake up the engine if either a spark is
placed on the global spark queue or a context is placed on the context run
queue.
When the engine wakes,
it will re-execute the loop.


\section{Original spark scheduling performance}
\label{sec:original_scheduling_performance}

\begin{figure}
\begin{center}
\subfigure[Right recursive]{%
\label{fig:map_right_recursive}
\begin{tabular}{l}
\code{map(\_, [], []).} \\
\code{map(P, [X $|$ Xs], [Y $|$ Ys]) :-} \\
\code{~~~~P(X, Y) \&} \\
\code{~~~~map(P, Xs, Ys).} \\
\end{tabular}
}
\subfigure[Left recursive]{%
\label{fig:map_left_recursive}
\begin{tabular}{l}
\code{map(\_, [], []).} \\
\code{map(P, [X $|$ Xs], [Y $|$ Ys]) :-} \\
\code{~~~~map(P, Xs, Ys) \&} \\
\code{~~~~P(X, Y).} \\
\end{tabular}
}%
\end{center}
\caption{Right and left recursive map/3}
\label{fig:map_right_and_left_recursive}
\end{figure}

Section \ref{sec:gc} we ran our benchmarks with a recent version of the
runtime system.
In the rest of this chapter we describe many of the improvements to the
runtime system that improved parallel performance.

\plan{Introduce right recursion.}
Figure \ref{fig:map_right_and_left_recursive} shows two alternative, parallel
implementations of \code{map/3}.
While their declarative semantics are identical,
their operational semantics are very different.
In Section \ref{sec:backgnd_merpar} we explained that parallel conjunctions
are implemented by spawning off the second and later conjuncts and executing
the first conjunct directly.
In the right recursive case (Figure \ref{fig:map_right_recursive}),
the recursive call is spawned off as a spark,
and in the left recursive case (Figure \ref{fig:map_left_recursive}),
the recursive call is executed directly, and the loop \emph{body} is
spawned off.
Declarative programmers are taught to prefer tail recursion,
and therefore tend to write right recursive code.

\begin{table}
\begin{center}
\begin{tabular}{r|rr|rrrr}
\multicolumn{1}{c|}{Max no.} &
\multicolumn{2}{c|}{Sequmential} &
\multicolumn{4}{c}{Parallel w/ $N$ Engines} \\
\Cbr{of contexts} & \C{not TS} & \Cbr{TS} & \C{1}& \C{2}& \C{3}& \C{4}\\
\hline
4        & 23.2 (0.93) & 21.5 (1.00)
         & 21.5 (1.00) & 21.5 (1.00) & 21.5 (1.00) & 21.5 (1.00) \\
64   &-&-& 21.5 (1.00) & 21.5 (1.00) & 21.5 (1.00) & 21.5 (1.00) \\
128  &-&-& 21.5 (1.00) & 19.8 (1.09) & 20.9 (1.03) & 21.2 (1.01) \\
256  &-&-& 21.5 (1.00) & 13.2 (1.63) & 15.5 (1.38) & 16.5 (1.30) \\
512  &-&-& 21.5 (1.00) & 11.9 (1.81) &  8.1 (2.66) &  6.1 (3.55) \\
1024 &-&-& 21.5 (1.00) & 11.8 (1.81) &  8.0 (2.67) &  6.1 (3.55) \\
2048 &-&-& 21.5 (1.00) & 11.9 (1.81) &  8.0 (2.67) &  6.0 (3.55) \\
\end{tabular}
\end{center}
\caption{Right recursion performance.}
\label{tab:right}
\end{table}

\plan{Show performance figures.}
Table \ref{tab:right} shows average elapsed time in seconds for the
mandelbrot\_lowalloc program from 20 test runs.
We use the mandelbrot\_lowalloc program from Section \ref{sec:gc}.
Using this program we can easily observe the
speedup due to parallelism in Mercury without the effects of the garbage
collector.
The loop that iterates over the rows in the image uses right recursion.
It is similar to \code{map/3}
in Figure \ref{fig:map_right_recursive}.
The left most column shows the maximum number of contexts permitted at
any time.
This is the limit that was mentioned in the previous section.
The next two columns give the elapsed execution time for a sequential
version of the program,
in this version of the program no parallel conjunctions where used.
These two columns give results without and with thread safety.
The following four columns give the elapsed execution times
using one to four Mercury engines.
The numbers in parentheses are the ratio between the time and the
sequential thread safe time.

\plan{Observations}
In general we acheive more parallelism when we use more contexts,
until a threshold of 601 contexts,
as there are 600 rows in the image and a base case each one consumes a
context.
This is why the program does not benifit greatly from a high limit such as
1024 or 2048 xontexts.
This program may use fewer than 600 contexts as any sequentially executed
sparks use their parent context.
This is possibly why a limit of 512 contexts also results in a good parallel
speedup.
When only 256 contexts are used the four core version achieves a speedup
of 1.30,
compared to 3.55 for 512 or more contexts.
Given that mandelbrot uses independent parallelism there should never be any
need to suspend a context.
Therefore the program should parallelise well enough when restricted to
a small number of contexts (four to eight).
Too many contexts are needed to execute this program at the level of
parallelism that we want.
We noticed the same pattern in other programs with right recursive
parallelism.

\label{context_limit}
\plan{Describe the context limit problem.}
%Right recursion uses a context for each iteration of the loop,
%and then suspends that context.
To understand this problem we must consider how parallel conjunctions are
executed (see Section \ref{sec:backgnd_merpar}).
The original context creates a spark for the second and later conjuncts and
puts it on the global spark queue.
It then executes the first conjunct \code{P/2}.
This takes much less time to execute than the spawned off call to
\code{map/3}.
After executing \code{P/2} the original context will call the
\joinandcontinue barrier.
It then attempts to execute a spark from its local spark stack,
this will fail because the only spark was placed on the global spark queue.
The original context will be needed after the other conjunct finishes, 
to execute the code after the parallel conjunction.
The original context cannot proceed but will be needed later,
therefore it is suspended until all the other 599 iterations of the loop
have finished.
Meanwhile the spark that was placed on the global run queue is converted
into a new context.
This new context enters the recursive call and
becomes blocked within the recursive instance of the same parallel
conjunction, it must wait for the remaining 598 iterations of the loop.
This process continues to repeat itself,
allocating more contexts which can consume large amounts of memory.
Each context consumes a significant amount of memory,
much more than one stack frame,
Therefore
this problem makes programs that look tail recursive
\emph{consume much more memory than}
sequential programs that are not tail recursive.
We implement the limit on the number of contexts to prevent pathological
cases such as this one from crashing the program or the whole computer.
Once the limit is reached sparks cannot be placed on the global run queue
and sequential execution is used.
Therefore the context limit is a trade off between memory usage and
parallelism.

\picfigure{linear_context_usage}{Linear context usage in right recursion}

\plan{Diagram from the loop control talk}
An example of context usage over time using four engines is shown in
Figure \ref{fig:linear_context_usage}.
Time is shown on the Y axis and contexts are drawn so that each additional
context is further along the X axis.
At the top left,
four contexts are created and they execute four iterations of a loop;
this execution is indicated by boxes.
Once each of these iterations finishes,
its context stays in memory but is suspended,
indicated by the long vertical lines.
Another four iterations of the loop create another four contexts, and so on.
Later when all iterations of the loop have been executed,
each of the blocked contexts resumes execution and immediately exits,
indicated by the horizontal line at the bottom of each of the vertical
lines.
Later in the project we developed a solution to this problem
(see Chapter \ref{chap:loop_control}).
\paul{XXX: Point exactly to the workarounds.}
In this chapter we describe our original workarounds before
we developed the solution.

\plan{Extra engines require more contexts}
\paul{XXX}
When using either 128 or 256 contexts,
we noticed that when we used three or four Mercury engines
the program ran more slowly than with two Mercury engines.
A possible reason is that:
the more Mercury engines there are the more often at least one engine is
idle.
When creating a spark the runtime system checks four an idle engine,
if there is one then the spark may be placed on the global queue, subject to
    the context limit (see Section \ref{sec:original_scheduling}).
If there is no idle engine,
a spark is placed on the context's local queue regardless of the current
number of contexts.
When there are fewer Mercury engines being used then it is less likely that
one of them is idle.
Therefore more sparks are placed on the local queue and executed
sequentially and the program does not hit the context limit as quickly as
one using more Mercury engines.
The program can exploit more parallelism as it hits the context limit
later,
and achieve better speedups.

\plan{Suggest that left recursion might fix this,
(This is what we thought at the time).}
Only right recursion behaves in this way.
A left recursive predicate (Figure \ref{fig:map_left_recursive} has its
recursive call on the left of the parallel conjunction.
The context executing the conjunction sparks off the body of the loop,
in the figure this is \code{P},
and executes the recursive call directly.
By the time the recursive call finishes,
the conjunct containing \code{P} should have also finished.
The context that created the parallel conjunction is less likely to be
blocked at the barrier,
and the context executing the spark is never blocked since it is not the
original context that executed the conjunction.

\begin{table}
\begin{center}
\begin{tabular}{r|rrrrrr}
\multicolumn{1}{c|}{Max no.} &
\multicolumn{2}{c|}{Sequmential} &
\multicolumn{4}{c}{Parallel w/ $N$ Engines} \\
\Cbr{of contexts} & \C{not TS} & \Cbr{TS}  & \C{1}& \C{2}& \C{3}& \C{4}\\
\hline
\hline
\multicolumn{7}{c}{Right recursion} \\
\hline
4        & 23.2 (0.00) & 21.5 (0.04)
         & 21.5 (0.02) & 21.5 (0.00) & 21.5 (0.01) & 21.5 (0.02) \\
64   &-&-& 21.5 (0.03) & 21.5 (0.01) & 21.5 (0.01) & 21.5 (0.02) \\
128  &-&-& 21.5 (0.02) & 19.8 (0.01) & 20.9 (0.03) & 21.2 (0.03) \\
256  &-&-& 21.5 (0.02) & 13.2 (0.05) & 15.5 (0.06) & 16.5 (0.07) \\
512  &-&-& 21.5 (0.02) & 11.9 (0.11) &  8.1 (0.09) &  6.1 (0.08) \\
1024 &-&-& 21.5 (0.03) & 11.8 (0.11) &  8.0 (0.06) &  6.1 (0.08) \\
2048 &-&-& 21.5 (0.03) & 11.9 (0.10) &  8.0 (0.08) &  6.0 (0.06) \\
\hline
\hline
\multicolumn{7}{c}{Left recursion (Sparks included in context limit)} \\
\hline
4        & 23.2 (0.01) & 21.5 (0.03)
         & 21.5 (0.02) & 21.5 (0.04) & 21.5 (0.04) & 21.5 (0.02) \\
64   &-&-& 21.5 (0.02) & 21.5 (0.02) & 21.4 (0.03) & 21.5 (0.03) \\
128  &-&-& 21.5 (0.04) & 21.5 (0.03) & 21.5 (0.03) & 21.5 (0.03) \\
256  &-&-& 21.5 (0.02) & 18.3 (0.75) & 18.2 (0.03) & 19.6 (1.37) \\
512  &-&-& 21.5 (0.02) & 17.9 (0.83) & 15.5 (1.29) & 16.4 (7.09) \\
1024 &-&-& 21.5 (0.03) & 18.0 (0.85) & 14.7 (2.18) & 16.1 (5.23) \\
2048 &-&-& 21.5 (0.02) & 18.0 (0.85) & 15.4 (2.15) & 17.8 (5.25) \\
\hline
\hline
\multicolumn{7}{c}{Left recursion (Sparks excluded from context limit)} \\
\hline
N/A      & 23.3 (0.01) & 21.5 (0.02)
         & 21.7 (0.78) & 17.9 (0.60) & 15.6 (2.49) & 14.2 (6.94) \\
\end{tabular}
\end{center}
\caption{
Right and Left recursion shown with standard deviation}
\label{tab:2009_left_nolimit}
\end{table}

\plan{Show performance figures for left recursion.}
Table \ref{tab:2009_left_nolimit} shows benchmark results using left
recursion.
The table is broken into three sections:
a copy of the right recursion data from Table \ref{tab:right},
which is presented again for comparison;
the left recursive data,
which we will discuss now;
and left recursive data with a modified context limit,
which we will discuss in a moment.
As above, the first column represents the maximum number of contexts
allowed.
The next two columns give the sequential execution time, without and
with thread safety.
And the remaining four columns give the results for the parallelised
program using one to four Mercury engines.
All results are the mean of 20 test runs,
printed next to each result is the sample standard deviation.

The left recursive figures are underwhelming;
they are worse than right recursion.
Furthermore,
the sample standard deviations in for left recursion results are much
higher.
In particular,
the more contexts and Mercury engines are used, the higher the
deviation.
This means that few observations can be made using this data,
however,
the results are still much slower than the right recursive results and
the large amount of variance in the results are observations that we can
make.

\plan{Explain the premature scheduling problem that affects left-recursive programs.}
The left recursion problem looks similar to the right recursive problem
since they are both affected by the context limit.
However, they are different.
To demonstrate the left recursion problem,
let us assume that there is a context limit of eight,
and there are two Mercury engines.
The first Mercury engine is executing the original context,
which enters the parallel conjunction and spawns off a spark adding it
to the global queue.
The context then executes the recursive call continues to spawn off
sparks.
As we described in Section \ref{sec:original_scheduling},
each spark on the global spark queue may be converted into a new
context,
and therefore, sparks on the global queue contribute towards the context
limit.
After eight recursions,
it is very likely that the context limit has been exceeded.
It may not have been.
The number of contexts and sparks will be reduced
when the second engine executes its second spark,
as it allocates a new context to execute the first spark and re-uses it
for the second.
If the second engine is quick enough,
the first engine may have been able to place 9 or 10 sparks on the
global queue before the limit was exceeded,
but this is unlikely.
The first Mercury engine is doing nothing more than following a
recursive call,
setting up a synchronisation term and
then scheduling a spark;
each iteration is very quick.
Meanwhile,
the second Mercury engine is executing the call to \code{P} in each
recursion;
which is much slower.
Therefore,
sparks are being created more quickly than they can be executed.
When using a low context limit most of these sparks are placed on
the original context's local stack,
and will be executed sequentially.

\plan{Left recursion w/out limit results}
The left recursive version of mandelbrot uses independent
parallelism and left recursion;
it never needs more than one context per Mercury engine.
Therefore, we are able to safely remove sparks from the context limit
since we know that any contexts allocated in order to evaluate sparks
can be re-used.
The final row group (of a single row) in Table
\ref{tab:2009_left_nolimit} shows results for the left recursive test
with a modified context limit.
As such, the context limit is no-longer relevant to the test,
so only one row of values is shown.
The standard deviation for this result is still very high,
nearly seven seconds for four Mercury engines,
so it cannot be easily compared with other results.
Nevertheless, this result is similar to those in the left recursive
group that have a sufficiently high context limit.
This supports the idea above, however the program still performs poorly,
and the variance is high.

\plan{Second problem, discuss the high variance}
The high variance in all the left recursive results is
indicative of a nondeterminism.
The cause is going to be something that either varies between execution
runs or does not always effect execution runs.
Parallel task scheduling is a potential cause.
When a spark is scheduled and placed on either the global queue or
context local stack.
the context limit is one of the two things used to make this scheduling
decision;
the other is, if there is no idle engine then the spark is always
placed on the local spark stack (Section \ref{sec:original_scheduling}).
At the time when the parallel conjunction in \code{map/3} is executed,
the other engines will initially be idle but will quickly become busy,
once they are busy the original context will not place sparks on the
global spark queue.
If the other engines are slow to respond to the first sparks placed
on the global queue,
then a larger number of sparks are placed on this queue and more
parallel work is available.
If they are quick to respond,
then more sparks will be placed on the original contexts local queue,
where they will be executed sequentially.

\plan{Reinforce that these results support the idea that scheduling decisions are made prematurely}
Both of the left recursive problems have the same cause.
The decision to execute a spark sequentially or in parallel is
made too early.
This decision is made when the spark is scheduled,
and placed either on the global spark queue or the context local spark
stack.
The data used to make the decision includes the number of contexts in
use,
the number of sparks on the global queue,
and if there is an idle engine.
These conditions will be different when the spark is scheduled compared
to when it may be executed,
and the conditions when it is scheduled are not a good indication of
the conditions when it may be executed.
We will refer to this problem as the \emph{premature spark scheduling problem}.
In the next section,
we solve this problem by delaying the decision to execute a spark in
parallel or in sequence.

%In the left recursive program scheduling is quite different.
%The parallel conjunction creates a spark for \code{P(X, Y)} and executes the
%recursive call directly.
%The spark is converted into a context,
%that context does not execute another parallel conjunction since it does not
%execute the recursive call.
%Therefore, it will not become blocked on the \joinandcontinue barrier in any
%nested parallel conjunction.
%It will execute the barrier after \code{P(X, Y)},
%this however does not block this context.
%The context is not the conjunction's original context and therefore once it
%reaches this barrier it is free,
%if it has any sparks on its local queue it may execute them,
%otherwise the engine executing it will look for global work,
%either another context or a spark from the global queue.
%If there is a spark on the global queue the engine will use this context to
%execute it since the context is otherwise unused.
%
%This led us to believe that the left recursion would be more efficient than
%right recursion,
%namely that since contexts are reused, the number of contexts wouldn't climb
%and prevent parallelism from being exploited.
%As Table \ref{tab:right_v_left} shows, we were wrong:
%the context limit is affecting performance.
%As discussed, a left-recursive loop spawns of calls to \code{P} as sparks
%and executes its recursive call directly.
%It will, very quickly,
%make many recursive calls, spawn off many sparks.
%The context limit includes sparks on the global queue since
%executing them can require the creation of new contexts,
%Furthermore, if they were not included and the runtime system refused to
%convert a spark on the global queue into a context the system could become
%deadlocked.
%In the left recursive case,
%the context limit will be reached very quickly,
%often before engines have begun taking sparks from the queue and executing
%them.
%Once the limit is reached sparks are placed on the contexts local queues
%where they cannot be executed in parallel.
%The smaller the context limit,
%the more quickly the limit is reached and the fewer contexts are placed on
%the global queue.
%Additionally,
%the loop placing sparks on its context's local stack will execute very
%quickly.
%
%We concluded that
%in the left recursive case
%the scheduling decision for each spark is made much earlier than the spark's
%execution.
%Specifically,
%when the decision to place the spark on the global queue or local stack is
%made,
%often the context limit has already been reached:
%\paul{Need to decide how I communicate who the actor is for scheduling
%decisions.}
%the context will place the spark on its local stack.
%Later, when a different engine becomes idle,
%it cannot access the spark since it is on another engine's context's spark
%stack.
%At this point it is apparent that the scheduling decision made when the
%spark was placed on the local stack was incorrect,
%as there is an idle engine ready to execute the spark,
%and because contexts are re-used (in left recursion) there is either a free
%context or we can easily create one.

\section{Initial work stealing implementation}
\label{sec:work_stealing}

\status{This section is ready for review.}

\plan{Introduce work stealing}
Work stealing is a popular method for managing parallel work in a shared
memory multiprocessor system.
A number of language implementations use work stealing.
% XXX: There may be earlier papers by keller 1984 as cited by halstead.
\paul{XXX: I have skimmed this paper, I must go back and read it
properly}
One of the earliest was Multilisp \citep{halstead:1985:multilisp};
In this paper,
Halstead discusses ``an unfair scheduling policy'',
in which each worker thread maintains a queue of tasks
and there is no global queue.
When a thread spawns off a task it places the task on its queue.
After completing a task, a thread looks for its next task first 
on its own queue, and if there is none
the thread attempts to steal a task from the other threads.
\paul{XXX: I have not read this paper yet.
It ranks highly in google scholar results and Peter used it so I suspect
that it is notable.}
Many more papers have been published about work stealing.
A notable example is
\citet{blumofe:1999:work-stealing},
which quantifiably proves that work stealing is more efficient than
\emph{work sharing}
(using a global pool of work).

\paul{XXX: Make sure these paragraphs flow once I read the papers above}
\plan{Refer to Peter Wang's conclusion of the early scheduling problem}
\citet{wang:2006:hons} also mentioned the premature spark scheduling problem
from the previous section.
However, he did not discuss it detail or
recognise that left recursion can create pathological behaviour.
Wang also proposes work stealing as a solution to the problem.
In a work stealing system a spark placed on a context local stack
is not committed to running on that context;
it may be executed on a different context if it is stolen.
This delays the decision of where to execute a spark until the moment
before it is executed.
Work stealing's other benefits can be summarised as follows:

\begin{itemize}

    \item
    A worker can schedule parallel work quickly,
    it is quicker to place work on a local queue rather than a global
    one:
    There is less contention on a local queue than a global one.

    \item
    A idle worker can take work from its own queue quickly.
    Again, there is less contention on a local queue.

    \item
    In ideal circumstances,
    an idle worker rarely needs to steal work from another's queue.
    Stealing work is more costly as it requires communication between
    processors.

\end{itemize}

The initial work stealing implementation for Mercury was collaboration
between Peter Wang and myself, Wang contributed about 80\% of the work
and I contributed the remaining 20\%.
Wang's honours thesis \citep{wang:2006:hons} describes his proposal on which
our implementation is heavily based.

\plan{Mercury's needs for a deque}
So far each context has used a stack to manage sparks.
The last item pushed onto the stack is the first to be popped off the
stack,
this \emph{last-in-first-out} order is important for Mercury's parallel
conjunctions.
The main reason for this is that when parallelism is not used and
parallel conjunctions are nested,
either within the same predicate
or due to one or more predicate calls,
the last-in-first-out order ensures that the left most spark is executed
first.
This is important because it allows a future to \signal a variable in an
inner conjunct before \wait is executed on the future in the other
conjunct.
Therefore the data structure used for the deque must return sparks
for the local context in a last-in-first-out order.
This execution order does not affect multiple conjuncts in the same
parallel conjunction.
The second and later conjuncts are spawned off as a single spark
and the left most conjunct is executed directly.
The second reason to use a stack to manage context local sparks
is that the last-in-first-out order ensures that a poped spark's data
has a good chance of being hot in the processor's cache.

Sparks added to Mercury's global spark queue where 
returned in \emph{first-in-first-out} order.
This does not encourage a left to right execution order.
However \citet{wang:2006:hons} proposes that this order may be better:

\begin{quote}
The global spark queue, however, is a queue because we assume that a
spark generated earlier will have greater amounts of work underneath it
than the latest generated spark, and hence is a better candidate for
parallel execution.
\end{quote}

\plan{Describe the data structure used to implement these stacks and its
properties.}
To preserve the last-in-first-out behaviour of context local spark
stacks,
and first-in-first-out behaviour of the global queue;
we chose to use double ended queues (deques) for context local spark
storage.
The deque implementation we chose 
is described by
\citet{Chase_2005_wsdeque}.
It supports nonblocking operation and is dynamically resizable,
which are important for a work stealing deque.
The deque supports the following operations:

\begin{description}

    \item[\code{void push\_spark(deque *d, spark *s)}]
    The context that owns the deque can push items onto the top%
\footnote{
        Note when we refer to the top of the deque
        \citet{Chase_2005_wsdeque} refers to the bottom and vice-versa.
        We prefer to use the terminology of pushing onto and popping off of
        top, as we would when discussing a stack.
        We clarify this with the hot and cold metaphor.}
    of the deque.
    This can be done without synchronisation.
    If necessary
    \push will grow the array that is used to implement the
    deque.

    \item[\code{bool pop\_spark(deque *d, spark *s)}]
    The deque's owner can pop items from the top of the queue.
    This can also be done without synchronisation,
    except for when the deque contains only one item.
    In this case an atomic compare and swap operation is used to
    determine if the thread lost a race to another thread attempting to
    steal a task from the deque (a \emph{thief}).
    When this happens the single item was stolen by the thief and the
    owner's call to \pop returns false,
    indicating that the deque was empty.
    Internally the deque is stored as an array of sparks, not spark
    pointers.
    This is why the second argument in which the result is returned is not a
    double pointer as one might expect.
    This implementation detail avoids memory allocation for sparks inside
    the deque implementation.
    Callers of these function usually store sparks on their stack.

    \item[\code{result steal\_spark(deque *d, spark *s)}]
    A thread other than the deque's owner can steal
    items from the bottom of the deque.
    This always uses an atomic compare and swap operation as multiple
    thieves may call \steal on the same deque at the same time.
    \steal can return one of three different values:
    ``success'', ``failure'' and ``abort''.
    ``abort'' indicates that the thief lost a race with either the owner
    (\emph{victim}) or another thief.

\end{description}

All the atomic compare and swap operations here are wait free,
rather than looping \pop will return false and \steal will abort.
\push and \pop are very fast, this is good as a context uses its own
spark deque more often than calling \steal on another's.
\steal is slightly slower than either \push or \pop,
but is still quite fast.
This is good because a context will 
\push and \pop sparks onto and off of its local deque
more often than it will \steal a spark from another's deque.
This means that the top of the deque is used more often than the bottom.
Therefore, we refer to the top as the \emph{hot} end,
and to the bottom as the \emph{cold} end.
These terms are less confusing than the disagreement between the stack
like behaviour of a deque from its context's perspective,
and the terminology used by \citet{Chase_2005_wsdeque}. 

Mercury was already using Chase's deques for spark storage,
most likely because Wang had always planned to implement work stealing.
We did not need to replace the data structure used for a context local
spark storage,
but we will now refer to it as a deque rather than a stack.
We have removed the global spark queue,
as work stealing does not use one.
Consequently,
when a context creates a spark that spark is always placed on the
context's local spark queue.

\begin{algorithm}
\paul{XXX: Place these environments once we know what pagination will be
used}
\begin{algorithmic}[1]
\Procedure{MR\_join\_and\_continue}{$ST, ContLabel$}
  \State $finished \gets$ atomic\_dec\_and\_is\_zero($ST.num\_outstanding$)
  \If{$finished$}
    \If{$ST.orig\_context = this\_context$}
      \Goto{$ContLabel$}
    \Else
      \While{$ST.orig\_context.resume\_label \neq ContLabel$}
        \State CPU\_relax
      \EndWhile
      \State schedule($ST.parent$, $ContLabel$)
      \Goto{MR\_get\_global\_work}
    \EndIf
  \Else
    \State $got\_spark \gets$ pop\_spark($this\_context.spark\_deque$,
        \&$spark$)
    \If{$got\_spark$}
      \Goto{$spark.code\_label$}
    \Else
      \If{$ST.orig\_context = this\_context$}
         \State suspend($this\_context$)
         \State $this\_context.resume\_label \gets ContLabel$
         \State $this\_context \gets$ NULL
      \EndIf
      \Goto{MR\_get\_global\_work}
    \EndIf
  \EndIf
\EndProcedure
\end{algorithmic}
\caption{MR\_join\_and\_continue}
\label{alg:join_and_continue_ws1}
\end{algorithm}

\plan{Barrier code}
A context accesses its own local spark queue in the \joinandcontinue barrier
introduced in Section \ref{sec:original_scheduling}.
The new version of \joinandcontinue is shown in Algorithm
\ref{alg:join_and_continue_ws1}.
The first difference between this version and the previous one,
is that this version is lock free.
All the synchronisation is performed by atomic CPU instructions, memory
write ordering and one busy loop.
The number of outstanding contexts in the synchronisation term is
decremented and the result is checked for zero atomically.\footnote{
    On x86/x86\_64 this is a ``lock dec'' instruction, we read the zero flag
    to determine if the decrement caused the value to become zero.}
The next difference is around line 17,
when the context pops a spark of its stack, it does not check if the spark
was created by a callee's parallel conjunction.
Because all sparks are placed on the context local spark deques and
sparks are stolen from the cold end of the deque,
if there is an outstanding conjunct then its spark will be on the top of the
deque,
otherwise the deque will be empty.

The next difference is on lines 18--19 and 7--10,
This code prevents a race condition which could otherwise occur as follows.
A conjunction of two conjuncts is executed in parallel.
The original context, $C_{Orig}$,
enters the barrier, decrements the counter from two to one,
and because there is another outstanding conjunct it executes else
branch on line 13.
At almost the same time another context, $C_{Other}$,
enters the barrier, decrements the counter and finds that there is no more
outstanding work.
$C_{Other}$ attempts to schedule $C_{Orig}$ on line 10.
However at this point $C_{Orig}$ has not been suspended, this would cause an
inconsistent state and memory corruption.
Therefore lies 7--10 wait until $C_{Orig}$ has been suspended, which it
indicates by setting its resume label \emph{after} it was suspended at lines
18--19.
Lines 18--19 must also include memory write ordering.
These are the only changes that we have made to \joinandcontinue since shown
on page \pageref{alg:join_and_continue_peterw}.

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{MR\_get\_global\_work}{}
  \State acquire\_lock($MR\_runqueue\_lock$)
  \Loop
    \If{$MR\_exit\_now$}
      \State release\_lock($MR\_runqueue\_lock$)
      \State MR\_destroy\_thread()
    \EndIf
    \State $ctxt \gets$ MR\_get\_runnable\_context()
    \If{$ctxt$}
      \State release\_lock($MR\_runqueue\_lock$)
      \If{$current\_context$}
        \State MR\_release\_context($current\_context$)
      \EndIf
      \State MR\_load\_context($ctxt$)
      \Goto $ctxt.resume$
    \EndIf
    \If{$MR\_num\_outstanding\_contexts < MR\_max\_contexts$}
      \State $result \gets$ MR\_try\_steal\_spark(\&$spark$)
      \If{$result$}
        \State release\_lock($MR\_runqueue\_lock$)
        \If{$\neg current\_context$}
          \State $ctxt \gets$ MR\_get\_free\_context()
          \If{$\neg ctxt$}
            \State $ctxt \gets$ MR\_create\_context()
          \EndIf
          \State MR\_load\_context($ctxt$)
        \EndIf
        \State $MR\_parent\_sp \gets spark.parent\_sp$
        \State $ctxt.thread\_local\_mutables \gets
          spark.thread\_local\_mutables$
        \Goto $spark.resume$
      \EndIf
    \EndIf
    \State timed\_wait($MR\_runqueue\_cond, MR\_runqueue\_lock$, $timeout$)
  \EndLoop
\EndProcedure
\end{algorithmic}
\caption{MR\_get\_global\_work}
\label{alg:MR_get_global_work_wsinitial}
\end{algorithm}

\plan{Other changes to the idle loop?}
We have also modified \getglobalwork to use spark stealing,
the new code is shown in Algorithm \ref{alg:MR_get_global_work_wsinitial}.
On line 15 we no longer dequeue a spark from the global spark queue.
Now a call to \trystealspark (see below) is used.
Also as spark creation does not affect the runqueue condition,
engines must wake up and check if there is any parallel work available.
This is done on line 26 using a call to \code{timed\_wait} with a
configurable timeout (it defaults to 2ms).
We have not attempted to find the best value for the timeout,
in fact we avoid it in Section \ref{sec:idle_loop}.

\plan{Accessing the array of deques}
Engines that attempt to steal a spark must have access to all the spark
deques.
We do this with a global array of pointers to contexts' deques.
When a context is created,
the runtime system attempts to add the context's deque to this array by
funding an empty slot,
one containing a null pointer,
and writing the pointer to the context's deque to that index in the array.
If there is no unused slot in this array, the runtime system will resize the
array.
When the runtime system destroys a context it writs \NULL to that context's
index in the deque array.
To prevent concurrent access from corrupting the array these operations are
protected by the $MR\_spark\_deques\_lock$ lock.

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{MR\_try\_steal\_spark}{$spark\_ptr$}
  \State acquire\_lock($MR\_spark\_deques\_lock$)

  \State $max\_attempts \gets$ min($MR\_max\_spark\_deques$,
    $MR\_worksteal\_max\_attempts$)

  \For{$attempt = 1$ to $max\_attempts$}
    \State $MR\_victim\_counter$++
    \State $deque \gets
       MR\_spark\_deques$[$MR\_victim\_counter \bmod MR\_max\_spark\_deques$]
    \If{$deque \neq$ NULL}
      \State $result \gets$ \steal($deque$, $spark\_ptr$)
      \If{$result$}
        \State release\_lock($MR\_spark\_deques\_lock$)
        \State \Return $true$
      \EndIf
    \EndIf 
  \EndFor
  \State release\_lock($MR\_spark\_deques\_lock$)
  \State \Return $false$
\EndProcedure
\end{algorithmic}
\caption{MR\_try\_steal\_spark}
\label{alg:try_steal_spark_initial}
\end{algorithm}

\plan{Thief behaviour}
The algorithm for \trystealspark is shown in 
Algorithm \ref{alg:try_steal_spark_initial}.
It must also acquire the lock before using the array.
A thief may need to make a number of attempts before it successfully finds a
deque with work it can steal,
even if a deque does contain work, only one of several thieves may
successfully steal the work.
Line 2 sets the number of attempts to use, wich is either the user
configurable $MR\_worksteal\_max\_attempts$ or the size of the array,
whichever is smallest.
A loop (beginning on line 3) attempts to steal work until it succeeds or it
has made $max\_attempts$.
We use a global variable, $MR\_victim\_counter$,
to implement a round-robin selection of the victim.
Once a thief either steals work, or gives up, the next thief will resume
this round robin selection of victims.
On lines 7 and 8 we attempt to steal work from a victim,
if the deque pointer array slot is non-null.
If the call to \steal succeeded it will have written spark data into the
memory pointed to by $spark\_ptr$,
then \trystealspark releases the lock and returns true.
Eventually \trystealspark may give up (lines 13--14),
it will release the lock and return false.

\plan{Work stealing fixes the premature scheduling decision problem}
All sparks created by a context are placed on its local spark deque.
Sparks are only removed to be executed in parallel when an idle engine
executes \trystealspark.
Therefore
the decision to execute a spark in parallel is only made once an engine is
idle and able to run the spark.
We expect this to correct the premature scheduling decision problem we
described in Section \ref{sec:original_scheduling_performance}.

\input{tab_work_stealing_initial}

\plan{Benchmark}
We benchmarked our initial work stealing implementation with the mandelbrot
program from previous sections.
Table \ref{tab:work_stealing_initial} shows the results of our benchmarks.
The first and their row groups are the same results as the first and second
row groups from Table \ref{tab:2009_left_nolimit} respectively.
They are included to allow for easy comparison. 
The second and fourth row groups where generated with a newer version of
Mercury that implements work stealing as described above.\footnote{
    The version of Mercury used is slightly modified,
    due to an oversight we had forgotten to include the context limit
    condition in \getglobalwork.
    The version we benchmarked includes this limit and matches the
    algorithms shown in this section.}
Table \ref{tab:work_stealing_initial} includes speedup figures as some
previous tables have.

The first conclusion that we can draw from the results is that
left recursion with work stealing is much faster than left recursion without
work stealing.
Therefore work stealing has definitely solved the premature spark scheduling
problem.

Also in the left recursion with work stealing case we can see that the
context limit no-longer affects performance.
There is no significant difference between the rows with 8, 16 or 32
contexts.
The case for four contexts does limit performance,
this is an artifact of our experimental design.
The Mercury runtime system allows the configuration of the maximum
number of contexts per engine, and does not allow the user to specify the
absolute number of contexts.
Therefore in the case for a maximum of four contexts, the number four was
divided,
with integer division,
by the number of engines in use before being passed to Mercury resulting in
the number one.
Mercury's runtime system then multiplied this by the number of engines.
After each engine but the initial one had stolen a spark and created a
context for it, there where $N$ contexts in use for $N$ engines.
Next time an engine executed \getglobalwork it would not attempt to steal a
spark as doing so would exceed the context limit.
As the default value for the number of contexts per engine is two,
this is not a problem under normal circumstances.

In the case for right recursion with high context limits, work stealing
performs better.
This is somewhat expected since work stealing is regarded to be a very
efficient way of managing parallel work on a shared memory system.
However we did not expect to get such a significant improvement in
performance.
In the non work stealing system, a spark is placed on the global spark queue
if both the context limit has not been reached, and there is an idle engine.
We believe that the premature scheduling problem was also affecting right
recursion.
There may not be an idle engine when a spark is being created,
however an engine may become idle soon and wish to execute a spark.
In the work stealing case all sparks are placed on local deques and an idle
engine will attempt to steal work when it is ready ---
the decision to run a spark in parallel is no-longer premature.
This is also the same reason as to why work stealing benefits left
recursive programs so much,
left recursive programs simply happen to have a pathological case of
the premature scheduling problem.

There is a second observation we can make about right recursion.
In the cases for 256 or 512 contexts work stealing performs worse.
We believe that the original results are faster because more work was done
in parallel.
When a context spawned off a spark it would often find that there was not a
free engine, and place the spark on its local queue.
After completing the first conjunct of the parallel conjunction it would
execute \joinandcontinue.
Where it would find that it had a spark on its local stack and it would
execute this spark directly.
When a spark is executed directly the existing context is re-used and so the
context limit does not increase as quickly.
The work stealing version does not do this,
it always places the spark on its local stack and almost always
another engine will steal that spark,
and the creation of a new context will cause the context limit to rise.
The work stealing program is more likely to reach the context limit earlier,
whereby it will be forced into sequential execution.

\section{Reorder independent conjunctions}

\status{Not written}

\plan{Avoid the right-recursion problem by reordering independent conjunctions.}

\section{Idle loop and better work stealing}
\label{sec:idle_loop}
\status{Not written}

\plan{Describe the problem with the current algorithm.}
Engines wake up and periodically check for work by attempting to
steal work,
firstly, this means that there can be up to a 2ms (average 1ms) delay before
a spark is executed.
secondly, this checks for work too often, wasting resources.

\plan{Solution, wake engines for different types of work}
We modified the RTS so that waking an engine is easy, and it can be given a
message so that it knows where to look for work.

\plan{A lot of work went into preventing deadlocks due to race conditions,
a thread that is not yet sleeping if notified must wake up immediately.}

\plan{Extra benefit: when an engine is woken it can be told directly where
to find work, or be given the work directly.}

\plan{Extra benefit: we can be selective about which engine to wake,
while not implemented fully, we can wake a `nearby' engine so that
we can avoid communication between dies or sockets.}

\plan{Show the algorithm for the new idle loop.}
Note that we execute contexts before sparks,
this is more-likely to produce futures and it may reduce memory consumption.

\subsection{Revised work stealing implementation}

\plan{Work stealing attempts}

\plan{Describe work stealing timeout.}


\plan{Describe problems with associating stacks with contexts}
The number of stacks varies,
Stealing uses a global lock to determine which stack to steal from.

\plan{Prove that even though there are N engines and M contexts and M may be
larger than N, that there will be at most N of the M contexts with work on
their queues}
Therefore:
Stealing is unnecessary complicated.
In pathological cases many attempts can be made without success,
a thief may give up even though there is parallelism.

\plan{We associate stacks with engines}
This removes the above problems how.

\plan{Show stealing algorithm there are any,}
Find out if I started tracking stealing per engine or not.

\plan{Show how this is safe.}
When a parallel conjunction's barrier is executed and a conjunct is
outstanding, if its spark is on this engine's stack it must be at the top of
the stack.
This invariant should be kept because the context can be re-used if
'compatible' work is found at the top of the spark stack.
This invariant is soft.

\input{tab_work_stealing_revised}

\plan{Benchmark}

\section{Thread pinning}
\label{sec:thread_pinning}
\status{Not written}

\plan{Explain why we want $P$ engines when there are $P$ processors}

\plan{Explain briefly how we detect how many processors there are,}
Explain that this method is good because it is (mostly) cross platform.

\plan{Explain why we want thread pinning.}
I wonder if there's any relevant literature.

\plan{Explain how we get thread pinning.}
This method is also (mostly) cross platform.

\plan{What about SMT, not all processors are equal.}
This does not matter when we are creating $P$ engines.
But it does matter when we create less than $P$ engines,
explain how bad CPU assignments are sub-optimal.

\plan{How do we handle SMT}
This uses a support library, which is cross platform, provided that it is installed.
We fall back to setcpuafinity() when it is not.

\paul{I am not going to talk about busy waiting since I have not written the
runtime system in a way that I can test or change this easily.}

\section{Proposed scheduling tweaks}
\label{sec:proposed_tweaks}
\status{Not written, May move to TS chapter}

I really think that this section will move to the \tscope chapter,
it will have more in common with that chapter and more data will be
available.
Secondly, \tscope can be used with micro-benchmarks to measure the
average costs of certain operations in the RTS.
I will not write it until at least the rest of this chapter is finished.

%\section{Proposed kernel support to manage processor resources}
%\label{sec:kernel_scheduling_help}
%
%\status{This may not be worth discussing until someone actually does it}
%
%Should I describe our proposal for OS kernel's to help
%applications with how many threads to use.
%GCD is related but does not fit into a language runtime system so
%easily~\cite{apple_gcd}.
%See also N:M threading.

%\section{Spare text}
%
%\status{This text will be moved up into one of the work stealing sections
%once those sections are ready}

%The spark is added to a global run queue of sparks, or if that queue is too
%full, because there's already enough parallelism,
%then the spark is added to a queue owned by the current context.
%\citet{wang:2006:thesis} intended to use the local queues for work stealing
%but had not completed his implementation,
%The work-stealing dequeue structure
%is described in \citet{Chase_2005_wsdeque}.
%see Chapter \ref{chap:rts} for details.
%
%and finds that spark is still at the head of its queue,
%it will pick it up and run it itself.
%This is a useful optimisation,
%% it is also really well-known.
%since it avoids using a separate context in the relatively common case
%that all the other CPUs are busy with their own work.
%This optimisation is also useful since it can avoid the creation of superfluous
%contexts and their stacks.
%

%When an engine becomes idle, it will first try
%to resume a suspended but runnable context if there is one.
%If not, it will attempt to run a spark from the global spark queue.
%If it successfully finds a spark, it will allocate a context,
%and start running the spark in that context.

% XXX: Mention global spark queue and spark sheduling above.
% XXX:

%Barrier code is placed at the end of each conjunct,
%this is named \code{join\_and\_continue} (Figure \ref{fig:par_conj}).
%This code starts by atomically decrementing the number of outstanding
%conjuncts in the conjunction's syncterm and checking the result for zero
%(the whole operation is thread-safe, not just the decrement).
%Algorithm \ref{alg:join_and_continue} shows the pseudo code for
%join\_and\_continue.


%XXX
%Parallel conjunctions are evaluated as described in Section
%\ref{sec:backgnd_merpar}.
%Sparks are added to the global spark queue if the global queue has room,
%otherwise they are pushed onto a context-local spark stack.
%Creating a lot of parallel work and using a single global work queue can be
%pesimistic,
%the queue itself can become a bottleneck:
%when many processors try to access it the same location in memory there will
%be many cache misses and delays.
%This is why \citet{wang:2006:thesis} choose to introduce local queues,
%and place work on them when there is a surplus of work on the global queue.
%
%When a engine finishes executing a context and reaches the barrier at the
%end of a parallel conjunct,
%it will check the context's local spark stack for any other work and attempt
%to execute it.
%Otherwise, it will either save the context to resume later or release the
%context before checking the global spark queue and global context queue.
%
%XXX Algorithm.
%
%Deciding too early, results.
%
