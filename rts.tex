
\status{This chapter is currently WIP}

Early in the project
we tested two manually parallelised programs:
a raytracer and a mandelbrot image generator.
Both of them have a single sagnificant loop
whose iterations are independent of one-another.
We parallelised this loop in each program as it was the best place to
introduce parallelism,
but we did not get the speedups that we expected.
For these programs, automatic parallelisation cannot acheive
a greater speedup than we have acheived with manual parallelism.
Therefore,
we chose to address the performance problems with manual parallelism
before we worked on automatic parallelism.

The chapter is structured as follows:
Section \ref{sec:gc} describes the garbage collector's effect on parallel
execution performance and how tuning some parameters of the garbage
collector can improve performance.
Section \ref{sec:old_scheduling} describes how the existing runtime
system schedules sparks.
It provides background material for Section
\ref{sec:old_scheduling_performance}
which benchmarks the runtime system and describes a sagnificant problem with
spark scheduling.
We address those problems by introducing work stealing in Section
\ref{sec:work_stealing}.
This section is seperated into two sub-sections
which describe the initial and revised versions of the work stealing
implementation.
Section \ref{sec:work_stealing} also includes benchmarks that show
how work stealing fixes the spark scheduling problem.
We made a number of improvements to the way that Mercury engines are created,
this includes thread pinning and support for SMT systems,
we describe these improvements in Section \ref{sec:thread_pinning}.
Section \ref{sec:idle_loop} describes our changes to how engines idle,
and how and when they wake up to execute parallel work.
Finally, in Section \ref{sec:kernel_scheduling_help}
we describe an area of research that would improve resource usage when
multiple instances of a parallel Mercury program are executing in parallel.

\section{Garbage collection tweaks}
\label{sec:gc}

\status{Planned, finish prose and may need a benchmark}
\plan{Describe Mercury's behaviour WRT GC}
One of the sources of poor parallel performance is the behaviour of the
garbage collector.
Like some declarative languages,
Mercury does not allow destructive update, also known as mutation.%
%\footnote{
%    Destructive update is allowed via support for mutable variables.
%    Their use does not interfere with parallelism as they are used in
%    conjunction with impurity.
%    The compiler will not parallelise impure goals.}
Therefore a calculation usually returns its value in newly allocated memory
rather than modifying the memory of one of its parameters.
This means that Mercury programs often have a high rate of allocation,
which places a lot of stress on the garbage collector.
Therefore,
allocation and garbage collection have a strong impact on a program's
performance.
This effect usually becomes more significant when parallelism is introduced into a
program.

\plan{Introduce Boehm, \& details: conservative, mark and sweep, stop the
world, parallel marking.}
Mercury uses the Boehm-Demers-Weiser Conservative Garbage Collector (Boehm GC)
\citep{boehm_gc},
which is a conservative mark and sweep collector.
Boehm GC supports parallel programming.
It will stop all the threads (\emph{stop the world}) during its marking
phase.
It also supports parallel marking,
it will use its own set of pthreads to perform parallel marking.

\plan{Introduce collector time, mutator time.}
When discussing garbage collection,
a program's execution time is divided into two alternating phases:
collection time, which is when Boehm GC performs marking,
and mutator time, which is when the Mercury program runs.
The name mutator time refers to time that mutations (changes) to memory
structures are permitted.
The collector may also perform some actions,
such as sweeping,
concurrently with the mutator.

\plan{Describe theory of GC performance.}
Amdahl's law~\citep{amdahl} describes the maximum speedup that
can theoroetically  be achieved by parallelising some part of a program,
and allowing the rest of the program to run sequentially.
When sequential marking is used then can use Amdahl's
law to predict the speedup of our programs.
For example, consider a program with a runtime of 20 seconds
which can be seperated into one second of collector time and 19 seconds
of mutator time.
The sequential execution time of the program is $1 + 19 = 20$.
If we parallelise mutator part of the program and do not parallelise the
collector then the minimum parallel execution time is $1 + 19{\div}P$
for $P$ processors.
Using four processors the theoretical best speedup we can achieve is:
$(1 + 19) \div (1 + 19\div4) = 3.48$.
17\% of the parallel runtime ($1 + 19\div4$) is collector time.
If we use a machine with 100 processors then this becomes:
$(1 + 19) \div (1 + 19\div100) = 16.8$;
with 84\% of the runtime spent in the collector.
Adding more processors to the system means that 
As the number of processors increases a larger proportion of time is
wasted waiting for collection to complete.

\plan{Discuss predictions regarding parallel marking, locking and thread
local heaps.}
In order to reduce this problem,
\citet{boehm-gc} included parallel marking support in their collector.
Parallel garbage collection is a continuing area of research
and the Boehm GC project has only modest goals for multicore scalability.
Therefore,
parallel marking should partially remove the bottleneck described above.
Thread safety has two other significant performance costs:
The first is that
one less CPU register is available to GCC's code generator when compiling
parallel Mercury programs (Section \ref{sec:backgnd_merpar}).
The second reason is that
memory allocation routines must use locking to protect shared data
structures,
this will slow down allocation.
Boehm GC's authors recognised this problem and
added support for thread-local resources such as free lists.
Therefore,
during memory allocation a thread can use its own free lists rather than
locking a global structure.
From time to time, a thread will have to retrieve new free lists
from a global structure and will need to perform locking then,
but this cost will be amortised across a number of memory allocations.

\paul{Check that this is all independent parallelism}
\plan{Describe icfp2000, mandelbrot\_lowalloc and mandelbrot\_highalloc
programs.}
One of our benchmarks is a raytracer developed for the
ICFP programming contest in 2000.
For each pixel in the image the raytracer casts a ray into the scene to
determine what colour to paint that pixel.
Two nested loops build the pixels for the image:
the outer loop iterates over the rows in the image and 
the inner loop interates over the pixels in each row.
We manaully parallelised the program by introducing an independent parallel
conjunction into the outer loop;
indicating that rows can be drawn in parallel.
The raytracer uses many small structures to represent vectors and a pixel's
colour.
It is therefore memory allocation intensive.
Since we suspected that garbage collection was having a negative impact on
performance we developed a mandelbrot image generator.
The mandelbrot image generator has a similar structure to the raytracer,
it draws an image using two nested loops as above.
The mandelbrot image is drawn on the complex number plane.
A complex number $C$ is in the mandelbrot set if
$|N|$ is never larger than 2 where $N_{i+1} = N_{i}^2 + C$ and $N_0 = 0$.
For each pixel in the image the program tests if the pixel's coordinates are
in the set,
The pixel's colour is choosen based on how many interations are needed
before $|N| > 2$,
or black if the test survived 5,000 interations.
We parallelised this program the same way as we did the raytracer:
by introducing an independent parallel conjunction into the outer loop.
We created two versions of this program,
the first version stores coordinates and complex numbers on the stack and in
regsiters.
Therefore, this version has a low rate of memory allocation.
We call this version `mandelbrot\_lowalloc'.
The second version of the mandelbrot program is called
`mandelbrot\_highalloc'.
This program represents its coordinates and complex numbers as structures on
the heap.
Therefore,
it has a higher rate of memory allocation.

\begin{table}
\begin{center}
\begin{tabular}{l|r|rrrr}
\Cbr{Program} & \Cbr{GC Markers} &
\multicolumn{4}{|c}{Mercury Engines} \\
\Cbr{} & \Cbr{} & \C{1} & \C{2} & \C{3} & \C{4} \\
\hline
\multirow{6}{*}{raytracer} &
 Sequential & 33.1 (1.13) & -          & -          & - \\
&TS, no par & 37.4 (1.00) & -          & -          & - \\
&1          & 37.7 (0.99) & 28.0 (1.34)& 24.8 (1.51)& 23.7 (1.58) \\
&2          & 32.0 (1.17) & 21.6 (1.73)& 18.1 (2.07)& 16.7 (2.24) \\
&3          & 29.6 (1.26) & 19.7 (1.90)& 16.3 (2.29)& 14.5 (2.59) \\
&4          & 29.1 (1.29) & 18.9 (1.98)& 15.3 (2.45)& 13.7 (2.73) \\
\hline
\multirow{6}{*}{mandelbrot\_lowalloc} &
 Sequential & 15.3 (1.00) & -          & -          & - \\
&TS, no par & 15.3 (1.00) & -          & -          & - \\
&1          & 15.3 (1.00) & 7.7 (1.99) & 5.1 (2.98) & 3.9 (3.94) \\
&2          & 15.3 (1.00) & 7.7 (1.99) & 5.1 (2.98) & 3.9 (3.94) \\
&3          & 15.3 (1.00) & 7.7 (1.99) & 5.1 (2.98) & 3.9 (3.94) \\
&4          & 15.3 (1.00) & 7.7 (1.99) & 5.1 (2.98) & 3.9 (3.94) \\
\hline
\multirow{6}{*}{mandelbrot\_highalloc} &
 Sequential & 36.9 (1.25) & -          & -          & - \\
&TS, no par & 46.3 (1.00) & -          & -          & - \\
&1          & 47.4 (0.98) & 33.2 (1.39)& 26.7 (1.73)& 23.6 (1.96) \\
&2          & 44.7 (1.03) & 29.0 (1.59)& 22.3 (2.08)& 19.0 (2.44) \\
&3          & 43.0 (1.08) & 27.3 (1.69)& 21.0 (2.20)& 17.7 (2.62) \\
&4          & 42.7 (1.08) & 26.7 (1.73)& 20.1 (2.30)& 16.9 (2.73) \\
\end{tabular}
\end{center}
\paul{Consider drawing charts for raytracer and mandelbrot so that releative
performance between different scores can be seen easily.}
\caption{Parallelism and garbage collection}
\label{tab:gc}
\end{table}

\paul{Need to fix the mandelbrot benchmarks, I setup the tests incorrectly}
\plan{Benchmark data.}
Table \ref{tab:gc} shows benchmark results for these programs
with different numbers of Mercury engines and garbage collector threads.
All benchmarks have an initial heap size of 16MB.
Each result is measured in seconds and represents the mean of eight samples.
The first two rows of each group of rows show the time for a sequential
non thread safe version of the program,
and a sequential thread safe version
(the programs where executed in such a way so that they did not execute a
parallel conjunction).
Each of the other four rows represent the program's runtime with
1--4 marker threads (vertical) and
1--4 Mercury engines (horizontal).
The numbers in parentheses show the relative speedup when compared to the
``TS, no par'' option for the same program.
\label{cabsav}
All our benchmarking was performed on `cabsav',
a hand built system using a Intel i7-2600K CPU 
with 16GB of memory.
Cabsav runs Debian/GNU Linux 6
with a 2.6.32-5-amd64 Linux kernel and GCC 4.4.5-8.
The garbage collection benchmarks were collected using a recent version of
Mercury (rotd-2012-04-29);
the other improvements discussed in this chapter and the loop control
transformation (Chapter \ref{chap:loop_control})
are enabled for these tests.
Using the improved version of Mercury allows us to observe the effects of
garbage collection without other performance problems affecting the results.

\plan{Describe performance in practice.}
The raytracer program benefits from parallelism in both Mercury and the
garbage collector.
Using four engines speeds the program up by a factor of 1.58,
compared to 1.29 for four marker threads.
When using four engines and four marker threads the raytracer
achieves a speedup of 2.73,
this likely indicates that the program's high allocation rate causes
contention on the garbage collector's locks,
although we investigate another likely cause below.
mandelbrot\_lowalloc does not see any benefit from parallel marking.
It achieves very good speedups from multiple Mercury engines.
We know that this program has a low allocation rate
but is otherwise parallelised in the same way as raytracer.
Therefore,
these results support the hypothosis that garbage collection makes it more
difficult to achieve good speedups when parallelising programs.
mandelbrot\_highalloc, which stores its data on the heap,
sees similar trends in performance to that of the raytracer,
aid is universally slower than the first mandelbrot program.
mandelbrot\_highalloc achieves a speedup of 1.96 when using four Mercury
engines compared to 1.58 for the raytracer.
It also achieves a speedup of 1.08 when using four marker threads compared
to 1.29 for the raytracer.

\begin{table}
\begin{center}
\begin{tabular}{l|rr|r}
Program     & Allocations   & Total alloc'd bytes & Alloc rate (M alloc/sec) \\
\hline
raytracer   &   561,431,515 &       9,972,697,312 & 16.9 \\
mandelbrot\_highalloc
            & 1,829,971,662 &      29,275,209,824 & 49.6 \\
\end{tabular}
\end{center}
\caption{Memory allocation rates}
\label{tab:mem_alloc_rate}
\end{table}

We can see that mandelbrot-heap benefits from parallelism in Mercury
more than raytracer does,
similarly mandelbrot-heap benefits less from parallelism in the garbage
collector than raytracer does.
These results initially lead us to beleive that
mandelbrot\_highalloc is less allocation intensive
than raytracer,
but this is not true.
Using Mercury's deep profiler we measured the number and size of memory
allocations and how much memory was used by each program.
This is shown in Table \ref{tab:mem_alloc_rate}.
The rightmost column in this table gives the allocation rate,
measured in million allocations per second;
it is calculated by dividing the number of allocations in the second column
by the average sequential execution time reported in Table \ref{tab:gc}.
We noticed that while benchmarking the raytracer that if we rendered the
image upside-down (by iterating over the rows backwards)
we would get different performance results.
This is because the top of the image we used contains sky,
which was a flat colour,
and the bottom of the image will contains ground,
which has more detail.
Rendering the ground will natrually be more complicated and require more
memory allocation than rendering the sky.
Memory allocation in one part of a program can affect the performance of
allocation and collection in another part of the program,
which is why performance changed when we rendered an image upside down.
Simiarly,
the mandelbrot image has sections that are more complicated than others.
The apparent difference between the raytracer and mandelbrot\_highalloc
benchmarks are likly to be due to changing allocation rates throughout their
execution.


\plan{Compare performance vs Amdahl's predictions for sequential marking.}
\paul{XXX: Use threadscope to determine what percentage of the time programs
spend in the garbage collector and compare to prediction}

\plan{Parallel marking performance observations.}
not that parallelisation is not easy.

\begin{table}
\begin{center}
\begin{tabular}{r|rr|rrrr}
\Cbr{Initial} &
\multicolumn{2}{|c|}{Sequential} &
\multicolumn{4}{|c}{Mercury Engines} \\
\Cbr{heap size} & \C{no TS}   & \Cbr{TS}    & \C{1}       & \C{2}       & \C{3}       & \C{4} \\
\hline
\hline
\multicolumn{7}{c}{raytracer} \\
\hline
1MB    & 32.9 (1.14) & 37.4 (1.00) & 37.5 (1.00) & 28.2 (1.33) & 24.9 (1.50) & 23.4 (1.60) \\
16MB   & 33.1 (1.13) & 37.4 (1.00) & 37.7 (0.99) & 28.0 (1.34) & 24.8 (1.51) & 23.7 (1.58) \\
32MB   & 34.3 (1.09) & 38.4 (0.97) & 38.5 (0.97) & 28.8 (1.30) & 25.4 (1.47) & 23.5 (1.60) \\
64MB   & 33.4 (1.12) & 39.8 (0.94) & 39.7 (0.94) & 30.5 (1.23) & 26.8 (1.40) & 25.5 (1.47) \\
128MB  & 22.7 (1.65) & 27.5 (1.36) & 27.5 (1.36) & 18.9 (1.98) & 16.1 (2.32) & 15.3 (2.45) \\
256MB  & 18.7 (2.01) & 22.5 (1.66) & 22.6 (1.65) & 14.2 (2.64) & 11.1 (3.38) &  9.8 (3.82) \\
384MB  & 17.9 (2.09) & 21.3 (1.75) & 21.5 (1.74) & 12.7 (2.94) &  9.8 (3.80) &  8.4 (4.44) \\
512MB  & 17.6 (2.13) & 20.9 (1.79) & 21.0 (1.79) & 12.1 (3.09) &  9.2 (4.08) &  7.8 (4.78) \\
\hline
\hline
\multicolumn{7}{c}{mandelbrot\_heap} \\
\hline
1MB    & 38.7 (1.19) & 48.9 (0.95) & 50.9 (0.91) & 34.0 (1.36) & 27.4 (1.69) & 24.7 (1.87) \\
16MB   & 36.9 (1.25) & 46.3 (1.00) & 47.4 (0.98) & 33.2 (1.39) & 26.7 (1.73) & 23.6 (1.96) \\
32MB   & 35.6 (1.30) & 45.0 (1.03) & 48.3 (0.96) & 31.5 (1.47) & 25.0 (1.85) & 21.9 (2.11) \\
64MB   & 33.8 (1.37) & 44.2 (1.05) & 43.7 (1.06) & 28.8 (1.61) & 22.7 (2.04) & 19.6 (2.36) \\
128MB  & 32.4 (1.43) & 44.6 (1.04) & 42.7 (1.08) & 26.4 (1.75) & 20.0 (2.31) & 16.9 (2.73) \\
256MB  & 31.5 (1.47) & 41.4 (1.12) & 42.6 (1.09) & 24.5 (1.88) & 18.3 (2.53) & 15.0 (3.08) \\
384MB  & 31.2 (1.49) & 41.9 (1.10) & 41.2 (1.12) & 24.4 (1.90) & 17.6 (2.63) & 14.4 (3.21) \\
512MB  & 30.8 (1.50) & 41.3 (1.12) & 42.0 (1.10) & 23.6 (1.96) & 17.4 (2.67) & 14.1 (3.27) \\
\end{tabular}
\end{center}
\caption{Varying the initial heapsize in parallel Mercury programs.}
\label{tab:heapsize}
\end{table}

\plan{New data, vary the heap size,}
The more Mercury engines we use the more garbage collection hurts,
we can recover this by increasing the heap size to avoid GC'ing more often.

\plan{Criticise cache behaviour WRT parallel marking,
Thanks Simon Marlow.}
\paul{XXX: threadscope will also support our argument here.}

\plan{Discuss local heaps for threads, and their reliability problems.}
\paul{Stress this point, convince the reader that I can not test this}
Boehm GC has a few tunable parameters, one of these, \texttt{HBLKSIZE},
configures the size of a local heap among other things.
In some tests this did improve performance but we also found it to cause
reliability problems.
Therefore it cannot be used practically and we do not report on it here.


\section{Original spark scheduling algorithm}
\label{sec:old_scheduling}

\status{Checking plan for this section}

\plan{Global spark queue and Contention wrt global queue}
Mercury has a global spark queue.
The runtime can easily schedule a spark by placing it on the end of the
global queue.
An idle engine can take a spark from the beginning of the queue to work on
it.
The global spark queue must be protected by locks to prevent cuncurrent
access from currupting the queue.
The global spark queue and its lock can easily become a bottleneck when many
enignes content for access to the global queue.

\plan{Local spark stack --- relieves contention on global queue}
\citet{wang-hons} expected this problem and created context local spark stacks
to avoid contention on the global queue.
Furthermore, the local spark stacks do not require locking.
When a parallel conjunction spawns off a spark it can place the spark either
at the end of the global spark queue or at the top of its local spark stack.
\plan{Spark scheduling decision}
The runtime system appends the spark to the end of the global queue if:
an engine is idle, and
the number of contexts in use plus the number of sparks on the global queue
does not exceed the maximum number of contexts permitted.
Otherwise,
the runtime system pushes the spark onto the top of the context's local
spark stack.
Wang's scheduling decision has two aims:
Firstly, to reduce the contention on the global queue,
especially in the common case that there is enough parallel work.
This also reduces the amount of locking.
Secondly, the scheduling decision prevents the creation of too many contexts
as their stacks consume memory.
Globally scheduled sparks may be convered into contexts so they are also
included in this limit.
Note that sparks placed on the global queue are executed in a
first-in-first-out manner, while
sparks placed on a context's local stack are executed in a
last-in-first-out manner.

\begin{algorithm}
\begin{algorithmic}
\Procedure{join\_and\_continue}{$ST, ContLabel$}
  \State aquire\_lock($ST.lock$)
  \State $ST.num\_outstanding \gets ST.num\_outstanding - 1$
  \If{$ST.num\_outstanding = 0$}
    \If{$ST.parent = this\_context$}
      \State release\_lock($ST.lock$)
      \Goto{$ContLabel$}
    \Else
      \State schedule($ST.parent$)
      \State release\_lock($ST.lock$)
      \Goto{get\_global\_work}
    \EndIf
  \Else
    \State $spark \gets$ pop\_compatible\_spark
    \If{$spark$}
       \State release\_lock($ST.lock$)
       \Goto{$spark.code\_label$}
    \Else
      \If{$ST.parent = this\_context$}
         \State suspend($this\_context$)
         \State $this\_context \gets$ NULL
      \EndIf
      \State release\_lock($ST.lock$)
      \Goto{get\_global\_work}
    \EndIf
  \EndIf
\EndProcedure
\end{algorithmic}
\caption{join\_and\_continue}
\label{alg:join_and_continue_peterw}
\end{algorithm}

\paul{Make the pop compatible spark decision clearer.}

\plan{barrier code, this is used to explain the right recursion problem.}
%As an engine finishes executing a parallel conjunct,
%it will execute the the barrier at the end of the conjunct,
%named \joinandcontinue and shown in
%Algorithm \ref{alg:join_and_continue_peterw}.
%If it is known that the parallel conjunction has only been executed by
%one context,
%then it is safe to use a version of the barrier code that does not use
%locking,
%this optimisation is not shown as it is equivalent and not relevant to
%our discussion,
%it is described only for completeness.

%The algorithm begins by checking if there are any outstanding conjuncts in
%the parallel conjunction.
%If there are none and the current context is the parent
%context,
%then execution jumps to the label after the parallel conjunction.
%If the current context is not the parent context then
%we can infer that the parent context is suspended,
%therefore,
%the engine will schedule the parent context, before looking for global work.
%It looks for global work because its local work queue is guaranteed to be
%empty since this conjunction and any nested conjunctions are complete.
%Alternatively, if there are outstanding conjuncts then
%the local spark stack is checked for compatible work ---
%a spark whose parent context is the same as the current syncterm's parent
%context.
%If a compatible spark is at the top of the spark stack then it is removed
%and executed.
%Otherwise,
%if this context is the parent context it must be suspended
%before the engine looks for global work.

\plan{Explain how work begins executing, for completeness}
\paul{Talk about get\_global\_work().}
%An engine looks for global work first by checking the global context run queue.
%If it finds a runnable context and is still holding a context from a
%previous execution, it saves the old context onto the free context list.
%If there are no runnable contexts,
%it will take a spark from the global spark queue,
%and either use its current context to execute the spark,
%or allocate a new context (from the free context list if possible).
%If it is unsuccessful at finding work,
%it will go to sleep using a pthread condition variable and the global run
%queue's lock.
%This condition is used to wake engines when either contexts are added to the
%run queue,
%or sparks are added to the spark run queue.

\section{Prior spark scheduling performance}
\label{sec:old_scheduling_performance}

\status{Checking plan for this section.}

\begin{figure}
\begin{center}
\subfigure[Right recursive]{%
\label{fig:map_right_recursive}
\begin{tabular}{l}
\code{map(\_, [], []).} \\
\code{map(P, [X $|$ Xs], [Y $|$ Ys]) :-} \\
\code{~~~~P(X, Y) \&} \\
\code{~~~~map(P, Xs, Ys).} \\
\end{tabular}}
\subfigure[Left recursive]{%
\label{fig:map_left_recursive}
\begin{tabular}{l}
\code{map(\_, [], []).} \\
\code{map(P, [X $|$ Xs], [Y $|$ Ys]) :-} \\
\code{~~~~map(P, Xs, Ys) \&} \\
\code{~~~~P(X, Y).} \\
\end{tabular}}%
\end{center}
\caption{Right and left recursive map/3}
\label{fig:map_right_and_left_recursive}
\end{figure}

\plan{Describe the problem where we noticed that the context limit could limit
the amount of parallelism that can be exploited.}

\plan{Introduce right recursion.}

\plan{Show performance figures where limit is increased, confirming this problem.}

\plan{Suggest that left recursion might fix this,
(This is what we thought at the time).}
Explain why we think that left recursion will not have this problem.

\plan{Show performance figures for left recursion.}
The left recursive figures are underwhelming, they are worse than right recursion.

\plan{Explain the premature scheduling problem that affects left-recursive programs.}

\plan{A second table of results shows that adjusting the context limit allows left-recursion to run faster.}
\paul{I would like to try yet-another experiment on left recursion where we remove this limit completely}

\plan{Reinforce that these results support the idea that scheduling decisions are made prematurely}



%Figure \ref{fig:map_right_and_left_recursive} shows two alternative, parallel
%implementations of \code{map/3}.
%While their declarative semantics are identical,
%their operational semantics are very different.  In Section
%\ref{sec:backgnd_merpar} we explained that parallel conjunctions are
%implemented by spawning off the tail of the conjunction and executing the
%head directly.
%This means that in the right recursive case (Figure
%\ref{fig:map_right_recursive}), the recursive call is spawned off as a
%spark,
%and that in the left recursive case (Figure \ref{fig:map_left_recursive}),
%the recursive call is executed directly, and the loops \emph{body} is
%spawned off.

\begin{table}
\paul{Make this easier to analyse by making the second column the actual
number of contexts, not contexts per engine.}
\begin{center}
\begin{tabular}{lr|rrrrrr}
\multicolumn{1}{c|}{Recursion type} &
\multicolumn{1}{c|}{Max no.\ of contexts} &
\multicolumn{2}{|c|}{Sequmential} &
\multicolumn{4}{|c}{Parallel w/ $N$ Engines} \\
\Cbr{} & & \C{not TS} & \Cbr{TS} & \C{1}& \C{2}& \C{3}& \C{4}\\
\hline
\multirow{5}{*}{Right} &
 2      & 23.2       & 21.5     & 21.5 & 21.6 & 21.6 & 21.6 \\
&32     & -          & -        & 21.5 & 21.6 & 21.5 & 21.2 \\
&64     & -          & -        & 21.5 & 19.8 & 18.5 & 16.5 \\
&128    & -          & -        & 21.5 & 13.2 &  8.2 &  6.1 \\
&256    & -          & -        & 21.5 & 12.2 &  8.1 &  6.1 \\
\hline
\multirow{5}{*}{Left} &
 2      & 23.2       & 21.5     & 21.5 & 21.5 & 21.5 & 21.5 \\
&32     & -          & -        & 21.5 & 21.5 & 21.5 & 21.5 \\
&64     & -          & -        & 21.5 & 21.5 & 20.5 & 19.0 \\
&128    & -          & -        & 21.5 & 18.5 & 15.8 & 12.9 \\
&256    & -          & -        & 21.5 & 17.8 & 15.6 & 14.1 \\
\end{tabular}
\end{center}
\caption{Right vs.\ left recursion.}
\label{tab:right_v_left}
\end{table}

%Table \ref{tab:right_v_left} shows average elapsed time in seconds for the
%mandelbrot image generating program over 20 samples.
%The program iterates over the rows in the mandelbrot image using
%a left or right recursive parallel
%\code{map/3} predicate such as those in Figure
%\ref{fig:map_right_and_left_recursive},
%the leftmost column in the table indicates which predicate was used.
%The next column describes how many contexts may exist at once per Mercury
%engine.
%Values are omitted in the range 3--31 as they are not interesting:
%they are the same as the case for 2 contexts per engine.
%The next two columns give the runtime for a sequential version of the
%program,
%in this version of the program no parallel conjunctions where used.
%The first of these, labelled ``not TS'',
%is compiled without thread safety;
%the second, labelled ``TS'',
%is compiled with thread safety, meaning that it allows multiple engines to be
%used: requiring a register to point to a pthread's engine structure
%(Section \ref{sec:backgnd_merpar}),
%and compiling the garbage collector for thread safety.
%The following four columns give the runtimes for the parallel mandelbrot
%program with 1--4 Mercury engines.
%
%We can see that the right recursive program performs better than the left
%recursive one.
%The right recursive program, when using 4 cores and at least 128 contexts,
%achieves a speedup of 3.52 compared to the right recursive sequential thread
%safe program.
%At best the left recursive program is sped up by a factor of 1.67.
%The next observation is that as we allow more contexts to reside in memory
%at once,
%we achieve greater speedups.
%The exception to this is the case for left recursion with four engines and
%256 contexts, it is slower than the case for 256 contexts.
%These two results have standard deviations of 6.40 and 6.15 seconds
%respectively meaning that any apparent difference is most likely
%noise in the data.
%All the other results have a standard deviation of less than two seconds,
%most of these are less than half a second.
%We saw the same high variance in these two results when repeating the test.
%We will explain the probable cause for this later in this section.
%
%Parallel conjunctions are evaluated as described in Section
%\ref{sec:backgnd_merpar},
%the second and later conjuncts are spawned off while the first conjunct is
%executed by the current context.
%The barrier at the end of the first conjunct will block the context if
%the other conjuncts have not yet completed.
%It must be blocked because it would normally continue by executing the code
%after the parallel conjunction
%which may depend on values produced by the other conjuncts ---
%it is not safe for it to continue executing until all the conjuncts have
%been completely executed.
%
%\paul{consider diagram with stack}
%In the right recursive example,
%the recursive call is spawned off,
%the spark for the recursive call will be placed on the global spark queue.
%Another engine will wake up and take the spark from the global queue and
%convert it to a context.
%When it makes the recursive call it will execute the parallel conjunction
%inside and perform the same process.
%There is at most one spark on the global spark queue at any time.
%Each of these contexts,
%after creating the spark for their recursive call,
%will execute \code{P(X, Y)} and
%\joinandcontinue which blocks the context on the completion of
%its recursive call.
%This process continues:
%the runtime system will convert each spark into a context,
%and block each one at the barrier for the conjunction within its
%recursive call.
%This will quickly consume a lot of memory,
%most of which is used for the stacks within each context.
%
%Eventually the number of contexts in memory plus
%the number of sparks on the global queue is equal to the maximum number of
%contexts.
%At this point sparks are not added to the global queue but to their parent
%context's local stack.
%Contexts will not be created to execute these sparks.
%Therefore,
%this restricts the amount of memory allocated in contexts.
%Without this limit,
%a simple loop, such as in Figure \ref{fig:map_right_recursive},
%can easily consume all the physical memory in a system.
%This limit has the adverse effect of restricting how much parallelism
%in the program is exploited.
%This is why performance improves as we allow more contexts per engine.
%We will solve the memory usage problem and context limit in
%Chapter \ref{chap:loop_control}.
%
%In the left recursive program scheduling is quite different.
%The parallel conjunction creates a spark for \code{P(X, Y)} and executes the
%recursive call directly.
%The spark is converted into a context,
%that context does not execute another parallel conjunction since it does not
%execute the recursive call.
%Therefore, it will not become blocked on the \joinandcontinue barrier in any
%nested parallel conjunction.
%It will execute the barrier after \code{P(X, Y)},
%this however does not block this context.
%The context is not the conjunction's original context and therefore once it
%reaches this barrier it is free,
%if it has any sparks on its local queue it may execute them,
%otherwise the engine executing it will look for global work,
%either another context or a spark from the global queue.
%If there is a spark on the global queue the engine will use this context to
%execute it since the context is otherwise unused.
%
%This led us to believe that the left recursion would be more efficient than
%right recursion,
%namely that since contexts are reused, the number of contexts wouldn't climb
%and prevent parallelism from being exploited.
%As Table \ref{tab:right_v_left} shows, we were wrong:
%the context limit is affecting performance.
%As discussed, a left-recursive loop spawns of calls to \code{P} as sparks
%and executes its recursive call directly.
%It will, very quickly,
%make many recursive calls, spawn off many sparks.
%The context limit includes sparks on the global queue since
%executing them can require the creation of new contexts,
%furthermore, if they were not included and the runtime system refused to
%convert a spark on the global queue into a context the system could become
%deadlocked.
%In the left recursive case,
%the context limit will be reached very quickly,
%often before engines have begun taking sparks from the queue and executing
%them.
%Once the limit is reached sparks are placed on the contexts local queues
%where they cannot be executed in parallel.
%The smaller the context limit,
%the more quickly the limit is reached and the fewer contexts are placed on
%the global queue.
%Additionally,
%the loop placing sparks on its context's local stack will execute very
%quickly.
%
%We concluded that
%in the left recursive case
%the scheduling decision for each spark is made much earlier than the spark's
%execution.
%Specifically,
%when the decision to place the spark on the global queue or local stack is
%made,
%often the context limit has already been reached:
%\paul{Need to decide how I communicate who the actor is for scheduling
%decisions.}
%the context will place the spark on its local stack.
%Later, when a different engine becomes idle,
%it cannot access the spark since it is on another engine's context's spark
%stack.
%At this point it is apparent that the scheduling decision made when the
%spark was placed on the local stack was incorrect,
%as there is an idle engine ready to execute the spark,
%and because contexts are re-used (in left recursion) there is either a free
%context or we can easily create one.

\begin{table}
\begin{center}
\begin{tabular}{lr|rrrrrr}
\multicolumn{1}{c|}{} &
\multicolumn{1}{c|}{Max no.\ of contexts} &
\multicolumn{2}{|c|}{Sequmential} &
\multicolumn{4}{|c}{Parallel w/ $N$ Engines} \\
\Cbr{} & & \C{not TS} & \Cbr{TS}  & \C{1}& \C{2}& \C{3}& \C{4}\\
\hline
\multirow{5}{*}{Include} &
 2       & 23.2       & 21.5      & 21.5 & 21.5 & 21.5 & 21.5 \\
&32      & -          & -         & 21.5 & 21.5 & 21.5 & 21.5 \\
&64      & -          & -         & 21.5 & 21.5 & 20.5 & 19.0 \\
&128     & -          & -         & 21.5 & 18.5 & 15.8 & 12.9 \\
&256     & -          & -         & 21.5 & 17.8 & 15.6 & 14.1 \\
\hline
Exclude &
-        & 23.3       & 21.5      & 21.7 & 17.9 & 15.6 & 14.2 \\
\end{tabular}
\end{center}
\caption{Left recursion with and without global sparks included in the context
limit}
\label{tab:2009_nolimit}
\end{table}

\plan{Avoid the right-recursion problem by reordering independent conjunctions.}


\section{Work stealing implementation}
\label{sec:work_stealing}

The stack data structure used is described by \citet{workstealing_queue},
this was chosen as it could support work stealing (Section
\ref{sec:work_stealing}).
%The top of the stack is sometimes referred to as the \emph{hot end},
%since it is used frequently.
%Likewise, the bottom of the stack is sometimes called the
%\emph{cold end}, since it is only used to implement work stealing.
%The top of the stack, or hot end, can be used without locking or atomic
%operations.
%This is desirable, since it makes common operations inexpensive.
%The local context uses the top of the stack only,
%therefore sparks placed on this stack are scheduled in a
%last-in-first-out manner.

\status{Not written}

\plan{Acknowledge work stealing used elsewhere in other runtime systems.}

\subsection{Initial work stealing implementation}

\plan{I believe that Peter discussed work stealing briefly in his thesis,
I should check this and see what he said.}

\plan{Say that stacks queues are associated with contexts.}

\plan{Describe the data structure used to implement these stacks and its properties.}
Local context can use the hot end without synchronisation.
Other contexts can use the cold end with a CAS,
Memory barriers are used to ensure writes appear in the correct order.

\plan{Describe how work stealing policies work}
What happens to the global queue,
in what cases is work stolen,
Show algorithms for work stealing attempts.
Describe work stealing timeout.

\plan{Benchmark}

This subsection was joint work with Peter Wang.

\subsection{Final work stealing implementation}

\plan{Describe problems with associating stacks with contexts}
The number of stacks varies,
Stealing uses a global lock to determine which stack to steal from.

\plan{Prove that even though there are N engines and M contexts and M may be
larger than N, that there will be at most N of the M contexts with work on
their queues}
Therefore:
Stealing is unnecessary complicated.
In pathological cases many attempts can be made without success,
a thief may give up even though there is parallelism.

\plan{We associate stacks with engines}
This removes the above problems how.

\plan{Show stealing algorithm there are any,}
Find out if I started tracking stealing per engine or not.

\plan{Show how this is safe.}
When a parallel conjunction's barrier is executed and a conjunct is
outstanding, if its spark is on this engine's stack it must be at the top of
the stack.
This invariant should be kept because the context can be re-used if
'compatible' work is found at the top of the spark stack.
This invariant is soft.

\plan{Benchmark}

\section{Thread pinning}
\label{sec:thread_pinning}
\status{Not written}

\plan{Explain why we want $P$ engines when there are $P$ processors}

\plan{Explain briefly how we detect how many processors there are,}
Explain that this method is good because it is (mostly) cross platform.

\plan{Explain why we want thread pinning.}
I wonder if there's any relevant literature.

\plan{Explain how we get thread pinning.}
This method is also (mostly) cross platform.

\plan{What about SMT, not all processors are equal.}
This does not matter when we are creating $P$ engines.
But it does matter when we create less than $P$ engines,
explain how bad CPU assignments are sub-optimal.

\plan{How do we handle SMT}
This uses a support library, which is cross platform, provided that it is installed.
We fall back to setcpuafinity() when it is not.

\paul{I am not going to talk about busy waiting since I have not written the
runtime system in a way that I can test or change this easily.}

\section{Idle loop}
\label{sec:idle_loop}
\status{Not written}

\plan{Describe the problem with the current algorithm.}
Engines wake up and periodically check for work by attempting to
steal work,
firstly, this means that there can be up to a 2ms (average 1ms) delay before
a spark is executed.
secondly, this checks for work too often, wasting resources.

\plan{Solution, wake engines for different types of work}
We modified the RTS so that waking an engine is easy, and it can be given a
message so that it knows where to look for work.

\plan{A lot of work went into preventing deadlocks due to race conditions,
a thread that is not yet sleeping if notified must wake up immediately.}

\plan{Extra benefit: when an engine is woken it can be told directly where
to find work, or be given the work directly.}

\plan{Extra benefit: we can be selective about which engine to wake,
while not implemented fully, we can wake a `nearby' engine so that
we can avoid communication between dies or sockets.}

\plan{Show the algorithm for the new idle loop.}
Note that we execute contexts before sparks,
this is more-likely to produce futures and it may reduce memory consumption.

\section{Proposed scheduling tweaks}
\label{sec:proposed_tweaks}
\status{Not written, May move to TS chapter}

I really think that this section will move to the threadscope chapter,
it will have more in common with that chapter and more data will be
available.
Secondly, threadscope can be used with micro-benchmarks to measure the
average costs of certain operations in the RTS.
I will not write it until at least the rest of this chapter is finished.

%\section{Proposed kernel support to manage processor resources}
%\label{sec:kernel_scheduling_help}
%
%\status{This may not be worth discussing until someone actually does it}
%
%Should I describe our proposal for OS kernel's to help
%applications with how many threads to use.
%GCD is related but does not fit into a language runtime system so
%easily~\cite{apple_gcd}.
%See also N:M threading.

%\section{Spare text}
%
%\status{This text will be moved up into one of the work stealing sections
%once those sections are ready}

%The spark is added to a global run queue of sparks, or if that queue is too
%full, because there's already enough parallelism,
%then the spark is added to a queue owned by the current context.
%\citet{wang_hons_thesis} intended to use the local queues for work stealing
%but had not completed his implementation,
%The work-stealing dequeue structure
%is described in \citet{Chase_2005_wsdeque}.
%see Chapter \ref{chap:rts} for details.
%
%and finds that spark is still at the head of its queue,
%it will pick it up and run it itself.
%This is a useful optimisation,
%% it is also really well-known.
%since it avoids using a separate context in the relatively common case
%that all the other CPUs are busy with their own work.
%This optimisation is also useful since it can avoid the creation of superfluous
%contexts and their stacks.
%

%When an engine becomes idle, it will first try
%to resume a suspended but runnable context if there is one.
%If not, it will attempt to run a spark from the global spark queue.
%If it successfully finds a spark, it will allocate a context,
%and start running the spark in that context.

% XXX: Mention global spark queue and spark sheduling above.
% XXX:

%Barrier code is placed at the end of each conjunct,
%this is named \code{join\_and\_continue} (Figure \ref{fig:par_conj}).
%This code starts by atomically decrementing the number of outstanding
%conjuncts in the conjunction's syncterm and checking the result for zero
%(the whole operation is thread-safe, not just the decrement).
%Algorithm \ref{alg:join_and_continue} shows the pseudo code for
%join\_and\_continue.


%XXX
%Parallel conjunctions are evaluated as described in Section
%\ref{sec:backgnd_merpar}.
%Sparks are added to the global spark queue if the global queue has room,
%otherwise they are pushed onto a context-local spark stack.
%Creating a lot of parallel work and using a single global work queue can be
%pesimistic,
%the queue itself can become a bottleneck:
%when many processors try to access it the same location in memory there will
%be many cache misses and delays.
%This is why \citet{wang_hons_thesis} choose to introduce local queues,
%and place work on them when there is a surplus of work on the global queue.
%
%When a engine finishes executing a context and reaches the barrier at the
%end of a parallel conjunct,
%it will check the context's local spark stack for any other work and attempt
%to execute it.
%Otherwise, it will either save the context to resume later or release the
%context before checking the global spark queue and global context queue.
%
%XXX Algorithm.
%
%Deciding too early, results.
%
