
\status{This section is currently an outline,  the text below has been
coppied here as I may use it or a modification of it}

\section{Premature scheduling decisions}

This is the way the runtime system used to work,
I should describe its pesimistic behavour to motiviate work steeling.

\section{Work stealing}

\subsection{Initial work stealing implementation}

This subsection was joint work with Peter Wang.

\subsection{Engine local spark stacks}

\section{Thread pinning}

Thread pinning

SMT

Busy waiting?

Hardware locality

\section{Idle loop}

Idle loop structure

Independent engine wakeup

Engine work notification

\section{Scheduling tweeks}

Not implemented,

Feedback from threadscope?

\section{Garbage collector tweeks}

Large initial heap

Marking \& cache thrashing

Local free lists

\section{Proposed kernel support to manage processor resources}

\status{This may not be worth discussing until someone actually does it}
    
Should I describe our proposal for OS kernel's to help
applications with how many threads to use.
GCD is related but doesn't fit into a language runtime system so
easily~\cite{apple_gcd}.
See also N:M threading.

\section{Spare text}

\status{This text will be moved up into one of the work stealing sections
once those sections are ready}

When an engine becomes idle, it will first try
to resume a suspended but runnable context if there is one.
If not, it will attempt to run a spark from the global spark queue.
If it successfully finds a spark, it will allocate a context,
and start running the spark in that context.

% XXX: Mention global spark queue and spark sheduling above.
% XXX:  

\begin{algorithm}
\begin{algorithmic}
\Procedure{join\_and\_continue}{$ST, ContLabel$}
  \State $last\_conjs \gets$ atomic\_dec\_and\_is\_zero($ST.num\_outstanding$)
  \If{$last\_conjs$}
    \If{$ST.parent = this_context$}
      \Goto{$ContLabel$}
    \Else
      \State schedule($ST.parent$)
      \Goto{get\_global\_work}
    \EndIf
  \Else
    \State $spark \gets$ pop\_compatible\_spark
    \If{$spark$}
       \Goto{$spark.code\_label$}
    \Else
      \If{$ST.parent = this\_context$}
         \State suspend($this\_context$)
         \State $this\_context \gets$ NULL
      \EndIf
      \Goto{get\_global\_work}
    \EndIf
  \EndIf
\EndProcedure
\end{algorithmic}
\caption{join\_and\_continue}
\label{alg:join_and_continue}
\end{algorithm}

Barrier code is placed at the end of each conjunct,
this is named \code{join\_and\_continue} (Figure \ref{fig:par_conj}).
This code starts by atomically decrementing the number of outstanding
conjuncts in the conjunction's syncterm and checking the result for zero
(the whole operation is thread-safe, not just the decrement).
If there are no remaining conjuncts and the current context is the parent
context,
then execution jumps to the label after the parallel conjunction.
If the current context is not the parent context then
we can infer that the parent context is suspended,
therefore, 
the engine will schedule the parent context, before looking for global work.
It looks for global work because its local work queue is gaurenteed to be
empty since this conjunction and any nested conjunctions are complete.
Alternativly, if there are outstanding conjuncts then
the local spark stack is checked for compatible work ---
a spark whose parent context is the same as the current syncterm's parent
context.
If a compatible spark is at the top of the spark stack then it is removed
and executed.
Otherwise,
if this context is the parent context it must be suspended
before the engine looks for global work.
Algorithm \ref{alg:join_and_continue} shows the pesudo code for
join\_and\_continue.

An engine looks for global work first by check the global context run queue.
If it finds a runnable context and is still holding a context from a
previous execution, it saves the old context onto the free context list.
If there are no runnable contexts,
it will then attempt to steal work from other contexts.
If unsuccessful, it will become idle and sleep
until it is woken up because a context has become runnable,
or a spark is added to the global spark queue.
