
\status{This chapter is currently WIP}

When manually parallelising Mercury programs we
did not get the speedups that we expected;
this would also reduce the performance of automatically parallelised
programs.
Therefore,
we chose to address the parallel performance problems
in the runtime system before we worked on automatic parallelism.

This chapter describes the changes that we made to the runtime system to
improve parallel execution performance.
The chapter is structured as follows:
Section \ref{sec:gc} describes the garbage collector's effect on parallel
execution performance and how tuning some parameters of the garbage
collector can improve performance.
Section \ref{sec:old_scheduling} describes how the existing runtime
system's schedules sparks,
it provides background material for Section
\ref{sec:old_scheduling_performance}
which benchmarks the runtime system and describes the problem with spark
scheduling.
We address those problems by introducing work stealing in Section
\ref{sec:work_stealing},
this section is broken into two sub-sections,
these describe the initial and final versions of the work stealing
implementation.
Section \ref{sec:work_stealing} also includes benchmarks that show
how work stealing fixes the spark scheduling problem.
\paul{I may keep or remove the right recursion section (Section
\ref{sec:right_recursion}.}
We made a number of improvments to the way that Mercury engines are created,
this includes thread pinning and support for SMT systems.
We describe these improvments in Section \ref{sec:thread_pinning}
Section \ref{sec:idle_loop} describes our changes to how engines idle
and when they wake up to execute parallel work.
\paul{Proposed scheduling tweeks may move to the ThreadScope chapter.}
Finally, in Section \ref{sec:kernel_scheduling_help}
we describe an area of research that would improve resource usage when
multiple instances of a parallel Mercury program are executing in parallel.

\section{Garbage collection tweeks}
\label{sec:gc}

\status{Planned, finish prose and may need a benchmark}
\plan{Describe Mercury's behviour WRT GC}
One of the sources of poor parallel performance is the behaviour of the
garbage collector.
Like many declarative languages,
Mercury discourages distructive update also known as mutation of data
structures.\footnote{
    We have not covered mutables in Mercury as they are not relelvent to the
    disertation.
    Their use does not interfer with parallelism as they are used in
    conjunction with impurity,
    the compiler will not parallelise impure goals.}
Therefore a calculation usually returns its value in newly allocated memory
rather than modifying the memory of one of its parameters.
This means that Mercury programs often have a high rate of allocation,
which places a lot of stress on the garbage collector.
Therefore,
allocation and garbage collection have a strong impact on a program's
performance,
this usually becomes more sagnificant when parallelism is introduced into a
program.

\plan{Introduce Boehm, \& details: conservative, mark and sweep, stop the
world, parallel marking.}
Mercury uses Boehm-Demers-Weiser Conservative Garbage Collector (Boehm GC)
\citep{boehm_gc},
which is a conservative mark and sweep collector.
Boehm GC supports parallel programming,
it will stop all the threads (\emph{stop the world}) during its marking
phase.
It also supports parallel marking,
it will use its own set of pthreads to perform parallel marking.

\plan{Introduce colletor time, mutator time.}
When discussing garbage collection,
a program's execution time is divided into two alternating phases:
collection time, which is when Boehm GC performs marking
and mutator time, which is when the Mercury program may run.
The name mutator time refers to time that mutations (changes) to memeory
structures are permitted.
The collector may perform some actions concurently with the mutator,
such as sweeping.

\plan{Describe theory of GC performance.}
Amdahl's law~\citep{amdahl} describes the maximum theoretical speedup that
can be acheived by parallelising a program where some part of that program
cannot be parallelised.
When sequential marking is used then can use Amdahl's
law to predict the speedup of our programs.
For example, consider a program with a runtime of 20 seconds,
if one second of that (5\%) is collection time
then using four processors the theoretical best speedup we can acheive is:
$(1 + 19) \div (1 + 19\div4) = 3.48$,
in this case the program spends 17\% of its time doing garbage collection.
If we use a machine with 100 processors then this becomes:
$(1 + 19) \div (1 + 19\div100) = 16.8$,
in this case 84\% of the program's time is collection time.
As the number of processors increases the benifit of adding each additional
processor drops,
and at the same time collection time dominates the execution time of the
program.

\plan{Discuss predictions regarding parallel marking, locking and thread
local heaps.}
In practice however,
Boehm GC supports parallel marking;
we can resonably expect better speedups when using parallel marking.
However,
efficient parallel marking is not perfect,
it is a continuing area of research.
Therefore,
we cannot expect to get strong speedups due to parallel marking.
Additionally, 
the garbage collector must be thread safe to support multiple mutator
threads.
The allocation routines must therefore use locking to protect shared data
structures,
this will slow down allocation.
We can resonably expect that a program may slow down when thread safty is
enabled from the cost of locking alone.
Boehm GC's authors recognize problem,
to address it each thread is given some thread-local resources such as free
lists.
Therefore,
during memory allocation a thread can use its own free lists to allocate
memory rather than locking any global structure.
From time-to-time, a thread will have to retrive new free lists or other
data from a global structure and will need to perform locking then,
however this cost is amortized across a number of memory allocations.

\paul{Check that this is all independent parallelism}
\plan{Describe icfp2000, mandelbrot and mandelbrot\_heap programs.}
One of our benchmarks is parallel version of a raytracer developed for the
ICFP programming context in 2000,
it is named icfp2000.
The raytracer is parallelised by modifying its loop over rows in the image
so that each row may be computed in parallel with the other rows.
The raytracer uses many small structures to represent vectors and a pixel's
colour.
Therefore,
it is very memory allocation intensive.
Since we suspected that garbage collection was having a negative impact on
performance we developed a mandelbrot image generator.
For a given viewport into the image the mandelbrot program 
tries to determine if that pixel's coordinates are inside of the mandelbrot
set or outside.
This involves many iterations for each pixel,
after a maximum number of iterations the program `gives up' on that pixel
The colour of each pixel represents how many iterations before the program
determined that the pixel's coordinates are outside the mandelbrot set.
and assumes that it is inside the mandelbrot set.
This program is parallelised in a similar way to the raytracer,
the loop that iterates over rows of the image was parallelised.
The main feature of the mandelbrot program,
is that it is not memory allocation intensive.
Coordiates and complex numbers are not represented as structures,
their componenets are stored on the stack.
We created a second version of the mandelbrot program called
mahdelbrot\_heap.
This program does represent coordinates and complex numbers in structures,
and therefore Mercury uses pass-by-reference and places these values on the
heap.
mandelbrot\_heap is more allocation intensive than mandelbrot.

\begin{table}
\begin{center}
\begin{tabular}{l|r|rrrr}
\Cbr{Program} & \Cbr{GC Markers} &
\multicolumn{4}{|c}{Mercury Engines} \\
\Cbr{} & \Cbr{} & \C{1} & \C{2} & \C{3} & \C{4} \\
\hline
\multirow{6}{*}{mandelbrot} &
 Sequential & 15.3 (1.0) & -          & -         & - \\
&TS, no Par & 15.3 (1.0) & -          & -         & - \\
&1          & 15.3 (1.0) & 7.7 (2.0)  & 5.1 (3.0) & 3.9 (3.9) \\
&2          & 15.3 (1.0) & 7.7 (2.0)  & 5.1 (3.0) & 3.9 (3.9) \\
&3          & 15.3 (1.0) & 7.7 (2.0)  & 5.1 (3.0) & 3.9 (3.9) \\
&4          & 15.3 (1.0) & 7.7 (2.0)  & 5.1 (3.0) & 3.9 (3.9) \\
\end{tabular}
\end{center}
\caption{Garbage collection effects.}
\end{table}

\plan{Benchmark data.}

\plan{Describe performance in practice.}

Boehm GC allows for parallel marking,
but first we describe sequential marking:
We have to account for Amdahl's law~\citep{amdahl}.
Amdahl's law says that if some of the program is parallelisable and some
is not,
then as the number of processors increases the sequential part of the
program will dominate the program's runtime.
Therefore,
a sequential marking phase in a garbage collector will affect the speedup
the program can gain due to parallelism.
If a sequential program runs for 20 seconds and sepends one second of this
(5\%) marking sequentially,
and we parallelise the program's 19 seconds across four processors we get
a speed up of
$(1 + 19) \div (1 + 19\div4) = 3.48$,
in this case the program spends 17\% of its time doing garbage collection.
Even with one hundred processors we can not acheive a speedup greater than
$(1 + 19) \div (1 + 19\div100) = 16.8$
The theoretical maximum speedup of such a program is 20.
Boehm GC uses helper threads in an attempt to parallelise the marking phase.
We would expect that as we parallelise a program,
such as icfp2000,
that when using sequential marking our observations should match the
predictions of Amdahl's law,
and that when using parallel marking we should see speed-up,
although we do not expect it to parallelise well
as this is a continuing field of research.

\plan{Compare performance vs Amdahl's predictions. for sequential marking.}

\plan{Parallel marking performance observations.}
not that parallelisation is not easy.

\plan{New data, vary the heap size,}

\plan{Criticise cache behaviour WRT parallel marking,
Thanks Simon Marlow.}

\plan{Discuss local heaps for threads, and their reliability problems.}


To illustrate the difference that garbage collection can make we wrote a new
benchmark, mandelbrot.
This program is a micro-benchmark, and tests the performance of 
Mercury's parallel conjunction operator independent of the garbage
collector's effects.
The mandelbrot set calculation is done on complex numbers,
most programmers will use a tuple-like data type to represent each number,
each number would be allocated as a heap object.
\paul{I could do a test between mandelbrot with complex data type vs
without.}
To avoid stress on the garbage collector stored the real and imaginary parts
of each number on the stack.
This reduces the program's reliance on the garbage collector,
as a result the program does very little allocation or collection.
We can see this in Table \ref{tab:mandelbrot_vs_icfp2000}.
We will use this benchmark in the remainder of this chapter since we want to
measure the runtime system's performance independent of the garbage collector.

\section{Prior spark scheduling algorithm}
\label{sec:old_scheduling}

\status{Complete}

Sparks are managed in two different data structures.
The first data structure is a global queue,
since it is global it must be protected with a mutex.
It is used in a first-in-first-out manner.
The second is a context-local stack,
The stack data structure used is described by \citet{workstealing_queue},
this was chosen as it could support work stealing.
The top of the stack is sometimes referred to as the \emph{hot end},
since it is used frequently.
Likewise, the bottom of the stack is sometimes called the
\emph{cold end}, since it is only used to implement work stealing.
The top of the stack, or hot end, can be used without locking or atomic
operations.
This is desirable, since it makes common operations inexpensive.
The local context uses the top of the stack only,
therefore sparks placed on this stack are scheduled in a
last-in-first-out manner.

When a spark is spawned off by a parallel conjunction
it is added to a global spark queue provided that there is not too much
globally available parallel work.
This is true if:
an engine is idle and,
the number of contexts in use plus the number of sparks in the global queue
does not exceed the maximum number of contexts permitted.
If these tests fail then the spark will be placed on the local context's
spark stack,
which is done safely without locking.
This is an optimisation,
in the common case that there is already enough parallel work,
a spark can be scheduled without contending for a lock which could slow the
system down.
The limit of contexts in use plus the number of sparks in the global queue
represents how many contexts would be in use of all the sparks in the global
queue where converted to contexts.
This limit places an upper bound on the number of contexts that may be
allocated,
which limits the amount of memory that can be allocated as stack space for
contexts
(recall that each context contains space for stacks).

\begin{algorithm}
\begin{algorithmic}
\Procedure{join\_and\_continue}{$ST, ContLabel$}
  \State aquire\_lock($ST.lock$)
  \State $ST.num\_outstanding \gets ST.num\_outstanding - 1$
  \If{$ST.num\_outstanding = 0$}
    \If{$ST.parent = this_context$}
      \State release\_lock($ST.lock$)
      \Goto{$ContLabel$}
    \Else
      \State schedule($ST.parent$)
      \State release\_lock($ST.lock$)
      \Goto{get\_global\_work}
    \EndIf
  \Else
    \State $spark \gets$ pop\_compatible\_spark
    \If{$spark$}
       \State release\_lock($ST.lock$)
       \Goto{$spark.code\_label$}
    \Else
      \If{$ST.parent = this\_context$}
         \State suspend($this\_context$)
         \State $this\_context \gets$ NULL
      \EndIf
      \State release\_lock($ST.lock$)
      \Goto{get\_global\_work}
    \EndIf
  \EndIf
\EndProcedure
\end{algorithmic}
\caption{join\_and\_continue}
\label{alg:join_and_continue_peterw}
\end{algorithm}

As an engine finishes executing a parallel conjunct,
it will execute the the barrier at the end of the conjunct,
named \joinandcontinue and shown in 
Algorithm \ref{alg:join_and_continue_peterw}.
If it is known that the parallel conjunction has only been executed by
one context,
then it is safe to use a version of the barrier code that does not use
locking,
this optimisation is not shown as it is equivalent and not relevant to
our discussion,
it is described only for completeness.

The algorithm begins by checking if there are any outstanding conjuncts in
the parallel conjunction.
If there are none and the current context is the parent
context,
then execution jumps to the label after the parallel conjunction.
If the current context is not the parent context then
we can infer that the parent context is suspended,
therefore, 
the engine will schedule the parent context, before looking for global work.
It looks for global work because its local work queue is guaranteed to be
empty since this conjunction and any nested conjunctions are complete.
Alternatively, if there are outstanding conjuncts then
the local spark stack is checked for compatible work ---
a spark whose parent context is the same as the current syncterm's parent
context.
If a compatible spark is at the top of the spark stack then it is removed
and executed.
Otherwise,
if this context is the parent context it must be suspended
before the engine looks for global work.

An engine looks for global work first by checking the global context run queue.
If it finds a runnable context and is still holding a context from a
previous execution, it saves the old context onto the free context list.
If there are no runnable contexts,
it will take a spark from the global spark queue,
and either use its current context to execute the spark,
or allocate a new context (from the free context list if possible).
If it is unsuccessful at finding work,
it will go to sleep using a pthread condition variable and the global run
queue's lock.
This condition is used to wake engines when either contexts are added to the
run queue,
or sparks are added to the spark run queue.

\section{Prior spark scheduling performance}
\label{sec:old_scheduling_performance}

\status{Run another test then finish the prose}

\begin{figure}
\begin{center}
\subfigure[Right recursive]{%
\label{fig:map_right_recursive}
\begin{tabular}{l}
\code{map(\_, [], []).} \\
\code{map(P, [X $|$ Xs], [Y $|$ Ys]) :-} \\
\code{~~~~P(X, Y) \&} \\
\code{~~~~map(P, Xs, Ys).} \\
\end{tabular}}
\subfigure[Left recursive]{%
\label{fig:map_left_recursive}
\begin{tabular}{l}
\code{map(\_, [], []).} \\
\code{map(P, [X $|$ Xs], [Y $|$ Ys]) :-} \\
\code{~~~~map(P, Xs, Ys) \&} \\
\code{~~~~P(X, Y).} \\
\end{tabular}}%
\end{center}
\caption{Right and left recursive map/3}
\label{fig:map_right_and_left_recursive}
\end{figure}

Figure \ref{fig:map_right_and_left_recursive} shows two alternative, parallel
implementations of \code{map/3}.
While their declarative semantics are identical,
their operational semantics are very different.  In Section
\ref{sec:backgnd_merpar} we explained that parallel conjunctions are
implemented by spawning off the tail of the conjunction and executing the
head directly.
This means that in the right recursive case (Figure
\ref{fig:map_right_recursive}), the recursive call is spawned off as a
spark,
and that in the left recursive case (Figure \ref{fig:map_left_recursive}),
the recursive call is executed directly, and the loops \emph{body} is
spawned off.

\begin{table}
\paul{Make this easier to analyse by making the second column the actual
number of contexts, not contexts per engine.}
\begin{center}
\begin{tabular}{lr|rrrrrr}
\multicolumn{1}{c|}{Recursion type} &
\multicolumn{1}{c|}{Max no.\ of contexts} &
\multicolumn{2}{|c|}{Sequmential} &
\multicolumn{4}{|c}{Parallel w/ $N$ Engines} \\
\Cbr{} & & \C{not TS} & \Cbr{TS} & \C{1}& \C{2}& \C{3}& \C{4}\\
\hline
\multirow{5}{*}{Right} &
 2      & 23.2       & 21.5     & 21.5 & 21.6 & 21.6 & 21.6 \\
&32     & -          & -        & 21.5 & 21.6 & 21.5 & 21.2 \\
&64     & -          & -        & 21.5 & 19.8 & 18.5 & 16.5 \\
&128    & -          & -        & 21.5 & 13.2 &  8.2 &  6.1 \\
&256    & -          & -        & 21.5 & 12.2 &  8.1 &  6.1 \\
\hline
\multirow{5}{*}{Left} &
 2      & 23.2       & 21.5     & 21.5 & 21.5 & 21.5 & 21.5 \\
&32     & -          & -        & 21.5 & 21.5 & 21.5 & 21.5 \\
&64     & -          & -        & 21.5 & 21.5 & 20.5 & 19.0 \\
&128    & -          & -        & 21.5 & 18.5 & 15.8 & 12.9 \\
&256    & -          & -        & 21.5 & 17.8 & 15.6 & 14.1 \\
\end{tabular}
\end{center}
\caption{Right vs.\ left recursion.}
\label{tab:right_v_left}
\end{table}

Table \ref{tab:right_v_left} shows average elapsed time in seconds for the
mandelbrot image generating program over 20 samples.
The program iterates over the rows in the mandelbrot image using
a left or right recursive parallel
\code{map/3} predicate such as those in Figure
\ref{fig:map_right_and_left_recursive},
the leftmost column in the table indicates which predicate was used.
The next column describes how many contexts may exist at once per Mercury
engine.
Values are omitted in the range 3--31 as they are not interesting:
they are the same as the case for 2 contexts per engine.
The next two columns give the runtime for a sequential version of the
program,
in this version of the program no parallel conjunctions where used.
The first of these, labelled ``not TS'',
is compiled without thread safety;
the second, labelled ``TS'',
is compiled with thread safety, meaning that it allows multiple engines to be
used: requiring a register to point to a pthread's engine structure
(Section \ref{sec:backgnd_merpar}),
and compiling the garbage collector for thread safety.
The following four columns give the runtimes for the parallel mandelbrot
program with 1--4 Mercury engines.
All our benchmarking is performed on `cabsav',
an ASRock Z68-Pro3 based Intel i7-2600K system.

We can see that the right recursive program performs better than the left
recursive one.
The right recursive program, when using 4 cores and at least 128 contexts,
achieves a speedup of 3.52 compared to the right recursive sequential thread
safe program.
At best the left recursive program is sped up by a factor of 1.67.
The next observation is that as we allow more contexts to reside in memory
at once,
we achieve greater speedups.
The exception to this is the case for left recursion with four engines and
256 contexts, it is slower than the case for 256 contexts.
These two results have standard deviations of 6.40 and 6.15 seconds
respectively meaning that any apparent difference is most likely 
noise in the data.
All the other results have a standard deviation of less than two seconds,
most of these are less than half a second.
We saw the same high variance in these two results when repeating the test.
We will explain the probable cause for this later in this section.

Parallel conjunctions are evaluated as described in Section
\ref{sec:backgnd_merpar},
the second and later conjuncts are spawned off while the first conjunct is
executed by the current context.
The barrier at the end of the first conjunct will block the context if
the other conjuncts have not yet completed.
It must be blocked because it would normally continue by executing the code
after the parallel conjunction
which may depend on values produced by the other conjuncts ---
it is not safe for it to continue executing until all the conjuncts have
been completely executed.

\paul{consider diagram with stack}
In the right recursive example,
the recursive call is spawned off,
the spark for the recursive call will be placed on the global spark queue.
Another engine will wake up and take the spark from the global queue and
convert it to a context.
When it makes the recursive call it will execute the parallel conjunction
inside and perform the same process.
There is at most one spark on the global spark queue at any time.
Each of these contexts,
after creating the spark for their recursive call,
will execute \code{P(X, Y)} and
\joinandcontinue which blocks the context on the completion of
its recursive call.
This process continues:
the runtime system will convert each spark into a context,
and block each one at the barrier for the conjunction within its
recursive call.
This will quickly consume a lot of memory,
most of which is used for the stacks within each context.

Eventually the number of contexts in memory plus
the number of sparks on the global queue is equal to the maximum number of
contexts.
At this point sparks are not added to the global queue but to their parent
context's local stack.
Contexts will not be created to execute these sparks.
Therefore,
this restricts the amount of memory allocated in contexts.
Without this limit,
a simple loop, such as in Figure \ref{fig:map_right_recursive}, 
can easily consume all the physical memory in a system.
This limit has the adverse effect of restricting how much parallelism
in the program is exploited.
This is why performance improves as we allow more contexts per engine.
We will solve the memory usage problem and context limit in
Chapter \ref{chap:loop_control}.

In the left recursive program scheduling is quite different.
The parallel conjunction creates a spark for \code{P(X, Y)} and executes the
recursive call directly.
The spark is converted into a context,
that context does not execute another parallel conjunction since it does not
execute the recursive call.
Therefore, it will not become blocked on the \joinandcontinue barrier in any
nested parallel conjunction.
It will execute the barrier after \code{P(X, Y)},
this however does not block this context.
The context is not the conjunction's original context and therefore once it
reaches this barrier it is free,
if it has any sparks on its local queue it may execute them,
otherwise the engine executing it will look for global work,
either another context or a spark from the global queue.
If there is a spark on the global queue the engine will use this context to
execute it since the context is otherwise unused.

This led us to believe that the left recursion would be more efficient than
right recursion,
namely that since contexts are reused, the number of contexts wouldn't climb
and prevent parallelism from being exploited.
As Table \ref{tab:right_v_left} shows, we were wrong:
the context limit is affecting performance.
As discussed, a left-recursive loop spawns of calls to \code{P} as sparks
and executes its recursive call directly.
It will, very quickly,
make many recursive calls, spawn off many sparks.
The context limit includes sparks on the global queue since
executing them can require the creation of new contexts,
furthermore, if they were not included and the runtime system refused to
convert a spark on the global queue into a context the system could become
deadlocked.
In the left recursive case,
the context limit will be reached very quickly,
often before engines have begun taking sparks from the queue and executing
them.
Once the limit is reached sparks are placed on the contexts local queues
where they cannot be executed in parallel.
The smaller the context limit,
the more quickly the limit is reached and the fewer contexts are placed on
the global queue.
Additionally,
the loop placing sparks on its context's local stack will execute very
quickly.

We concluded that
in the left recursive case
the scheduling decision for each spark is made much earlier than the spark's
execution.
Specifically,
when the decision to place the spark on the global queue or local stack is
made,
often the context limit has already been reached:
\paul{Need to decide how I communicate who the actor is for scheduling
decisions.}
the context will place the spark on its local stack.
Later, when a different engine becomes idle,
it cannot access the spark since it is on another engine's context's spark
stack.
At this point it is apparent that the scheduling decision made when the
spark was placed on the local stack was incorrect,
as there is an idle engine ready to execute the spark,
and because contexts are re-used (in left recursion) there is either a free
context or we can easily create one.

\begin{table}
\begin{center}
\begin{tabular}{lr|rrrrrr}
\multicolumn{1}{c|}{} &
\multicolumn{1}{c|}{Max no.\ of contexts} &
\multicolumn{2}{|c|}{Sequmential} &
\multicolumn{4}{|c}{Parallel w/ $N$ Engines} \\
\Cbr{} & & \C{not TS} & \Cbr{TS}  & \C{1}& \C{2}& \C{3}& \C{4}\\
\hline
\multirow{5}{*}{Include} &
 2       & 23.2       & 21.5      & 21.5 & 21.5 & 21.5 & 21.5 \\
&32      & -          & -         & 21.5 & 21.5 & 21.5 & 21.5 \\
&64      & -          & -         & 21.5 & 21.5 & 20.5 & 19.0 \\
&128     & -          & -         & 21.5 & 18.5 & 15.8 & 12.9 \\
&256     & -          & -         & 21.5 & 17.8 & 15.6 & 14.1 \\
\hline
Exclude &
-        & 23.3       & 21.5      & 21.7 & 17.9 & 15.6 & 14.2 \\
\end{tabular}
\end{center}
\caption{Left recursion with and without global sparks included in the context
limit}
\label{tab:2009_nolimit}
\end{table}

XXX: Lead in to next section.


\section{Work stealing implementation}
\label{sec:work_stealing}

\status{Not written}

\subsection{Initial work stealing implementation}

This subsection was joint work with Peter Wang.

\subsection{Final work stealing implementation}

\section{Avoiding the right recursion problem}
\label{sec:right_recursion}
\status{Not written}

\paul{I am not sure if I need a section for this at all.}

There's ony one point to make, and that is where a parallelisation is
idependent we re-arange the conjuncts to avoid the right recursion problem.

\section{Thread pinning}
\label{sec:thread_pinning}
\status{Not written}

Detecting the number of processors.

Thread pinning

SMT

Busy waiting?

Hardware locality

\section{Idle loop}
\label{sec:idle_loop}
\status{Not written}

Idle loop structure

Independent engine wakeup

Engine work notification

\section{Proposed scheduling tweeks}
\label{sec:proposed_tweeks}
\status{Not written, May move to TS chapter}

Not implemented,

Feedback from ThreadScope?

\section{Proposed kernel support to manage processor resources}
\label{sec:kernel_scheduling_help}

\status{This may not be worth discussing until someone actually does it}
    
Should I describe our proposal for OS kernel's to help
applications with how many threads to use.
GCD is related but does not fit into a language runtime system so
easily~\cite{apple_gcd}.
See also N:M threading.

\section{Spare text}

\status{This text will be moved up into one of the work stealing sections
once those sections are ready}

%The spark is added to a global run queue of sparks, or if that queue is too
%full, because there's already enough parallelism,
%then the spark is added to a queue owned by the current context.
%\citet{wang_hons_thesis} intended to use the local queues for work stealing
%but had not completed his implementation,
%The work-stealing dequeue structure
%is described in \citet{Chase_2005_wsdeque}.
%see Chapter \ref{chap:rts} for details.
%
%and finds that spark is still at the head of its queue,
%it will pick it up and run it itself.
%This is a useful optimisation,
%% it is also really well-known.
%since it avoids using a separate context in the relatively common case
%that all the other CPUs are busy with their own work.
%This optimisation is also useful since it can avoid the creation of superfluous
%contexts and their stacks.
%

When an engine becomes idle, it will first try
to resume a suspended but runnable context if there is one.
If not, it will attempt to run a spark from the global spark queue.
If it successfully finds a spark, it will allocate a context,
and start running the spark in that context.

% XXX: Mention global spark queue and spark sheduling above.
% XXX:  

Barrier code is placed at the end of each conjunct,
this is named \code{join\_and\_continue} (Figure \ref{fig:par_conj}).
This code starts by atomically decrementing the number of outstanding
conjuncts in the conjunction's syncterm and checking the result for zero
(the whole operation is thread-safe, not just the decrement).
Algorithm \ref{alg:join_and_continue} shows the pseudo code for
join\_and\_continue.


%XXX
%Parallel conjunctions are evaluated as described in Section
%\ref{sec:backgnd_merpar}.
%Sparks are added to the global spark queue if the global queue has room,
%otherwise they are pushed onto a context-local spark stack.
%Creating a lot of parallel work and using a single global work queue can be
%pesimistic,
%the queue itself can become a bottleneck:
%when many processors try to access it the same location in memory there will
%be many cache misses and delays.
%This is why \citet{wang_hons_thesis} choose to introduce local queues,
%and place work on them when there is a surplus of work on the global queue.
%
%When a engine finishes executing a context and reaches the barrier at the
%end of a parallel conjunct,
%it will check the context's local spark stack for any other work and attempt
%to execute it.
%Otherwise, it will either save the context to resume later or release the
%context before checking the global spark queue and global context queue.
%
%XXX Algorithm.
%
%Deciding too early, results.
%
