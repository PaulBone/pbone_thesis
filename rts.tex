
\status{This section is currently an outline,  the text below has been
coppied here as I may use it or a modification of it}

Before beginning sagnificant amounts of work on automatic parallelism we
choose to improve the performance of manual parallelisation.
Any improvment to the runtime system for manual parallelisation would have
benifits for automatic parallelism.
In Mercury, parallelisation can be introduced with the parallel conjunction
operator (\code{\&}).
A programmer can replace a sequential conjunction with a parallel one,
declaring to the compiler that the conjuncts should be executed in parallel.
However doing so did not speed up Mercury programs as expected.
In this chapter we will discuss
some of the problems affectiong performance and
many of the improvments that have made parallel execution faster.

\section{Garbage collection}

While using parallel benchmarks we found that Mercury's garbage collector,
the Boehm-Demers-Weiser Conservative Garbage Collector (Boehm GC)
\citep{boehm_gc},
could easily interfere with parallel performance.
Like many declarative languages,
Mercury programs tend to have a high rate of allocation,
placing a lot of stress on the garbage collector.
In particular,
Boehm GC uses a stop-the-world marking phase,
all threads must stop, sequentialising the program.

XXX: Discussion of Amdahl's law WRT stop-the-world.
Use icfp2000's example.

XXX: Introduce parallel marking,

XXX: Criticise cache behaviour wrt parallel marking,
Thanks Simon Marlow.

XXX: Benchmark icfp2000 with different initial heap sizes.

XXX: Discuss local heaps for threads, and their reliability problems.

XXX: Add table.

To illistrate the difference that garbage collection can make we wrote a new
benchmark, mandelbrot.
This program is a micro-benchmark, and tests the performance of 
Mercury's parallel conjunction operator independent of the garbage
collector's effects.
The mandelbrot set calculation is done on complex numbers,
most programmers will use a tuple-like data type to represent each number,
each number would be allocated as a heap object.
\paul{Ohh, I could do a test between mandelbrot with complex data type vs
without.}
To avoid stress on the garbage collector stored the real and imaginary parts
of each number on the stack.
This reduces the program's reliance on the garbage collector,
as a result the program does very little allocation or collection.
We can see this in Table \ref{tab:mandelbrot_vs_icfp2000}.
We will use this benchmark in the remainder of this chapter since we want to
measure the runtime system's performance independent of the garbage collector.

\section{Spark queueing}

Sparks are managed in two different data structures.
The first data structure is a global queue,
since it is global it must be protected with a mutex.
It is used in a first-in-first-out manner.
The second is a context-local stack,
The stack datastructure used is described by \citet{workstealing_queue},
this was choosen as it could support work stealing.
The top of the stack is sometimes referred to as the \emph{hot end},
since it is used frequently.
Likewise, the bottom of the stack is sometimes called the
\emph{cold end}, since it is only used to implement work stealing.
The top of the stack, or hot end, can be used without locking or atomic
operations.
This is desirable, since it makes common operations inexpensive.
The local context uses the top of the stack only,
therefore sparks placed on this stack are scheduled in a
last-in-first-out manner.

When a spark is spawned off by a parallel conjunction
it is added to a global spark queue provided that there is not too much
globaly-available parallel work.
This is true if:
an engine is idle and,
the number of contexts in use plus the number of sparks in the global queue
does not exceed the maximum number of contexts permitted.
If these tests fail then the spark will be placed on the local context's
spark stack,
which is done safely without locking.
This is an optimisation,
in the common case that there is already enough parallel work,
a spark can be scheduled without contending for a lock which could slow the
system down.
The limit of contexts in use plus the number of sparks in the global queue
represents how many contexts would be in use of all the sparks in the global
queue where converted to contexts.
This limit places an upper bound on the number of contexts that may be
allocated,
which limits the amount of memory that can be allocated as stack space for
contexts
(recall that each context contains space for stacks).

\begin{algorithm}
\begin{algorithmic}
\Procedure{join\_and\_continue}{$ST, ContLabel$}
  \State aquire\_lock($ST.lock$)
  \State $ST.num\_outstanding \gets ST.num\_outstanding - 1$
  \If{$ST.num\_outstanding = 0$}
    \If{$ST.parent = this_context$}
      \State release\_lock($ST.lock$)
      \Goto{$ContLabel$}
    \Else
      \State schedule($ST.parent$)
      \State release\_lock($ST.lock$)
      \Goto{get\_global\_work}
    \EndIf
  \Else
    \State $spark \gets$ pop\_compatible\_spark
    \If{$spark$}
       \State release\_lock($ST.lock$)
       \Goto{$spark.code\_label$}
    \Else
      \If{$ST.parent = this\_context$}
         \State suspend($this\_context$)
         \State $this\_context \gets$ NULL
      \EndIf
      \State release\_lock($ST.lock$)
      \Goto{get\_global\_work}
    \EndIf
  \EndIf
\EndProcedure
\end{algorithmic}
\caption{join\_and\_continue}
\label{alg:join_and_continue_peterw}
\end{algorithm}

As an engine finishes executing a parallel conjunct,
it will execute the the barrier at the end of the conjunct,
named \joinandcontinue and shown in 
Algorithm \ref{alg:join_and_continue_peterw}.
If it is known that the parallel conjunction has only been executed by
one context,
then it is safe to use a version of the barrier code that does not use
locking,
this optimisation is not shown as it is equivilent and not relevent to
our discussion,
it is described only for completeness.

The algorithm begins by checking if there are any outstanding conjuncts in
the parallel conjunction.
If there are none and the current context is the parent
context,
then execution jumps to the label after the parallel conjunction.
If the current context is not the parent context then
we can infer that the parent context is suspended,
therefore, 
the engine will schedule the parent context, before looking for global work.
It looks for global work because its local work queue is gaurenteed to be
empty since this conjunction and any nested conjunctions are complete.
Alternativly, if there are outstanding conjuncts then
the local spark stack is checked for compatible work ---
a spark whose parent context is the same as the current syncterm's parent
context.
If a compatible spark is at the top of the spark stack then it is removed
and executed.
Otherwise,
if this context is the parent context it must be suspended
before the engine looks for global work.

An engine looks for global work first by checking the global context run queue.
If it finds a runnable context and is still holding a context from a
previous execution, it saves the old context onto the free context list.
If there are no runnable contexts,
it will take a spark from the global spark queue,
and either use its current context to execute the spark,
or allocate a new context (from the free context list if possible).
If it is unsuccessful at finding work,
it will go to sleep using a pthread condition variable and the global run
queue's lock.
This condition is used to wake engines when either contexts are added to the
run queue,
or sparks are added to the spark run queue.

\section{Premature scheduling decisions}

\begin{figure}
\begin{center}
\subfigure[Right recursive]{%
\label{fig:map_right_recursive}
\begin{tabular}{l}
\code{map(\_, [], []).} \\
\code{map(P, [X $|$ Xs], [Y $|$ Ys]) :-} \\
\code{~~~~P(X, Y) \&} \\
\code{~~~~map(P, Xs, Ys).} \\
\end{tabular}}
\subfigure[Left recursive]{%
\label{fig:map_left_recursive}
\begin{tabular}{l}
\code{map(\_, [], []).} \\
\code{map(P, [X $|$ Xs], [Y $|$ Ys]) :-} \\
\code{~~~~map(P, Xs, Ys) \&} \\
\code{~~~~P(X, Y).} \\
\end{tabular}}%
\end{center}
\caption{Right and left recursive map/3}
\label{fig:map_right_and_left_recursive}
\end{figure}

Figure \ref{fig:map_right_and_left_recursion} shows two alternative, parallel
implementations of \code{map/3}.
While their declarative semantics are identical,
their operational semantics are very different.  In Section
\ref{sec:backgnd_merpar} we explained that parallel conjunctions are
implemented by spawning off the tail of the conjunction and executing the
head directly.
This means that in the right recursive case (Figure
\ref{fig:map_right_recursive}), the recursive call is spawned off as a
spark,
and that in the left recursive case (Figure \ref{fig:map_left_recursive}),
the recursive call is executed directly, and the loops \emph{body} is
spawned off.

\begin{table}
\paul{Make this easier to analyse by making the second column the actual
number of contexts, not contexts per engine.}
\begin{center}
\begin{tabular}{lr|rrrrrr}
\multicolumn{1}{c|}{Recursion type} &
\multicolumn{1}{c|}{Max no.\ of contexts} &
\multicolumn{2}{|c|}{Sequmential} &
\multicolumn{4}{|c}{Parallel w/ $N$ Engines} \\
\Cbr{} & & \C{not TS} & \Cbr{TS} & \C{1}& \C{2}& \C{3}& \C{4}\\
\hline
\multirow{5}{*}{Right} &
 2      & 23.8       & 20.0     & 22.0 & 22.0 & 22.1 & 22.1 \\
&32     & -          & -        & 22.0 & 22.1 & 22.1 & 21.7 \\
&64     & -          & -        & 22.0 & 20.3 & 19.0 & 17.0 \\
&128    & -          & -        & 22.0 & 13.7 &  8.7 &  6.7 \\
&256    & -          & -        & 20.0 & 12.5 &  8.7 &  6.8 \\
\hline
\multirow{5}{*}{Left} &
 2      & 23.7       & 21.9     & 22.0 & 22.0 & 22.0 & 22.0 \\
&32     & -          & -        & 22.3 & 22.0 & 22.0 & 21.9 \\
&64     & -          & -        & 22.0 & 21.9 & 20.9 & 18.8 \\
&128    & -          & -        & 22.0 & 18.8 & 15.0 &  9.0 \\
&256    & -          & -        & 22.0 & 18.5 & 12.4 & 10.0 \\
\end{tabular}
\end{center}
\caption{Right vs.\ left recursion.}
\label{tab:right_v_left}
\end{table}

Table \ref{tab:right_v_left} shows average elapsed time in seconds for the
mandelbrot image generating program over 20 samples.
The program iterates over the rows in the mandelbrot image using
a left or right recursive parallel
\code{map/3} predicate such as those in Figure
\ref{fig:map_right_and_left_recursive},
the leftmost column in the table indicates which predicate was used.
The next column describes how many contexts may exist at once per Mercury
engine.
Values are omitted in the range 3--31 as they are not interesting:
they are the same as the case for 2 contexts per engine.
The next two columns give the runtime for a sequential version of the
program without and with thread safty.
\paul{Write what this means.}
\paul{The TS column is broken, fix it}
The following four columns give the runtimes for the parallel mandelbrot
program with 1--4 Mercury engines.
All our benchmarking is performed on `cabsav',
an ASRock Z68-Pro3 based Intel i7-2600K system.

We can see that the right recursive program performs better than the left
recursive one.
It is also quite noticeable that there is a larger than expected amount of
variance in the results.
For example in the left recursive cases 256 contexts per engine
performance better than 128 contexts per engine,
except for the case with four engines.
In the 128 context-per-engine case the standard deviation is 4.3 seconds,
and in the 256 context-per-engine case the standard deviation is 5.6
seconds,
this means that these results cannot be compared meaningfully as the
reported mean may not represent the true mean.
Most of the other standard deviations are low, less than half a second.
The results vary more widly for the left recursive tests with 3--4 engines
and 128--256 contexts per engine.
This is not seen for the right recursive programs,
this leads us to beleive that it is due to how the left recursive programs
are scheduled.

Parallel conjunctions are executed by spawning off the second and later
conjuncts and executing the first conjunct directly.
This means that in the right recursive example the recursive call is spawned
off,
the spark for the recursive call will be placed on the global spark queue.
Another engine will wake up and take the spark from the global queue and
convert it to a context.
When it makes the recursive call it will execute the parallel conjunction
inside and perform the same process.

After creating the spark for the recursive call,
each of these contexts will execute \code{P(X, Y)}
(Figure \ref{fig:map_right_recursive})
followed by \joinandcontinue which blocks this context on the completion of
the recursive call.
As this process continues the number of contexts in memory builds up,
creating more stacks that consume more and more memory.
This continues until the number of contexts in memory plus
the number of sparks is equal to the maximum number of contexts.
Without this limit a simple loop can easily consume all the physical memory
in a system.
Once this limit is exceeded sparks are always placed on the local context's
queue, and will therefore only be executed by the context's current engine.
This limits the amount of parallelism in the program,
it is why performance improves as we allow more contexts per engine in
Table \ref{tab:right_vs_left}.
We will address the memory usage problem and context limit in
Chapter \ref{chap:loop_control}.

In the left recursive program,
scheduling is quite different.
The parallel conjunction creates a spark for \code{P(X, Y)} and executes the
recursive call directly.
The recursive call will very quickly begin execution of the next parallel
conjunction and spark off another call to \code{P}.
By the time another engine wakes up and begins taking work from the global
queue a number of sparks will have been placed on the queue.
Because a spark can be convered into a context (even though an existing
cotext may be used),
the number of sparks on the global queue contributes towards the maximum
number of contexts aloud.
If the context limit is low, many sparks will be placed on the current
context's spark queue and will be executed sequentially rather than in
parallel.
The left recursive mandelbrot program runs more slowly than the right
recursive program,
this is because all the sparks for the whole program are created at the
beginning of the programs execution.


% XXX: Here's where I'm working.

%XXX
Parallel conjunctions are evaluated as described in Section
\ref{sec:backgnd_merpar}.
Sparks are added to the global spark queue if the global queue has room,
otherwise they are pushed onto a context-local spark stack.
Creating a lot of parallel work and using a single global work queue can be
pesimistic,
the queue itself can become a bottleneck:
when many processors try to access it the same location in memory there will
be many cache misses and delays.
This is why \citet{wang_hons_thesis} choose to introduce local queues,
and place work on them when there is a surplus of work on the global queue.

When a engine finishes executing a context and reaches the barrier at the
end of a parallel conjunct,
it will check the context's local spark stack for any other work and attempt
to execute it.
Otherwise, it will either save the context to resume later or release the
context before checking the global spark queue and global context queue.

XXX Algorithm.

Deciding too early, results.


\section{Work stealing}

\subsection{Initial work stealing implementation}

This subsection was joint work with Peter Wang.

\subsection{Engine local spark stacks}

\section{Right vs.\ left recursion}

\section{Thread pinning}

Detecting the number of processors.

Thread pinning

SMT

Busy waiting?

Hardware locality

\section{Idle loop}

Idle loop structure

Independent engine wakeup

Engine work notification

\section{Scheduling tweeks}

Not implemented,

Feedback from threadscope?

\section{Garbage collector tweeks}

Large initial heap

Marking \& cache thrashing

Local free lists

\section{Proposed kernel support to manage processor resources}

\status{This may not be worth discussing until someone actually does it}
    
Should I describe our proposal for OS kernel's to help
applications with how many threads to use.
GCD is related but does not fit into a language runtime system so
easily~\cite{apple_gcd}.
See also N:M threading.

\section{Spare text}

\status{This text will be moved up into one of the work stealing sections
once those sections are ready}

%The spark is added to a global run queue of sparks, or if that queue is too
%full, because there's already enough parallelism,
%then the spark is added to a queue owned by the current context.
%\citet{wang_hons_thesis} intended to use the local queues for work stealing
%but had not completed his implementation,
%The work-stealing dequeue structure
%is described in \citet{Chase_2005_wsdeque}.
%see Chapter \ref{chap:rts} for details.
%
%and finds that spark is still at the head of its queue,
%it will pick it up and run it itself.
%This is a useful optimisation,
%% it is also really well-known.
%since it avoids using a separate context in the relatively common case
%that all the other CPUs are busy with their own work.
%This optimisation is also useful since it can avoid the creation of superfluous
%contexts and their stacks.
%

When an engine becomes idle, it will first try
to resume a suspended but runnable context if there is one.
If not, it will attempt to run a spark from the global spark queue.
If it successfully finds a spark, it will allocate a context,
and start running the spark in that context.

% XXX: Mention global spark queue and spark sheduling above.
% XXX:  

Barrier code is placed at the end of each conjunct,
this is named \code{join\_and\_continue} (Figure \ref{fig:par_conj}).
This code starts by atomically decrementing the number of outstanding
conjuncts in the conjunction's syncterm and checking the result for zero
(the whole operation is thread-safe, not just the decrement).
Algorithm \ref{alg:join_and_continue} shows the pesudo code for
join\_and\_continue.

