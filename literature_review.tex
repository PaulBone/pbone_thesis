
%\paul{
%This should just 'set the stage' for my work.  I would
%like to leave the contrasting discussions until later in the
%thesis, perhaps at the end of each major chapter.
%}
%
%\paul{
%I do not want to introduce all my citations here (the way Liz has to
%due to the APA-formatting standard).  Only enough that the user
%understand the context for reading the research chapters.  So,
%closely related work should be introduced for the first time when
%it is compared with my work.
%}

\status{Ready for proof reading.}

In the rest of this chapter we will explore the literature concerning
parallel programming.
The languages in which parallelism is used have a strong influence on the
semantics and safety of the parallelism.
We group languages into the following two classifications.

\begin{description}

    \item[Imperative] programming languages are those that allow
    \emph{side-effects}.
    A side effect is an action that a computation may perform without
    where there is no declaration of the possibility of the action.
    This can include input/output (IO) and modification of data structures.

    \item[Pure declarative] programming languages are those that forbid
    side-effects.
    The effect of executing any computation must be declared in the
    computation's signature.
    Because all effects are declared, there are no side-effects.
    This has numerous benefits, we are interested in the specific benefit
    that this makes it easy for both compilers and programmers to understand
    if it is safe to parallelise any particular computation.

\end{description}

\noindent
Not all researchers agree on the definitions above.
However these definitions are exactly what is required when discussing
the safety of parallelism.
Additionally a language clearly fits into one category or the other.

There are many ways to introduce parallelism in computer programming.
We find it useful to create the following classifications for these methods.

\begin{description}
    \item[Parallelism via concurrency]:
    concurrency can be used to introduce parallelism.
    While these concepts are related and often discussed together they are
    orthogonal.
    We will explain this and discuss the systems that use concurrency in
    order to achieve parallelism (\emph{parallelism via concurrency})
    (section \ref{sec:intro_concurrency}).

    \item[Explicit parallelism] is safer and easier to use compared to
    concurrency, however it is less expressive.
    We will explain how explicit parallelism is distinct from and improves
    upon \emph{concurrency for parallelism}.

    \item[Implicit parallelism] is a category that includes
    systems that attempt to make parallel execution the typical form of
    execution.
    We describe a number of systems and explain why the general approach is
    flawed.
    Some systems use do not fit cleanly into either of our explicit
    or implicit parallelism definitions.
    We will discuss both approaches together in section
    \ref{sec:intro_par}i.

    \item[Automatic parallelism] is our last category.
    In these systems sequential execution is the typical form of execution
    and parallel execution is used sparingly.
    Many authors often group these systems with implicit parallelism systems.
    We have chosen to separate them because the two systems
    approach the same problem from different directions
    (parallel as default vs.\ sequential as default).
    Prior work in automatic parallelism is discussed in section
    \ref{sec:intro_auto_par}.
\end{description}

\noindent
We explore the literature in this order as each successive category builds
on the work of the previous category.
Our own system is an automatic parallelism system;
it makes use of concepts described in all four categories.
Therefore
we cannot discuss it meaningfully until we have explored the literature
from giants upon whose shoulders we are standing.


\section{Parallelism via concurrency}
\label{sec:intro_concurrency}

% Parallelism and concurrency.
It is important to distinguish between \emph{parallelism} and
\emph{concurrency}.

\begin{description}
    \item[Concurrency] is defined as multiple computations executing simultaneously,
    which may communicate with one another \citep{hoare:1978:csp}.
    As an example consider a single-threaded web server which is required to
    handle multiple requests concurrently.

    \item[Parallelism] occurs when a program uses multiple cores or other
    components of a computer to get more work done in the same amount of
    time.
    A suitable example may be a program that uses SIMD instructions to
    multiply matrices, performing several basic operations with a single
    instruction.
\end{description}

\noindent
Often parallelism is achieved through concurrency
which leads to confusion among many programmers between the terms.
For example,
multiple concurrent processes executing on a multicore processor in parallel
is both concurrent and parallel.

Concurrency such as \citet{hoare:1978:csp}'s
communicating sequential processes (CSP)
or \citet{milner:pi}'s $\pi$-calculus
provide the programmer with a natural way to express many problems,
such as the web server example above.
In these cases concurrency is (and should be) used to make the program
easier to write.
If the concurrency uses parallelism provided by the hardware many programs
will speed up due to this parallelism,
but not all.

Often programmers will deliberately use concurrency in order to speed up
their programs;
we call this \emph{parallelism via concurrency}.
It often works when the program is easy to express using concurrency
(but we think there are better methods).
However when the problem cannot be expressed easily using concurrency,
many programmers will attempt to \emph{force} their program into a concurrent
model in the name of parallelism.
Even if a programmer manages to profitably parallelise their program,
they will usually find that the program is no longer easy to read, understand
and debug.
In this section we will discuss systems that encourage parallelism through
concurrency.
Our critique of these systems should not be misunderstood as a critique of
concurrency which in general is outside the scope of this dissertation.

% Threading.
The most common method of introducing concurrency is with \emph{threads}.
Some languages support threads natively such as Java \citep{java-threads}
and others such as C \citep{c} support it via a library.
Libraries for using threads with C include
POSIX Threads \citep{butenhof1997:pthreads} or Windows
Threads \citep{winthreads}, depending on the operating system.
These libraries tend to be low level;
they simply provide an interface to the operating system.
Different threads of execution run independently,
sharing a heap and static data.
Because they share these areas where data may be stored they can, and often do,
use the same data (unlike CSP).
Threads communicate by writing to and reading from the shard data.
Regions of code that do this are called \emph{critical sections},
and they must be protected from concurrent access:
a thread reading shared data while another writes to it, will read
inconsistent data;
and when two threads write to the same data some of the changes to the data
may be lost.
Synchronisation such as \emph{mutual exclusion locks} (mutexes)
\citep{Dijkstra:Mutex} must be used avoid concurrent access.
Upon entering a critical section a thread must acquire the lock associated with
the data,
it must release the lock when it leaves the critical section and the lock
ensures that only one thread may hold the lock at a time.
The programmer is responsible for defining critical sections and using locks
correctly.

Mutual exclusion locking is problematic,
it is too easy to make mistakes when using locks.
Frequent mistakes include: forgetting to synchronise access to a critical
section,
making the critical section too small, or too large,
these can lead to incorrect program behaviour, inconsistent memory states and
crashes.
When using multiple locks in a single critical section,
because that critical section uses multiple resources,
the locks must be acquired in the same order in each such critical section
otherwise deadlocks can occur.
This also prevents critical sections from being nestable,
causing significant problems for software composability.
All of these problems are difficult to debug because their symptoms are
intermittent.
Often so intermittent that introducing tracing code or compiling with debugging
support enabled can prevent the problem from occurring,
this is humorously known as a \emph{heisenbug} ---
a testament to just how frustrating it can be to find.
While it is possible to deal with these problems,
we believe that it is better to make problems impossible;
doing so guarantees that the programmer takes on fewer risks.

% Message passing.
Another popular model is \emph{message passing}.
Unlike threads, message passing's \emph{processes} do not share static or heap
data,
processes communicate by passing messages to one another.
The major benefit of message passing is that it does not make assumptions about where processes
are executing:
they can be running all on one machine or spread-out across a network.
Therefore, message passing is very popular for high performance computing,
where shared memory systems are not feasible.
Message passing also avoids the problems with threading and mutexes,
however other synchronisation techniques can introduce a subset of the
problems that threading suffers.
For example it is possible to create deadlocks with cyclic dependencies
between messages:
a process $a$ will send a message to $b$ but only after it has received a
message from $b$,
and $b$ will send a message to $a$ but only after it has received its message.
(We can create a similar situation with locks.)
Most deadlocks are more subtle and harder to find than this trivial example.

Notable examples of message passing application programming interfaces (APIs)
for C and Fortran \citep{backus:1957:fortran} are
MPI \citep{mpi} and PVM \citep{pvm}.
Every process can send and receive messages with any other process,
We can think of this in terms of \emph{mailboxes}:
each process owns (and can read from) its own mailbox and can place messages
in any other mailbox.
MPI and to a lesser extent PVM support powerful addressing modes
such as broadcasting the same message to multiple processes, or
distributing the contents of an array evenly between
multiple processes.
These features make MPI and PVM very powerful interfaces for high performance
computing.
However, the programmer is responsible for encoding and decoding messages,
which adds extra flexibility can be very tedious.
The Erlang language \citep{erlang} supports message passing natively and
encoding and decoding is done for the programmer.
Erlang allows processes to be created dynamically,
making it more flexible than MPI or PVM.
These two differences make it easier to program in Erlang than C with MPI or
PVM.
Erlang is typically used where fault tolerance is important,
where as MPI and PVM are used for high performance such as in scientific
computing.
All three systems support distributed computing.

% CSP
A different interpretation of message passing is
communicating sequential processes \citep{hoare:1978:csp}.
Even though we introduced PVM and MPI first, CSP predates them by 11 years.
In the CSP model communication is performed through \emph{channels},
channels and processes may be created dynamically.
Channels are more powerful than mailboxes as multiple processes can read from
the same channel (but not at the same time).
A process can also set up multiple channels for different types of message,
the same benefit can be gained by tagging messages in the mailbox analogy.
The first language based on CSP is occam \citep{occam1, occam3}.

\citet{milner:pi}'s $\pi$-calculus is similar to CSP.
The difference between the two process algebras is that in the $\pi$-calculus
channels are first class, meaning that a channel can be sent in a channel.
This allows programmers to compose a dynamic communication network.
%The Jomel language \citep{jomel} (an ML \citep{ml}) is based on the $\pi$-calculus
Jocaml \citep{jocaml} (an Ocaml
\citep{ml-types, ocaml-modules, ocaml-bytecode, ocaml-native}
derivative) is a language based on the related join-calculus
\citep{join-calculus}.
Join-calculus differs by using asynchronous communication and adding a number
of restrictions, which are not important to our discussion.
Jocaml differs from Occam in the same ways that the join-calculus differs
from CSP.
However Occam, in particular its first version \citep{occam1},
was intended as a research tool to experiment with CSP, Jocaml was not.
Later versions of occam \citep{occam3} were designed for more practical use.
Jocaml is more widely used and appears to be much more practical.
Jocaml, like Erlang, is a powerful language with support for distributed
computing.

There are many other languages and libraries based on these process algebra,
one that is currently enjoying a lot of popularity is Go
\citep{balbaert:2012:go}.
Go appears to follow a model similar to the $\pi$-calculus and the
join-calculus.
However Go also uses shared memory without providing any synchronisation
for communicating through shared memory.
In particular data sent in messages is sent \emph{by reference},
meaning that if one process modifies data that has been shared,
other processes can see the modification and therefore read inconsistent data
or write inconsistent data into memory.
Programmers are discouraged from using shared memory for communication,
however this makes several classes of bug possible through accidental use of
shared memory.
We suspect that the developers made this decision to make Go perform faster on
multicore systems.
It is not clear if this is part of the language specification or simply
Google's implementation.

% STM
All the systems we have introduced so far are prone to deadlocks,
and many are prone to other problems.
Many problems can be avoided using software transactional memory (STM)
\citep{stm}.
STM is similar to the threading model, it includes the concept of critical
sections.
Critical sections are not protected by mutexes,
instead optimistic locking is used.
Variables that are shared between threads are known as STM variables,
they may only be modified within a critical section and a good implementation
will enforce this.
STM works by logging all the reads and writes to STM variables within a
critical section, and not actually performing them until the end of the
critical section.
At that point the STM system acquires a mutex for each STM variable (in the
order that they were accessed)
and checks for sequential consistency by comparing its log with the current
values of the variables.
If no inconsistencies are found
it may go ahead and modify and unlock the variables.
Otherwise it must retry the critical section or fail.
Compared to the threading model,
the user does not need to use mutexes and critical sections are now
nestable (two nested sections commit when the outer one commits).
STM does not suffer from deadlocks.
However, programmers must still place their critical sections correctly in
their program.
A common criticism of STM is that it performs poorly when there is a lot of
contention as a failed critical section must be re-executed and this often
adds to the amount of contention.
Including side-effecting computations inside an STM critical section can also
be a problem.
This can be avoided in a pure declarative language such as
Mercury \citep{mercury_jlp} or Haskell \citep{haskell98} as side effects cannot
be expressed.
\citet{mika:mercury-stm} is an implementation for Mercury
and \citet{harris:2005:haskell-stm} is an implementation for Haskell.

\section{Explicit and implicit parallelism}
\label{sec:intro_par}

% Parallelism w/o concurrency in imperative languages.
Parallelism can also be achieved without concurrency.
This is done with language annotations that do not affect the (declarative)
semantics of the program.
The annotations describe \emph{what} should be parallelised,
and rarely describe \emph{how} to parallelise each computation.
We will refer to this as \emph{explicit parallelism}
since parallelism is expressed directly by the programmer by the explicit
annotations.

OpenMP \citep{openmp} is a library for C++ \citep{cplusplus} and Fortran
that allows programmers to annotate parts of their programs for parallel
execution.
In OpenMP most parallelism is achieved by annotating loops.
C++ is an imperative language and therefore the compiler cannot determine
which computations are safe to parallelise.
Therefore the programmer is responsible for guaranteeing that the iterations
of the loops are independent,
that is to say, that no iteration depends on the results of affects of any of
the previous iterations.
If an iteration makes a function call, then this guarantee must also be true for
the callee all transitive callees.
The programmer must also declare which variables are shared, local or used
for reductions.
This is necessary so that the compiled code is parallelised correctly.
The programmer is invited to specify how the iterations of the loop may be
combined into parallel tasks (known as \emph{chunking}).
This means that to some extent the programmer still describes \emph{how} to
parallelise the loop,
although the programmer has much less control than with parallelism through
concurrency systems.
OpenMP is one of the two explicit parallelism systems that we review that
does this.
Describing how something should be parallelised is rare.
Provided that the programmer can annotate their loops correctly,
explicit parallelism with OpenMP avoids the problems
with threading and message passing.

% Declarative languages.
In pure declarative languages
a function has no side effects,
all possible effects of calling a function are specified in the function's
declaration, (this can be considered an upper bound).
This is often done by expressing effects as part of the type system such as
with:
Monads \citep{haskell-monads}, which are used by Haskell \citep{haskell98}
or
uniqueness-typing, which is used by
Mercury\footnote{
    Mercury's uniqueness typing is part of its mode system.
    Despite this, it is still considered uniqueness typing.}
\citep{mercury_jlp}
and Clean \citep{brus:1987:clean}.
There are also some very novel approaches such as effect-typing \citep{ddc}.
In any of these systems both the programmer and compiler can trivially
determine if parallel execution of a particular computation is safe.
Furthermore,
in Haskell, Mercury and Clean
the concept of a variable is different from that in imperative programming,
it is not a storage location that can be updated,
but a binding of a name to a value.
Therefore variables that are shared
(those that we would have to declare as shared in OpenMP)
are really shared immutable values\footnote{
    We find it amusing that the misnomer ``variable'' is used at all.}.
Explicit parallelism in declarative languages is much easier and safer to use
than in imperative languages.
The next two sub-sections describe parallelism in functional and logic
languages respectively.

%Pure declarative languages like Mercury~\citep{mercury_jlp},
%Haskell~\citep{haskell98} and Clean~\citep{1991:concurrent-clean} do not
%allow side-effects.
%\paul{I should be able to find a more useful citation for Clean.}
%This is done by ensuring that a function's declaration declares all of the
%functions effects.
%Therefore,
%the programmer or the compiler can read a function's declaration to see if it
%is safe to parallelise.
%Mercury and Haskell both support explicit parallelism,
%the programmer need only annotate their code, indicating that it should be
%executed in parallel.

\subsection{Parallelism in functional languages}
\label{sec:intro_par_func}

% \paul{Concurrent Lisp}
% Concurrent lisp clearly uses concurrency as in the previous section.

Pure functional languages like Haskell and Clean can be evaluated using a
process known as \emph{graph reduction}.
Graph reduction works by starting with an expression and a program and
applying reductions.
Reductions use an equation in the program to replace part of the expression
with a new expression.
In a lazy language (or lazy implementation of a non-strict language)
each reduction step performs \emph{outermost reduction}.
This means that the arguments of a function are not normally evaluated
before a reduction replaces the function call with an equation for that
function.
Some reductions require the evaluation of an argument,
these include primitive operations and pattern matching.
The system continues applying reductions until the expression is
a value;
the value may have parameters which are still expressions (they have not
been evaluated).
This is called weak head normal form (WHNF);
Graph reduction itself can be parallelised:
If a subgraph (an argument in the expression) is known to be required in the
future,
then it can be evaluated in parallel using the same graph reduction process.
Several projects experimented with this technique
\citep{augustsson:1989:parallel-graph-reduction,burn:1989:parallel-reduction-machine,peyton-jones:1989:parallel-graph-reduction}.

Because the programmer does not need to annotate their program this
parallelism is implicit.
While implicit parallelism is always safe there is a serious performance
issue.
The cost of creating a parallel evaluation is significant,
so is any communication between parallel tasks.
There are also distributed costs throughout the execution of these systems,
for example many parallel graph reduction systems introduce locking on every
graph node.
Such locking increases the overheads of evaluation even when no parallel
tasks are created.
Therefore parallelised computations should be large enough that the benefit
of evaluating them in parallel outweighs the costs of doing so.
Unfortunately most parallelism in an implicit parallel system is very
fine grained, meaning that it is not large enough to make parallel
evaluation worthwhile.
There is also a lot of this parallelism,
this means that the system spends most of its time managing the parallel
tasks rather than evaluating the program.
This is called embarrassing parallelism as the amount of fine-grained
parallelism can cause the evaluation to perform \emph{more slowly} than
sequential evaluation.
We will discuss other implicitly parallel systems,
most of them share the same problems.
Many systems such as \citet{peyton-jones:1989:parallel-graph-reduction} go
to lengths to exploit only course grained parallelism.
In our opinion the problem with implicit parallelism is that it approaches
the parallel programming challenge from the wrong direction:
it makes everything execute in parallel \emph{by default} and then attempts
to introduce sequential execution;
we believe that one should make sequential execution the default and then
attempt to introduce parallel execution \emph{only where it is profitable}.

In contrast explicit parallelism allows the programmer to introduce
parallelism only where they believe it would be profitable.
This means that the default execution strategy is sequential.

Multilisp \citep{halstead:1984:multilisp,halstead:1985:multilisp} is an
explicitly parallel implementation of Scheme.
Like scheme, Multilisp is not pure, it allows side effects.
This means that as programmers introduce parallelism, they must be sure that
parallel computations cannot be affected by side effects,
and do not have side effects themselves.
Therefore multilisp is not safe as side effects may cause inconsistent
results or corrupt memory.
Multilisp programmers create parallelism by creating delayed computations
called \emph{futures}.
These computations will be evaluated concurrently
(and potentially in parallel).
When the value of a future is required,
the thread blocks until the future's value is available.
Many operations such as assignment and parameter parsing do not require the
value of a future.

Haskell is a pure functional language.
Being pure it does not have the problem with side effects that multilisp
does above.
The Haskell language standard specifies that it is non-strict\footnote{
    Non-strict evaluation is more broadly defined than lazy evaluation};
in practice however, all implementations use lazy evaluation in most cases.
Patterns and cases where a sub-expression's value is always required are
evaluated eagerly.
Haskell programmers can introduce explicit parallelism into their programs
with two functions (\code{par} and \code{pseq}).
\citet{gph:gum,loidi:2008:gph-significant-parallelism} first added
these functions to GUM (A parallel Haskell implementation).
Then \citet{harris:2005:haskell-smp} added them to the
Glasgow Haskell Compiler (GHC),
arguably the most popular implementation of Haskell.
The \code{par} and \code{pseq} functions are have the types:

\begin{verbatim}
par :: a -> b -> b
pseq :: a -> b -> b
\end{verbatim}
They both take two arguments and return their first;
their declarative semantics are identical to the \emph{const} function.
However their \emph{operational} semantics are different,
the \code{par} function may spawn off a parallel task that evaluates its
second argument to WHNF,
before returning its first argument.
While the \code{pseq} function will evaluate its second argument to WHNF
\emph{and then} return its first argument.
We can think of these functions as \code{const} with different evaluation
strategies.

Unfortunately lazy evaluation strategy interacts poorly with
parallel evaluation.
Because parallel tasks are only evaluated to WHNF,
most tasks do not create enough work to make parallelisation worthwhile.
The developers tried to reduce this problem by allowing programmers to
specify an evaluation strategy,
which describes how deeply to evaluate expressions \citep{trinder:98:strategies}.
his description is written in Haskell itself,
this allows it to be implemented as a library and describe higher order
evaluation strategies.
This does not solve the problem,
it only gives the programmer a way to deal with it when they encounter it;
it is better to make it impossible for the problem to occur,
for example by restricting parallel computations to eager evaluation.
Similar problems occur with non-strict evaluation in sequential programs.
Evaluation strategies, in particular eager vs non-eager,
are a matter of personal preference.

An alternative Haskell library called Monad.Par \citep{marlow:monadpar}
requires parallel computations to be evaluated in full.
However the programmer must provide typeclass instances for each of their
own types that describe how to completely evaluate each type.
While this still requires a non-trivial amount of effort from the
programmer,
it is harder for the programmer to make mistakes.
Their program will not compile if they forget to provide a typeclass
instance.

\subsection{Parallelism in logic languages}
\label{sec:intro_par_logic}

Prolog is the most well known logic programming language.
Although logic programming has many sub-paradigms we will discuss
those that use selective linear resolution with definite clauses
(SLD resolution) \citep{kowalski_sld}.
SLD resolution attempts to answer a query by finding a Horn clause whose
head has the same atom name as the query,
performing variable substitution and then performing a query on each
conjunct in the clause's body.
If a query of a conjunct fails then the clause fails and an alternative
clause may be tried.
As multiple clause heads may match the query there may be more than one
solution,
and as the query may fail there might be zero solutions.
As such each query may have any number of solutions.

OR-parallel Prolog implementations may attempt to resolve a query against
multiple clauses in parallel.
Like parallel graph reduction this form of implicit parallelism can create
embarrassingly parallel workloads.
OR-parallelism has some additional problems.
Not all programs will provide enough OR-parallelism as many do not perform
non-deterministic searches.
Furthermore OR-parallelism is speculative:
the current goal may be executed in a context where only the first result
will be used,
therefore searching for more results in parallel is a waste of resources.
Systems that exploit OR-parallelism include
Aurora \citep{lusk:1990:aurora},
Muse \citep{ali:1990:muse}
and \&ACE \citep{gupta:1991:ace} (which also supports independent AND-parallelism).

AND-parallel Prolog implementations introduce parallelism by executing
conjuncts within clauses in parallel.
AND-parallelism comes in two varieties, independent and dependent.
independent AND-parallelism is much simpler, it only parallelises goals
whose executions are independent.
Independent AND-parallelism comes in two flavours, strict and non-strict.
Strict (also called restricted) independent AND-parallelism is where
conjoined goals that may be evaluated in parallel must have no variables in
common, including input variables.
It is trivial for a Prolog system to statically determine that the goals are
independent.
Non-strict (unrestricted) independent AND-parallelism is where some variables
are shared but none of the goals performs any instantiation on these variables
or anything they reference.
Examples non-strict independent AND-parallel Prolog systems include \&-Prolog
\citep{Hermenegildo:1991:and-parallel,
DBLP:journals/jlp/MuthukumarBBH99}
and \&ACE \citep{gupta:1991:ace}.
As non-strict parallelism is more general it is desirable for a system to
handle it.
Determining which goals are independent is much more complicated and
requires analysis;
\citet{DBLP:journals/tcs/GrasH09} and \citet{Hermenegildo1995} describe
suitable analyses.
Some of the analysis can be done at compile time,
however it is a whole program analysis which means that separate compilation
cannot be used.
Furthermore the analysis is often incomplete:
in most systems, checks are needed at runtime and these checks may traverse
large data structures.
The overheads of executing these checks
(which must be execute regardless of their outcome)
can outweigh the benefit of parallel execution.

\plan{Managing logic variables and retracting their values}
Prolog supports \emph{logic variables},
When two or more free variables are unified with one another they behave as one
variable;
any later successful unification on any of the variables implicitly modifies
the state of the other variables.
During sequential execution
part of the search space may bind values to variables then,
if it fails it must retract any bindings it made so that alternative
branches of the search tree are free to bind other values to variables.
With OR-parallelism a variable may be bound to distinct values in different
parts of the search tree at once.
There are many different ways to implement this,
for example Muse stores variables for different parallel parts of the search
tree in different address spaces \citep{ali:1990:muse}.
This introduces extra work such as copying the states of computations as new
work is executed in parallel.
Often OR-parallelism is used together with independent AND-parallels
such as in PEPSys \citep{baron:1988:pepsys},
these systems can also manage variable bindings in a similar way.

While it is straightforward to implement this correctly for OR-parallel
and independent AND-parallel systems;
it is much more difficult for dependent AND-parallel systems as
they must handle communication between parallel conjuncts.
Most systems allow parallel conjuncts to communicate variable bindings as
soon as they are produced.
This allows non-deterministic search computations that consume those bindings to
detect failure earlier and therefore execute more efficiently.
However if the producer of a binding is also part of a non-deterministic
search,
then if it fails any variable bindings made in the failed part of the tree
must be retracted.
Implementing this efficiently is difficult.
Andorra Prolog \citep{haridi:1990:andorra} handles models the
logic program as a Herbrand constraint problem.
It then solves that problem, introducing parallelism for
deterministic\footnote{
    Andorra uses a different definition for determinism than Mercury does
    (section \ref{sec:backgnd_mercury} explains Mercury's definition}
conjoined goals.
For Andorra's purposes deterministic means ``succeeds at most once''.

Another solution to this problem involve restricting the Prolog language
by removing backtracking.
This creates a committed-choice logic language such as
Concurrent Prolog \citep{saraswat86:concurrent_prolog_definition,
shapiro:flat_concur_prolog};
Parlog \citep{clark:84:parlog_sys_prog,clark:86:parlog}
and Guarded Horn Clauses (GHC) \citep{ueda:ghc}.
When these systems encounter multiple clauses during execution they support
committing to only one of the clauses.
Each of these system uses guarded clauses, which have the
form:
$H \leftarrow G_1 \wedge \ldots \wedge G_n : B_1 \wedge \ldots \wedge B_m$
where $G_i$ and $B_i$ are atoms in the guard and body of the
clause;
$H$ is the head of the clause.
The evaluation of the body is committed to if and only if the guard and any
unifications in the head are all true.
Each system handles this slightly differently.
In Concurrent Prolog clauses may execute but only once a clause is committed
to will any variables it has bound become visible to other parallel
evaluations.
This can cause the communication of variable's values to be delayed,
allowing other parallel evaluations to run ahead and possibly perform
speculative execution.
GHC solves this by suspending a computation if a unification in its guard
would cause a variable visible outside the procedure to become more
instantiated.
When another parallel computation updates the variable such that the
unification can proceed by only performing a simple test,
then the execution of the clause is resumed.
However suspending and resuming of tasks can be inefficient,
especially when any unification with most variables can cause a task to
suspend or resume.
(Even unifications in clause bodies can cause a computation to be suspended
as the predicate to which the clause body belongs may have been called from
another clause's guard.
Parlog also uses suspends computations,
however it determines when predicates should be suspended based on mode
information.
Goals that take a variable in an input mode are consumers, and may suspend
until a producer has unified the variable with something representing a
term, allowing the consumer to continue its execution.
In some ways this is analogous to Concurrent Prolog and GHC's usage of
variables,
as variables intended for output should be unified in the body of a clause,
and variables intended for input should be unified in the head or guard:
so a programmer in any of these three systems has to consider which of the
variables are input and which are output.
The programming style in committed choice languages is more imperative than
in traditional Prolog systems as the guard, and which goals are placed in
the guard, is extra-logical.

Most of these systems use implicit parallelism, parallelising almost
everything.
As we mentioned earlier this can lead to embarrassing parallelism.
To avoid this many of the above systems use some type of schedule analysis
to decide what is worth executing in parallel.
This can vary from using OR-parallelism near the top of a nondeterministic
search space \citep{hausman:1987:or},
to the compile time analyses performed by Ciao Prolog
\citep{hermenegildo_ciao} including \emph{granularity control}
\citep{lopez96:distance_granularity} (mostly compile time).

The concept of granularity control
\citep{debray:1990:granularity} is that only tasks
large enough such that their computational cost makes up for the cost of
parallel execution should be parallelised.
There are numerous different ways to estimate the runtime of a task.
%The runtime may also be measured in a previous execution of the program and
%this may provide an estimate for the runtime in the future
%\citep{sarkar:1989:feedback,harris,tannier};
%our own system, including the system we implemented before commencing the
%Ph.D.\ programme, \citet{bone:2001:honours}
For example \citet{lopez96:distance_granularity} constructs cost functions
at compile time which can be evaluated at runtime to give the cost of a
goal given the values of certain terms.
During compile time it uses the type, mode and determinism information that
the Ciao compiler makes available.
\citet{shen_98_granularity-control} created a different method for
granularity control.
They propose two methods for increasing the ``distance''
(and therefore runtime)
between two parallel job creation events.
The first method works at compile time and the second method works at
runtime.
The authors suggest they are used in cases where time complexity
analysis is difficult.

Granularity control and Ciao's conditional parallelism both help to
solve the problem of embarrassing parallelism.
However these systems tend to make parallel execution the \emph{default}
form of execution then use analyses such as Granularity control to
transform some of the parallel execution into sequential execution.
This approaches the problem from the wrong direction.
We believe that it is best to use sequential execution by default and
introduce parallelism only where it is likely to be beneficial.

Mercury,
a pure logic/functional language,
follows the principal of introducing parallelism only where it is
estimated to be beneficial.
\citet{conway:2002:par} introduced independent AND-parallelism
to Mercury.
Independence is tested using Mercury's strong mode system,
this system is much stronger than any of the systems mentioned above as
each predicate must have at least one \emph{known and precise} mode before
the compiler will accept it.
Modes may be declared or inferred.
Later \citet{wang:2006:hons} relaxed the requirement of independent
parallelism allowing Mercury to handle dependent AND-parallelism;
this was later published as \citet{wang:2011:dep-par}.
We will provide more detail about how this works in sections
\ref{sec:backgnd_merpar} and \ref{sec:backgnd_deppar},
for now we will concentrate on the differences between Mercury
and the Prolog and committed choice languages.
Firstly
Mercury allows parallelism only in deterministic code,
code with exactly one solution.
The strong determinism system provides this information.
Some of our critics (reviews of the paper \citet{bone:2011:overlap}
on which chapter \ref{chap:overlap} is based)
argue that this reduces the amount of parallelism that Mercury is able
to exploit \emph{too much}.
We disagree,
roughly 75\% of Mercury code is deterministic\footnote{
    This data was gathered by gathering determinism information for the
    largest open source Mercury program, the Mercury compiler.
}, and most of the rest is semi-deterministic
(it produces either zero or one solution).
While we are missing out on parallelising 25\% of the
program,
the 75\% that we can parallelise accounts for 97\% of the cost of the
program (table \ref{tab:recursion_types}).
Supporting parallelism in non-deterministic code to parallelise the
remaining 3\% of the program's cost is also prohibitively expensive.
The Prolog systems that support parallelism in goals that can fail and
goals with more than one solution must manage variable bindings
in very complicated ways;
this adds a lot of runtime cost.
In contrast, Mercury's simpler implementation allows it to handle a
smaller number of predicates, \emph{much} more efficiently.

The variable binding management in Prolog must also be applied to all
variables,
as most Prologs cannot determine which variables are shared in dependent
AND-parallel goals.
Mercury's implementation,
which uses futures (section \ref{sec:backgnd_deppar},
increases the costs of unification for the shared variables only.
A related comparison, although it is not specific to parallelism,
is that unification is orders of magnitude simpler in Mercury than in
Prolog as Mercury does not support logic variables.
This is also a deliberate design decision.

In contrast to Haskell,
Mercury's evaluation is eager SLD resolution.
Therefore Mercury's support for explicit parallelism is easier to use
than Haskell's;
it does not require programmers to use an equivalent of
the \code{pseq} function or describe evaluation strategies.


% \plan{Reform Prolog}
% Reform Prolog handles loops cleverly, but I need to find a paper for it.

\subsection{Other language paradigms}
\label{sec:intro_par_other}

Some languages allow parallelism in other forms,
or are created specifically to make parallelism easy to expose.
Parallel languages that do not fall into one of the above paradigms are
assessed here.

Sisal \citep{feo:1990:sisal-report} is a data-flow language.
In some respects its syntax looks like that of a functional language
since has constructs for loops.
However functions are referentially transparent.
so it is also valid to call it a pure functional language.
Sisal supports implicit data-flow parallelism.
Sisal's inbuilt types include arrays and streams.
Streams have the same form as cons-lists (Lisp style) except that as one
computation produces results on the stream, another computation may be
reading items from the stream.
The concept of streams is not new here,
it has been seen in Scheme \citep{wizard-book} and probably other
languages.
A stream may have only one producer,
which makes them different from channels.
The other source of parallelism is the parallel execution of independent
loop iterations.
Sisal's for loops include a reduction clause;
this clause states how the loop is transformed into a result which may be a
scalar result, an array or stream.
The loop's body may not have any effects or make any variable bindings;
so the loop's only result is the one described by the result clause.
The Sisal project is unfortunately dead,
and information about the system is hard to find and access.
The Sisal language appears to be good for expressing data-flow
parallel programs.
It is more difficult to say anything about how well the system
transformed programs into executable code and how well it performed.

Data-parallel languages such as
NESL \citep{blelloch:95:nesl} and
Data Parallel Haskell (DpH)
\citep{dph:2007:status_report,dph:2008:harnessing_the_multicores}
parallelise operations on members of a data structure.
For example,
DpH's parallelises operations on its parallel array type.
Any operations (including nested operations) on these arrays are
executed in parallel.
Data-parallel languages are especially good at targeting SIMD hardware
including general purpose graphics processor units.
They also work well on SMP hardware.
While these systems are good at what they do,
they do not handle non data parallel problems well,
and data parallel programs are a very small subset of computer programs.

Data parallel and data-flow parallelism does not have the same
problems as implicit parallelism.
This is because data parallel and data-flow parallel programs have a
more regular structure than logic and functional programs.
Data parallel and data-flow parallel languages are able to take
advantage of the program's structure and compile efficient parallel code.


\section{Automatic Parallelisation}
\label{sec:intro_auto_par}

As we mentioned above,
automatic parallelism is often grouped with implicit parallelism.
However there is an important difference.
Most implicit parallelism systems attempt to parallelise almost all
computations against one another;
parallel execution is the default form of execution.
In contrast,
automatic parallelism systems make sequential execution the default form
of execution and then determine which, if any, computations should be
executed in parallel.

Programmers using an explicit parallelism system could work from the
same principle;
they can start with a sequential program and introduce parallelism to
speed it up.
However this requires programmers to know and keep track of a lot of
information in order to determine if parallel execution is worthwhile
for any given computation.
In the context of profiling and optimisation,
it is widely reported that programmers are poor at identifying the
hot-spots in their programs or estimating the costs of their program's
computations.
Programmers who are parallelising programs must also understand the
overheads of parallelism such as:
spawning parallel tasks,
cleaning up completed parallel tasks,
operating system scheduling and
hardware behaviour such as cache effects.
Even if a programmer determines that parallel execution is worthwhile in
a particular case,
they must also be sure that adding parallel execution to their program
does not create embarrassing parallelism.
They must therefore understand whether there will be enough
processors free at runtime to execute each parallelised computation.
When parallel computations are dependent,
such as when Futures are used in Multilisp or
dependent parallelism is used in Mercury,
programmers must also estimate how the dependencies effect parallelism.
In practice programmers will attempt to use trial and error to find the
best places to introduce parallelism,
but an incorrect assumption of any of the above variables can prevent them
from parallelising the program effectively.

\citet{harris_07_feedback_imp_par} developed a profiler
feedback directed automatic parallelisation approach for Haskell programs.
They have reported speed ups of up to 80\% compared to the sequential
execution of their test programs on a four core machine.
However they were not able to improve the performance of some
programs, they attributed this to a lack of parallelism
available in these programs.
They have shown that automatic parallelisation is a promising idea for
improving the performance of software.
Their system works by measuring the execution time of thunks and using
thsi information to inform the compiler how to parallelise the program.
Any thunk whose execution time is above a particular threshold is
spawned off for parallel evaluation.
This has two problems, both are due to Haskell's lazy evaluation.
The first was mentioned above, all thunks are evaluated to WHNF,
which means that most thunks represent very little parallel work
(they will often create a structure which itself contains new thunks).
The second problem is that thunk execution is speculative:
when a thunk is created it is usually unknown whether the thunk's value
will actually be used;
and if it is known at compile time that the thunk will be used,
the Glasgow Haskell Compiler (GHC) will optimise it so that the computation
is not represented by a thunk at all and is evaluated eagerly.

% Jerome's work.
\citet{tannier:2007:parallel_mercury} previously attempted to automatically
parallelise Mercury programs using profiler feedback.
His approach selects the most expensive predicates
of a program and attempts to parallelise conjunctions within them.
This is one of the few peices of work that attempts to measure how
dependencies affect the amount of parallelism available.
Tannier's approach counts the number of shared variables in a parallel
conjunction and uses this as an analogue for how restricted the
parallelism may be.
However in practice most producers produce dependant variables late in
their execution and most consumers consume them early.
Therefore Tannier's calculation is naive:
the time that these variables are produced by one conjunct and consumed
by the other may not correlate with the number of dependant variables.
Tannier's algorithm is, in general, too optimistic
about the parallelism available in dependant conjunctions.
Tannier also makes use of compile-time granularity control to reduce the
over-parallelisation that can occur in recursive code.

Mercury's deep profiler \citep{conway:2001:mercury-deep} provides
detailed and accurate profiling information.
Among other things,
the deep profiler records separate profiling information for separate
uses of the same code.
To improve on Tannier's methods we aimed to calculate when the
producing conjunct is most likely to produce the dependant values and
when the consuming conjunct is likely to need them\footnote{
    This work was part of my honours project}
\citep{bone:2008:hons}.
To do this we modified the profiler so that it could provide enough
information so that we could calculate the likely production and
consumption times.
This early work was restricted to situations with a
single dependent variable shared between two conjuncts.
It is described in more detail in section \ref{sec:backgnd_autopar}.
In this prior work and in this Ph.D.\ dissertation we make heavy use of the
advanced features provided by Mercury's deep profiler.
More information about the profiler is provided in section
\ref{sec:backgnd_deep}.
No equivalent profiler exists for Haskell or Clean.
This and Mercury's purity make it the best environment for our research.

We anticipate that automatic parallelisation will be easy to use and
will parallelise programs more effectivly that most programmers can by
hand.
Furthermore as programs change costs of computations within them will 
change,
this may make manual parallelisations (using explicit parallelism) less
effective.
An automatic parallelisation system will make it easier to maintain
program as the automatic parallelisation analysis can simply be
re-executed to re-parallelise the program.

The remainder of the dissertation is organised as follows.
Chapter \ref{chap:backgnd} provides background information about
Mercury and explicit and automatic parallelism in Mercury.
Chapter \ref{chap:rts} discusses a number of improvements that we made to
the runtime system.
Chapter \ref{chap:overlap} describes our new automatic parallelism
analysis.
Chapter \ref{chap:loop_control} addresses a long standing problem in the
parallel execution of most recursive code.
Chapter \ref{chap:tscope} describes our use of the \tscope visual
profiler that was developed for use with Haskell,
we have adopted it for use with Mercury.
Finally chapter \ref{chap:conc} concludes the dissertation, providing a
discussion that unifies the work presented in the four main chapters and
describes potential further work.

