
%\paul{
%This should just 'set the stage' for my work.  I would
%like to leave the contrasting discussions until later in the
%thesis, perhaps at the end of each major chapter.
%}
%
%\paul{
%I do not want to introduce all my citations here (the way Liz has to
%due to the APA-formatting standard).  Only enough that the user
%understand the context for reading the research chapters.  So,
%closely related work should be introduced for the first time when
%it is compared with my work.
%}

\status{I probably need to come back to this and add more references.}

\section{Concurrency}
\label{sec:backgnd_concurrency}

% Parallelism and concurrency.
It is important to distinguish between \emph{parallelism} and
\emph{concurrency}.

\begin{description}
    \item[Concurrency] is defined as multiple computations executing simultaneously,
    which may communicate with one another \citep{hoare:1978:csp}.
    As an example consider a single-threaded web server which is required to
    handle multiple requests concurrently.

    \item[Parallelism] occurs when a program uses multiple cores or other
    components of a computer to get more work done in the same amount of
    time.
    A suitable example may be a program that uses SIMD instructions to
    multiply matrices, performing several basic operations with a single
    instruction.
\end{description}

\noindent
Often parallelism is achieved through concurrency
which leads to confusion among many programmers between the terms.
For example,
multiple concurrent processes executing on a multicore processor in parallel
is both concurrent and parallel.

Concurrency such as \citet{hoare:1978:csp}'s
communicating sequential processes (CSP)
or \citet{milner:pi}'s $\pi$-calculus
provide the programmer with a natural way to express many problems,
such as the web server example above.
In these cases concurrency is (and should be) used to make the program
easier to write.
If the concurrency uses parallelism provided by the hardware many programs
will speed up due to this parallelism,
but not all.

Often programmers will use concurrency in order to speed up their programs;
we call this \emph{parallelism through concurrency}.
It often works when the program is easy to express using concurrency
(but we think there are better methods).
However when the problem cannot be expressed easily using concurrency,
many programmers will attempt to \emph{force} their program into a concurrent
model in the name of parallelism.
Even if a programmer manages to profitably parallelise their program,
they will usually find that the program is no longer easy to read, understand
and debug.
In this section we will discuss systems that encourage parallelism through
concurrency.
Our critique of these systems should not be misunderstood as a critique of
concurrency which in general is outside the scope of this dissertation.

% Threading.
The most common method of introducing concurrency is with \emph{threads}.
Some languages support threads natively such as Java \citep{java-threads}
and others such as C \citep{c} support it via a library.
Libraries for using threads with C include
POSIX Threads \citep{butenhof1997:pthreads} or Windows
Threads \citep{winthreads}, depending on the operating system.
These libraries tend to be low level;
they simply provide an interface to the operating system.
Different threads of execution run independently,
sharing a heap and static data.
Because they share these areas where data may be stored they can, and often do,
use the same data (unlike CSP).
Threads communicate by writing to and reading from the shard data.
Regions of code that do this are called \emph{critical sections},
and they must be protected from concurrent access:
a thread reading shared data while another writes to it, will read
inconsistent data;
and when two threads write to the same data some of the changes to the data
may be lost.
Synchronisation such as \emph{mutual exclusion locks} (mutexes)
\citep{Dijkstra:Mutex} must be used avoid concurrent access.
Upon entering a critical section a thread must acquire the lock associated with
the data,
it must release the lock when it leaves the critical section and the lock
ensures that only one thread may hold the lock at a time.
The programmer is responsible for defining critical sections and using locks
correctly.

Mutual exclusion locking is problematic,
it is too easy to make mistakes when using locks.
Frequent mistakes include: forgetting to synchronise access to a critical
section,
making the critical section too small, or too large,
these can lead to incorrect program behaviour, inconsistent memory states and
crashes. 
When using multiple locks in a single critical section,
because that critical section uses multiple resources,
the locks must be acquired in the same order in each such critical section
otherwise deadlocks can occur.
This also prevents critical sections from being nestable,
causing significant problems for software composability.
All of these problems are difficult to debug because their symptoms are
intermittent.
Often so intermittent that introducing tracing code or compiling with debugging
support enabled can prevent the problem from occurring,
this is humorously known as a \emph{heisenbug} ---
a testament to just how frustrating it can be to find.
While it is possible to deal with these problems,
we believe that it is better to make problems impossible;
doing so guarantees that the programmer takes on fewer risks.

% Message passing.
Another popular model is \emph{message passing}.
Unlike threads, message passing's \emph{processes} do not share static or heap
data,
processes communicate by passing messages to one another.
The major benefit of message passing is that it does not make assumptions about where processes
are executing:
they can be running all on one machine or spread-out across a network.
Therefore, message passing is very popular for high performance computing,
where shared memory systems are not feasible.
Message passing also avoids the problems with threading and mutexes,
however other synchronisation techniques can introduce a subset of the
problems that threading suffers.
For example it is possible to create deadlocks with cyclic dependencies
between messages:
a process $a$ will send a message to $b$ but only after it has received a
message from $b$,
and $b$ will send a message to $a$ but only after it has received its message.
(We can create a similar situation with locks.)
Most deadlocks are more subtle and harder to find than this trivial example.

Notable examples of message passing application programming interfaces (APIs)
for C and Fortran are MPI \citep{mpi} and PVM \citep{pvm}.
Every process can send and receive messages with any other process,
We can think of this in terms of \emph{mailboxes}:
each process owns (and can read from) its own mailbox and can place messages
in any other mailbox.
MPI and to a lesser extent PVM support powerful addressing modes
such as broadcasting the same message to multiple processes, or
distributing the contents of an array evenly between
multiple processes.
These features make MPI and PVM very powerful interfaces for high performance
computing.
However, the programmer is responsible for encoding and decoding messages,
which adds extra flexibility can be very tedious.
The Erlang language \citep{erlang} supports message passing natively and
encoding and decoding is done for the programmer.
Erlang allows processes to be created dynamically,
making it more flexible than MPI or PVM.
These two differences make it easier to program in Erlang than C with MPI or
PVM.
Erlang is typically used where fault tolerance is important,
where as MPI and PVM are used for high performance such as in scientific
computing.
All three systems support distributed computing.

% CSP
A different interpretation of message passing is
communicating sequential processes \citep{hoare:1978:csp}.
Even though we introduced PVM and MPI first, CSP predates them by 11 years.
In the CSP model communication is performed through \emph{channels},
channels and processes may be created dynamically.
Channels are more powerful than mailboxes as multiple processes can read from
the same channel (but not at the same time).
A process can also set up multiple channels for different types of message,
the same benefit can be gained by tagging messages in the mailbox analogy.
The first language based on CSP is Occam \cite{occam}.

Similar to CSP is the $\pi$-calculus \cite{milner:pi}.
The difference between the two process algebras is that in the $\pi$-calculus
channels are first class, meaning that a channel can be sent in a channel.
This allows programmers to compose a dynamic communication network.
%The Jomel language \citep{jomel} (an ML \citep{ml}) is based on the $\pi$-calculus
Jocaml \cite{jocaml} (an Ocaml \citep{ocaml} derivative) is a language based
on the related join-calculus.
Join-calculus differs by using asynchronous communication and adding a number
of restrictions, which are not important to our discussion.
Jocaml differs from Occam in the same ways that the join-calculus differs
from CSP.
However Occam, in particular its first version, was intended as a research
tool to experiment with CSP, Jocaml was not.
Jocaml is more widely used and appears to be much more practical.
Jocaml, like Erlang, is a powerful language with support for distributed
computing.

There are many other languages and libraries based on these process algebra,
one that is currently enjoying a lot of popularity is Go
\citep{balbaert:2012:go}.
Go appears to follow a model similar to the $\pi$-calculus and the
join-calculus.
However Go also uses shared memory without providing any synchronisation
for communicating through shared memory.
In particular data sent in messages is sent \emph{by reference},
meaning that if one process modifies data that has been shared,
other processes can see the modification and therefore read inconsistent data
or write inconsistent data into memory.
Programmers are discouraged from using shared memory for communication,
however this makes several classes of bug possible through accidental use of
shared memory.
We suspect that the developers made this decision to make Go perform faster on
multicore systems.
It is not clear if this is part of the language specification or simply
Google's implementation.

% STM
All the systems we have introduced so far are prone to deadlocks,
and many are prone to other problems.
Many problems can be avoided using software transactional memory (STM)
\citep{stm}.
STM is similar to the threading model, it includes the concept of critical
sections.
Critical sections are not protected by mutexes,
instead optimistic locking is used.
Variables that are shared between threads are known as STM variables,
they may only be modified within a critical section and a good implementation
will enforce this.
STM works by logging all the reads and writes to STM variables within a
critical section, and not actually performing them until the end of the
critical section.
At that point the STM system acquires a mutex for each STM variable (in the
order that they were accessed)
and checks for sequential consistency by comparing its log with the current
values of the variables.
If no inconsistencies are found
it may go ahead and modify and unlock the variables.
Otherwise it must retry the critical section or fail.
Compared to the threading model,
the user does not need to use mutexes and critical sections are now
nestable (two nested sections commit when the outer one commits).
STM does not suffer from deadlocks.
However, programmers must still place their critical sections correctly in
their program.
A common criticism of STM is that it performs poorly when there is a lot of
contention as a failed critical section must be re-executed and this often
adds to the amount of contention.
Including side-effecting computations inside an STM critical section can also
be a problem.
This can be avoided in a pure declarative language such as
Mercury \citep{jlp:mercury} or Haskell \citep{Haskell} as side effects cannot
be expressed.
\citet{mika:mercury-stm} is an implementation for Mercury
and \citet{harris:2005:haskell-stm} is an implementation for Haskell.


\subsection{Explicit Parallelism}
\label{sec:back_par_explicit}

% Parallelism w/o concurrency in imperative languages.
Parallelism can also be achieved without concurrency,
a popular language and library for this is OpenMP~\citep{openmp},
which allows programmers to annotate parts of their programs for parallel
execution without describing \textbf{how} this parallelism should be achieved;
making it much easier for programmers to use.
We will refer to this as \emph{explicit parallelism}
since parallelism is expressed directly by the programmer by the explicit
annotations.
In OpenMP most parallelism is achieved by annotating loops.
However,
the programmer is responsible for guaranteeing that the iterations of the loops
are independent,
that is to say, that no iteration depends on the results of affects of any of
the previous iterations.
If an iteration makes a function call, then this guarantee must also be true for
the callee all transitive callees.
Explicit parallelism otherwise avoids the problems with threading and message
passing.

% Declarative languages.
One of the easiest ways to make explicit parallelism easier is to prevent
side-effects from occurring, usually by designing them out of the language.
Pure declarative languages like Mercury~\citep{mercury_jlp},
Haskell~\citep{haskell98} and Clean~\citep{1991:concurrent-clean} do not
allow side-effects.
\paul{I should be able to find a more useful citation for Clean.}
This is done by ensuring that a function's declaration declares all of the
functions effects.
Therefore,
the programmer or the compiler can read a function's declaration to see if it
is safe to parallelise.
Mercury and Haskell both support explicit parallelism,
the programmer need only annotate their code, indicating that it should be
executed in parallel.

Glasgow Parallel Haskell (GpH) allows programmers to request parallel
evaluation of certain expressions by annotating them with a function
whose operational semantics cause parallel
evaluation~\citep{gph,loidi:2008:gph-semiexplicit-parallelism}.
Unfortunately Haskell's lazy evaluation strategy interacts poorly with
parallelism.
A parallelised computation will be evaluated to weak-head-normal-form,
that is to say that if it is a data term with parameters,
the parameters will not be evaluated.
This means that any parallelised work usually does not do enough work to
make parallelisation worthwhile.
The GpH implementors have tried to solve this problem by allowing
the programmer to express how deep evaluation should
continue~\citep{trinder:98:strategies}.
An alternative Haskell library called Monad.Par~\citep{marlow:monadpar}
fully evaluates the result of any spawned-off computation.
However, the programmer must still describe how any of their own
data types can be fully evaluated.
These solutions do not really solve the problem,
they still require the programmer to introduce strictness.
\paul{I can add support to my argument by describing problems with
space leaks, for example foldl, foldr, and foldl'.
I want to avoid a rant or a religious war, but I do want to
objectively assess the literature.}
It is better to avoid these and many other problems completely by using
a strict language.

\label{ref:parallel_conjunction}
Mercury allows programmers to request parallel evaluation of
conjunctions by replacing the normal conjunction operator with the
parallel conjunction operator introduced by Thomas Conway in
\citep{conway:2002:par,wang:2006:hons,wang:2011:dep-par}.
Since Mercury is a strict language it does not have Haskell's
lazyness problems,
programmers simply annotate where parallel execution
should be used.
A more detailed description of parallelism in Mercury can be found in
section \ref{sec:backgnd_merpar}

When a problem is not naturally a concurrent problem 
explicit parallelism such as in Mercury and Haskell
is preferable as the programmer does not need to force their program
into a concurrent model.
Especially since in all but the STM cases of concurrency programmers
must also describe how parallel computations communicate and
synchronise.

\paul{I would like a citation here, I think I found something a while ago
about profiling}
However, explicit parallelism is a drawback because it requires the
programmer to know where their program spends most of its execution
time, it is understood that most programmers are poor at this.
Parallel execution has additional overheads such as:
spawning parallel tasks,
cleaning up completed parallel tasks,
operating system scheduling and
hardware behaviour such as cache effects.
The speedup gained when parallelising a computation will depend upon
these costs.
Programmers must therefore know whether parallelising a particular
computation is going to be an improvement in spite of the additional
costs of parallel execution.
Furthermore programmers must know whether there will be enough
processors free at runtime to execute the parallelised computation:
the additional costs of parallel execution will have an effect even
if there is not a processor available to execute the parallel work.
Parallelisation is another optimisation such as inlining or efficient
register allocation.
Therefore,
it would be better for an optimising compiler to handle parallelisation
automatically;
the programmer will not need to worry about parallel evaluation any more
than they currently worry about inlining and register allocation.

\subsection{Implicit Parallelism}
\label{sec:lit_implicit-parallelism}

Some computer languages support implicit parallelism,
in these languages many parts of programs are executed in parallel.
Parallel execution is the normal mode of execution,
it is used in most places within the program.

% Implicitly parallel prologs. (and OR-parallelism)
A number of parallel Prolog-like languages that were developed during the
1980's are classified as implicitly parallel languages.
These included Concurrent
Prolog~\citep{saraswat85:probl_with_concur_prolog,saraswat86:concurrent_prolog_definition,shapiro:flat_concur_prolog},
Parlog~\citep{clark:84:parlog_sys_prog,clark:86:parlog} and GHC~\citep{ueda:ghc}.
Nearly all tasks in these languages were carried out in
parallel,
as a result the overheads of parallel execution are typically
greater than the benefit of running most small tasks in parallel.
Furthermore implicitly parallel programs have an \emph{embarrassingly
  parallel} workload,
this occurs when much more parallel work is available than the parallel
processing capacity of the machine;
thereby dramatically reducing the benefit of parallel execution while
the cost remained the same.
Both these effects often caused very poor performance.

% Granularity control
Granularity control was introduced in order to solve these
problems~\citep{lopez96:distance_granularity,shen_98_granularity-control}.
It attempts to reduce the amount of work being executed in parallel.
There are a number of different methods, some incur a
runtime cost in order to determine if there is already ample parallel
work available while other static methods do not.
All methods help improve the performance of parallel programs and are
quite valuable, especially in recursive procedures.
\paul{I need to point out cases where GC does not help or is not good
enough,
I am going to have to find this in the literature and come back to this
paragraph and possibly the previous one.}

Some languages allow for the parallelisation of data parallel
tasks such as NESL~\citep{blelloch:95:nesl} and Data Parallel
Haskell (DpH)~\citep{dph:2007:status_report,dph:2008:harnessing_the_multicores}.
These languages use special data types to denote parallelism,
sequences and parallel arrays in NESL and DpH respectively.
Only operations on elements of these collections are parallelised.
We have none-the-less classified these systems as implicitly parallel,
since every action on these data types is executed in parallel.
Because operations are data-parallel they are independent and suitable
\paul{Define SMP in the introduction}
for execution on vector machines as well as SMP machines.
By transforming code and arranging for one thread to work on more than
one data item at a time granularity can be improved.
The drawback of these data-parallel approaches is that they can
parallelise data parallel programs
--- only a small subset of computer programs.

With the exception of DpH, implicit parallelism often performs worse
than explicit parallelism.
It is understood that carefully adding a few explicit parallelism annotations
to a program with the aid of a profiler will produce a faster-running
program than implicitly parallelising most independent computations.

\subsection{Automatic Parallelisation}
\label{sec:lit_automatic-parallelisation}

% Look at automatic parallelisation in Haskell.
\citet{harris_07_feedback_imp_par} developed a profiler
feedback directed automatic parallelisation approach for Haskell programs.
They have reported speed ups of up to 80\% compared to the sequential
execution of their test programs on a four core machine.
However they were not able to improve the performance of some
programs, they attributed this to a lack of parallelism
available in these programs.
They have shown that automatic parallelisation is a promising idea for
improving the performance of software.

We believe that more can be done to improve the effectiveness of
automatic parallelisation,
In some cases it may be possible to transform common programming
pasterns that lack parallelism into equivalent patterns with available
parallelism.
Furthermore, an advanced profiler --- such as Mercury's deep
profiler~\citep{conway:2001:mercury-deep} --- can provide information
that enables a compiler to make good parallelisation choices.
However there are a number of challenges facing automatic
parallelisation.
When using profiler feedback the profiled execution of the program may
not be a typical execution of the program, or there may be several
typical executions of the program.
We cannot control whether users profile typical executions of their
programs, but we may be able to allow users to merge execution
profiles to create a composite profile that is more representative of
their program's usage.
Another challenge can occur when a program has very little parallelism
available in it, it may be difficult to parallelise effectively.
We hope that in some cases the compiler can transform such a program
into an equivalent program with more available parallelism.

% Jerome's work.
\citet{tannier:2007:parallel_mercury} previously attempted to automatically
parallelise Mercury programs using profiler feedback
information to automatically parallelise a program.
\citet{tannier:2007:parallel_mercury} approach selects the most expensive predicates
of a program and attempts to parallelise conjunctions within them.
Tannier also makes use of compile-time granularity
control to reduce the over-parallelisation that can occur in recursive
code.
Unfortunately, he estimated the costs and benefits of parallelising
dependant conjunctions based on the number of dependant variables that
they shared.
In practice most producers produce dependant variables late in their
execution and most consumers consume them early.
Therefore Tannier's calculation is na\"ive: the time that these
variables are produced by one conjunct and consumed by the other may
not correlate with the number of dependant variables.
We believe that Tannier's algorithm is, in general, too optimistic
about the parallelism available in dependant conjunctions.

% My honours thesis.
\citet{bone:2008:hons} improved on this approach by using
information from a modification of Mercury's deep profiler to
calculate when the producing conjunct is most likely to produce the
dependant values and when the consuming conjunct is likely to need
them.
This information can be used to estimate the parallel speedup of
dependant conjunctions.
The effectiveness of this approach is not yet clear.

Mercury's deep profiler~\citep{conway:2001:mercury-deep} provides
detailed and accurate profiling information,
among other things the deep profiler records separate profiling
information for separate uses of the same code.
This will make it easier to implement optimisations such as
parallel specialisation --- generating sequential and parallel
versions of one procedure and using the sequential version
in situations where parallelism is not an optimisation.
Extracting information from the deep profiler to guide compiler
optimisations is supported by the feedback framework developed by
\citet{bone:2008:hons}.
These are examples of the flexibility that the deep profiler provides,
describing other ideas is outside the context of a literature review.
No equivalent profiler exists for Haskell or Clean, making Mercury an
important choice for our implementation.

There is another challenge with automatic parallelism: a lot of
information about the execution of a program will not be recorded by the
profiler, often recording information in infeasible.
In these cases we must be careful to make safe, conservative
assumptions when calculating estimates of this information.

% Write about haskell's call centre stacks, maybe they provide enough
% information to perform similar optimisations.

% How does clean compare?

We expect that automatic parallelisation will more easily and
effectively parallelise declarative programs.
Furthermore, it will be easier to maintain such programs, as
characteristics of the program that are used to explicitly parallelise
a program will not necessarily be true in future versions or uses of that
program.
Automatic parallelisation allows the programmer to re-parallelise
their program quickly, based on a current execution profile of the
program.

