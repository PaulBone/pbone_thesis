
%\paul{
%This should just 'set the stage' for my work.  I would
%like to leave the contrasting discussions until later in the
%thesis, perhaps at the end of each major chapter.
%}
%
%\paul{
%I do not want to introduce all my citations here (the way Liz has to
%due to the APA-formatting standard).  Only enough that the user
%understand the context for reading the research chapters.  So,
%closely related work should be introduced for the first time when
%it is compared with my work.
%}

\section{Literature review}
\label{sec:literature_review}

In this section we will explore the literature concerning parallel
programming.

The languages in which parallelism is used have a strong influence on the
semantics and safety of the parallelism.
We group languages into the following two classifications.

\begin{description}

    \item[Pure declarative] programming languages are those that forbid
    \emph{side effects}.
    A side effect is an action that a computation may perform
    without a declaration of the possibility of the action.
    The effect of executing any computation must be declared in the
    computation's signature.
    Because all effects are declared, none of them are \emph{side} effects.
    This has numerous benefits,
    but we will restrict our attention to the specific benefit
    that this makes it easy for both compilers and programmers to understand
    if it is safe to parallelise any particular computation.

    \item[Impure] programming languages are those that allow side effects.
    This includes imperative and impure declarative languages.
    Determining if parallelisation is safe is usually intractable and often
    requires whole program analysis.
    The analysis is almost always incomplete for programs whose size makes
    parallelism desirable.
    Thus parallelisation of programs written in impure languages is notoriously
    difficult.

\end{description}

There are many ways to introduce parallelism in computer programming.
We find it useful to create the following categories for these methods.
\begin{itemize}
    \item Parallelism via concurrency

    \item Language support for parallelism
%    is safer and easier to use compared to
%    \pvc, however it is less expressive.
%    We will explain how explicit parallelism is distinct from and improves
%    upon \pvc.

%    is a classification that includes
%    systems that attempt to make parallel execution the typical form of
%    execution.
%    We describe a number of systems and explain why the general approach is
%    flawed.
%    Some systems use do not fit cleanly into either of our explicit
%    or implicit parallelism definitions.
%    We will discuss both approaches together in Section~\ref{sec:intro_par}.

    \item Feedback directed automatic parallelism
%    is our last category.
%    In these systems sequential execution is the typical form of execution
%    and parallel execution is used sparingly.
%    Authors often group these systems with implicit parallelism systems.
%    We have chosen to separate them because the two systems
%    approach the same problem from different directions
%    (parallel as default versus sequential as default).
%    Prior work in automatic parallelism is discussed in
%    Section~\ref{sec:intro_auto_par}.

\end{itemize}
We explore the literature in this order as each successive category builds
on the work of the previous category.
Our own system is a feedback directed automatic parallelism system;
it makes use of concepts described in all four categories.
Therefore
we cannot discuss it meaningfully until we have explored the literature
of the giants upon whose shoulders we are standing.


\subsection{Parallelism via concurrency}
\label{sec:intro_concurrency}

% Parallelism and concurrency.
It is important to distinguish between parallelism and concurrency.
\begin{description}
    \item[Concurrency] is parallelism in the problem domain.

    As an example consider a web server running on a single core,
    which is required to handle multiple requests at once.
    Concurrency is used by the programmer to help him write the webserver in
    a more natural style.

    \item[Parallelism] is parallelism in the implementation domain.
   
    Consider a program that uses SIMD instructions to multiply matrices,
    performing several basic operations with a single instruction.
    Parallelism is used to reduce the elapsed execution time of the program. 

\end{description}

Concurrency may be implemented using parallelism,
because two independent tasks may be executed at the same time.
Languages which have been designed for concurrent programming,
and happen to be implemented using parallelism,
can be used by programmers to achieve parallelism.
Because the programmer uses concurrency to achieve parallelism,
we call this ``\pvc''.
Unfortunately,
this means that parallelism is both at the top and bottom levels of the
software stack, with concurrency in the middle.
The middle layer is superfluous, and should be avoided if possible.
In particular,
programmers should not have to shoe-horn their programs into a concurrent
model if that model is not a natural fit for the program.
This is particularly bad because it can make the program difficult to read,
understand and debug.
When a program can naturally be expressed using concurrency,
then it should probably be written to use concurrency anyway,
regardless of the benefits of parallelism.
In this section we will discuss systems that allow parallelism through
concurrency.
Our critique of these systems should not be misunderstood as a critique of
concurrency,
which is outside the scope of this dissertation.

Concurrency notations such as Hoare's
communicating sequential processes (CSP) \citep{hoare:1978:CSP}
or Milner's $\pi$-calculus \citep{milner:pi}
provide the programmer with a natural ways to express many programs,
such as webservers.
In these cases concurrency is (and should be) used to make the program
easier to write.
If concurrency uses parallelism provided by the hardware,
many programs will speed up due to this parallelism, but not all.

% Threading.
The most common method of introducing concurrency is with \emph{threads}.
Some languages, such as Java \citep{java-threads}, 
support threads natively; 
others, such as C \citep{c}, support it via libraries,
such as
POSIX Threads \citep{butenhof1997:pthreads} or Windows
Threads \citep{winthreads}.
These libraries tend to be low level;
they simply provide an interface to the operating system.
Different threads of execution run independently,
sharing a heap and static data.
Because they share these areas where data may be stored,
they can, and often do,
use the same data (unlike CSP).
Threads communicate by writing to and reading from the shared data.
Regions of code that do this are called \emph{critical sections},
and they must protect the shared data from concurrent access:
a thread reading shared data while another writes to it may read
inconsistent data;
and when two threads write to the same data,
some of the changes may be lost or corrupted.
Synchronisation such as \emph{mutual exclusion locks} (mutexes)
\citep{Dijkstra:Mutex} must be used to manage concurrent access.
Upon entering a critical section a thread must acquire one or more locks
associated with the critical section,
upon leaving it must release the locks.
The use of locks ensures that only one thread may be in the critical section
at a time.
(There are other ways that locks may be used to allow more than one thread
inside the critical section, such as the standard solution to the
readers-writers problem.)
The programmer is responsible for defining critical sections and using locks
correctly.

Mutual exclusion locking is problematic,
because it is too easy to make mistakes when using locks.
One frequent mistake is forgetting to protect access to a critical
section, or
making the critical section too small.
This can lead to incorrect program behaviour, inconsistent memory states and
crashes.
Making it too large can lead to deadlocks and livelocks.
When a critical section uses multiple shared resources,
it is normal to use multiple locks to protect access to a critical section.
The locks must be acquired in the same order in each such critical section,
otherwise deadlocks can occur.
This requirement for order prevents critical sections from being
nestable,
which causes significant problems for software composability.
Failing to acquire locks in the correct order or
failing to avoid nesting can also cause deadlocks.
All of these problems are difficult to debug because their symptoms are
intermittent.
Often so intermittent that introducing tracing code or compiling with debugging
support can prevent the problem from occurring;
these things are known as \emph{heisenbugs} and are very frustrating.
While it is possible to deal with these problems,
we believe that it is better to design problems out of systems;
doing so guarantees that the programmer takes on fewer risks.

% Message passing.
Another popular model of concurrency is message passing.
Unlike threads, message passing's processes do not share static or heap
data;
processes communicate by passing messages to one another.
The major benefit of message passing is that it does not make assumptions
about where processes are executing:
they can be running all on one machine or spread-out across a network.
Therefore, message passing is very popular for high performance computing,
where shared memory systems are not feasible.
Message passing also avoids the problems with threading and mutexes.
However message passing suffers from some of the same problems that
threading suffers from.
For example it is possible to create deadlocks with cyclic dependencies
between messages:
a process $a$ will send a message to $b$ but only after it has received a
message from $b$,
and $b$ will send a message to $a$ but only after it has received its message.
(We can create a similar situation with locks.)
Most deadlocks are more subtle and harder to find than this trivial example.

Notable examples of message passing application programming interfaces (APIs)
for C and Fortran 
%\citep{backus:1957:fortran}
are MPI \citep{mpi} and PVM \citep{pvm}.
Every process can send messages to and receive messages from any other
process.
We can think of this in terms of \emph{mailboxes}:
each process owns (and can read from) its own mailbox and can place messages
into any mailbox.
MPI and (to a lesser extent) PVM
support powerful addressing modes
such as multicasting the same message to multiple processes, or
distributing the contents of an array evenly between
multiple processes.
These features make MPI and PVM very powerful interfaces for high performance
computing.
They make the programmer is responsible for encoding and decoding messages.
While this adds extra flexibility,
it can also be very tedious.

Communicating sequential processes \citep{hoare:1978:CSP} is a notation for
concurrency using message passing.
Although we introduced PVM and MPI first, CSP predates them by 11 years.
Like MPI and PVM, CSP allows messages to be sent to and from processes
using mailboxes.
But unlike MPI and PVM, CSP allows processes to be created dynamically,
making it more flexible.
The first language based on CSP was occam \citep{occam1, occam3}.
The $\pi$-calculus \citep{milner:pi} was developed later.
It uses \emph{channels} rather than mailboxes for communication,
although each model is a dual of the other.
Unlike CSP, the $\pi$-calculus makes channels first class;
they may be sent over a channel to another process.
This allows programmers to compose a dynamic communication network.
%Lastly, the Join-calculus \citep{join-calculus}
%differs from $\pi$-calculus by using asynchronous
%communication and by adding a number of restrictions;
%neither of which is important to our discussion.

The Erlang language \citep{erlang} supports native message passing,
and unlike MPI and PVM, encoding and decoding is implicit.
%These two differences make it easier to program in Erlang than C with MPI or
%PVM.
Erlang is typically used where fault tolerance is important;
whereas MPI and PVM are used for high performance computing.
All three systems support distributed computing.

%%The Jomel language \citep{jomel} (an ML \citep{ml}) is based on the $\pi$-calculus
%Jocaml \citep{jocaml} (an Ocaml
%\citep{ml-types, ocaml-modules, ocaml-bytecode, ocaml-native}
%derivative) is a language based on join-calculus
%Jocaml differs from Occam in the same ways that the join-calculus differs
%from CSP.
%However Occam, in particular its first version \citep{occam1},
%was intended as a research tool to experiment with CSP, Jocaml was not.
%Later versions of occam \citep{occam3} were designed for more practical use.
%Jocaml is more widely used and appears to be much more practical.
%Jocaml, like Erlang, is a powerful language with support for distributed
%computing.

There are many other languages and libraries based on these process algebras
such as 
Jocaml \citep{jocaml} (an Ocaml
\citep{ml-types, ocaml-modules, ocaml-bytecode, ocaml-native}
derivative)
and Go \citep{google:2012:go}.
Go is similar in style to the $\pi$-calculus.
%and the join-calculus.
However it also uses shared memory without providing any synchronisation
for communicating through shared memory.
In particular data sent in messages may contain pointers,
and therefore different processes may end up referring to the same memory
locations.
If one process modifies such a memory location,
other processes can see the modification.
This creates the possibility that modifications to this data can 
cause a process to read inconsistent data or 
cause updates get lost or corrupted.
This makes several classes of bug possible through accidental use of
shared memory.
This is a result of the language specification's inclusion of pointers and
the implementation of channels.

% STM
All the systems we have introduced so far are prone to deadlocks,
and many are prone to other problems.
Many problems including deadlocks can be avoided using software
transactional memory (STM) \citep{stm}.
STM is similar to the threading model,
in that it includes the concept of critical sections.
STM does not protect critical sections using mutexes,
instead it uses optimistic locking.
Variables that are shared between threads are known as STM variables.
They may only be modified within a critical section and a good implementation
will enforce this.
STM works by logging all the reads and writes to STM variables within a
critical section,
and not actually performing the writes until the end of the critical
section.
At that point the STM system acquires a mutex for each STM variable
(in a pre-defined order to avoid deadlocks)
and checks for sequential consistency by comparing its log of the observed
values of the variables with the current
values of the variables.
If no inconsistencies are found (the variables' values are all the same),
it then performs any delayed writes and unlocks the variables.
Otherwise it must retry the critical section or fail.
This is much safer than the threading model because
the user does not need to use mutexes,
and critical sections are now nestable.
Code containing nested sections will perform the commit only when the 
outermost section is ready to commit.
However, programmers must still place their critical sections correctly in
their program.
A common criticism of STM is that it performs poorly when there is a lot of
contention as a failed critical section must be re-executed,
and this often adds to the amount of contention.
Including side-effecting computations inside an STM critical section can also
be a problem.
This can be avoided in a pure declarative language such as
Haskell \citep{haskell98} or Mercury \citep{mercury_jlp}.
\citet*{harris:2005:haskell-stm} describes an implementation for Haskell,
and \citet*{mika:mercury-stm} describes an implementation for Mercury.

\subsection{Language support for parallelism}
\label{sec:intro_par}

% Parallelism w/o concurrency in imperative languages.
Parallelism can also be achieved without concurrency.
This can be achieved with language annotations that do not affect the
(declarative) semantics of the program.
The annotations usually describe \emph{what} should be parallelised,
and rarely describe \emph{how} to parallelise each computation.
We will refer to this as \emph{explicit parallelism}
since parallelism is expressed directly by the programmer by the explicit
annotations.
In this section we also discuss \emph{implicit parallelism},
which covers cases where programming languages make parallel execution the
default method of execution.

OpenMP \citep{openmp} is a library for C++
%\citep{cplusplus}
and Fortran
that allows programmers to annotate parts of their programs for parallel
execution.
In OpenMP most parallelism is achieved by annotating loops.
C++ is an imperative language and therefore the compiler generally cannot
determine which computations are safe to parallelise.
The programmer is responsible for guaranteeing that the iterations
of the loops are independent,
in other words, that no iteration depends on the results of any previous
iterations.
If an iteration makes a function call, then this guarantee must also be true for
the callee and all transitive callees.
The programmer must also declare the roles of variables,
such as if they are shared between iterations or local to an iteration.
This is necessary so that the compiled code is parallelised correctly.
The programmer may specify how the iterations of the loop may be
combined into parallel tasks (known as \emph{chunking}).
This means that to some extent the programmer still describes \emph{how} to
parallelise the loop,
although the programmer has much less control than with \pvc systems.
Provided that the programmer can annotate their loops correctly,
explicit parallelism with OpenMP avoids the problems
with threading and message passing.

% Declarative languages.
In pure declarative languages
a function has no side effects:
all possible effects of calling a function are specified in its 
declaration.
This is often done by expressing effects as part of the type system;
Haskell \citep{haskell98} uses Monads \citep{haskell-monads},
Mercury\footnote{
    Mercury's uniqueness typing is part of its mode system.
    Despite this, it is still considered uniqueness typing.}
\citep{mercury_jlp}
and Clean \citep{brus:1987:clean}
use
uniqueness-typing, and 
Disciple \citep{ddc} which uses effect-typing.
\paul{Last time Zoltan read this I said "DDC" (the implementation) rather
than "Disciple" (the language).}
In any of these systems both the programmer and compiler can trivially
determine if executing a particular computation in parallel is safe.
In Haskell, Mercury and Clean (but not Disciple)
the concept of a variable is different from that in imperative programming,
it is not a storage location that can be updated,
but a binding of a name to a value.
Therefore variables that are shared
(those that we would have to declare as shared in OpenMP)
are really shared immutable values.
%\footnote{
%    We find it amusing that the misnomer ``variable'' is used at all.}.
Explicit parallelism in declarative languages is much easier and safer to use
than in imperative languages.
The next two sub-sections describe parallelism in functional and logic
languages respectively.

%Pure declarative languages like Mercury~\citep{mercury_jlp},
%Haskell~\citep{haskell98} and Clean~\citep{1991:concurrent-clean} do not
%allow side-effects.
%\paul{I should be able to find a more useful citation for Clean.}
%This is done by ensuring that a function's declaration declares all of the
%functions effects.
%Therefore,
%the programmer or the compiler can read a function's declaration to see if it
%is safe to parallelise.
%Mercury and Haskell both support explicit parallelism,
%the programmer need only annotate their code, indicating that it should be
%executed in parallel.

\subsubsection{Parallelism in functional languages}
\label{sec:intro_par_func}

% \paul{Concurrent Lisp}
% Concurrent lisp clearly uses concurrency as in the previous section.

Pure functional languages like Haskell and Clean can be evaluated using a
process known as \emph{graph reduction}.
Graph reduction works by starting with an expression and a program and
applying reductions to the expression using the program.
Each reduction uses an equation in the program to replace part of the
expression with an equivalent expression.
In a lazy language (or lazy implementation of a non-strict language)
each reduction step performs \emph{outermost reduction}.
This means that the arguments of a function are not normally evaluated
before a reduction replaces the function call with the right hand side of
equation for that function.
Some reductions require the evaluation of an argument,
these include primitive operations and pattern matching.
The system continues applying reductions until outermost symbol is a value.
The resulting value may have parameters which are still expressions;
they have not been evaluated.
This is called weak head normal form (WHNF).
Graph reduction itself can be parallelised.
If a subgraph (an argument in the expression) is known to be required in the
future,
then it can be evaluated in parallel using the same graph reduction process.
Several projects have experimented with this technique
\citep{augustsson:1989:parallel-graph-reduction,burn:1989:parallel-reduction-machine,peyton-jones:1989:parallel-graph-reduction}.

Because the programmer does not need to annotate their program this
parallelism is implicit.
While implicit parallelism is always safe\footnote{
    There are some systems that use explicit parallelism and allow
    side-effects such as
    Prolog implementations with extra-logical predicates and
    Multilisp with destructive update. 
    Nevertheless the \emph{intention} is that implicit parallelism should
    always be safe}
there is a serious performance issue.
The cost of spawning off a parallel evaluation is significant,
and so is the cost of communication between parallel tasks.
There are also distributed costs throughout the execution of these systems:
for example many parallel graph reduction systems introduce locking on every
graph node.
Such locking increases the overheads of evaluation even when no parallel
tasks are created.
Therefore parallelised computations should be large enough that the benefit
of evaluating them in parallel outweighs the costs of doing so.
Unfortunately most parallelism in an implicit parallel system is very
fine grained, meaning that it is not large enough to make parallel
evaluation worthwhile.
There is also a lot of this parallelism.
This means that the system spends most of its time managing the parallel
tasks rather than evaluating the program.
This is called embarrassing parallelism as the amount of fine-grained
parallelism can cause the evaluation to perform \emph{more slowly} than
sequential evaluation.
Most other implicitly parallel systems share these problems.
Many systems such as \citet{peyton-jones:1989:parallel-graph-reduction} go
to lengths to exploit only coarse grained parallelism.
In our opinion the problem with implicit parallelism is that it approaches
the parallel programming challenge from the wrong direction:
it makes everything execute in parallel \emph{by default} and then attempts
to introduce sequential execution;
rather than make sequential execution the default and then
attempt to introduce parallel execution \emph{only where it is profitable}.
The former creates thousands if not millions of parallel tasks,
whereas the latter creates far fewer, dozens or hundreds.
Since most systems have only a handful of processors the latter situation is
far more manageable having far fewer overheads;
therefore we believe that it is the correct approach.
In contrast to implicit parallelism,
explicit parallelism allows the programmer to introduce
parallelism only where they believe it would be profitable.
This means that the default execution strategy is sequential,
and far fewer parallel tasks are created.

\paul{I checked, Multilisp is capitalised as such.}
Multilisp \citep*{halstead:1984:multilisp,halstead:1985:multilisp} is an
explicitly parallel implementation of Scheme.
Like Scheme, Multilisp is not pure: it allows side effects.
This means that as programmers introduce parallelism, they must be sure that
parallel computations cannot be affected by side effects,
and do not have side effects themselves.
Therefore Multilisp is not safe as side effects may cause inconsistent
results or corrupt memory.
Multilisp programmers create parallelism by creating delayed computations
called \emph{futures}.
These computations will be evaluated concurrently
(and potentially in parallel).
When the value of a future is required,
the thread blocks until the future's value is available.
Operations such as assignment and parameter parsing do not require the
value of a future.

Haskell is a pure functional language.
Being pure it does not have the problem with side effects that Multilisp
does.
The Haskell language standard specifies that it is non-strict\footnote{
    Non-strict evaluation is more broadly defined than lazy evaluation};
in practice, all implementations use lazy evaluation most of the time.
A subexpression may be evaluated eagerly if it is always required in a
particular context.
Haskell programmers can introduce explicit parallelism into their programs
with two functions (\code{par} and \code{pseq}).
\citet{gph:gum}
%\citet*{loidi:2008:gph-semiexplicit-parallelism}
first added these functions to GUM (A parallel Haskell implementation).
Then \citet{harris:2005:haskell-smp} added them to the
Glasgow Haskell Compiler (GHC).
%arguably the most popular implementation of Haskell.
The \code{par} and \code{pseq} functions have the types:

\begin{verbatim}
par :: a -> b -> b
pseq :: a -> b -> b
\end{verbatim}
They both take two arguments and return their first.
Their declarative semantics are identical;
however their \emph{operational} semantics are different.
The \code{par} function may spawn off a parallel task that evaluates its
second argument to WHNF,
and returns its first argument.
The \code{pseq} function will evaluate its second argument to WHNF
\emph{and then} return its first argument.
We can think of these functions as the \code{const} function
with different evaluation strategies.

Unfortunately lazy evaluation interacts poorly with
parallel evaluation.
Because parallel tasks are only evaluated to WHNF,
most tasks do not create enough work to make parallelisation worthwhile.
\citet{trinder:98:strategies}
tried to reduce this problem by allowing programmers to
specify an evaluation strategy,
which describes how deeply to evaluate expressions.
This description is written in Haskell itself,
This support is implemented as a library and it can describe higher order
evaluation strategies.
This does not solve the problem,
it only gives the programmer a way to deal with it when they encounter it.
It would be better to make it impossible for the problem to occur.
%Non-strict evaluation can cause other performance problems;
%computations spend a lot of their time creating, testing and replacing
%thunks that can be avoided.
%including in sequential programs.
%Evaluation strategies, in particular eager versus non-eager,
%are often a matter of personal preference.

An alternative Haskell library called Monad.Par \citep{marlow:monadpar}
requires parallel computations to be evaluated in full.
However the programmer must provide typeclass instances for each of their
own types that describe how to completely evaluate that type.
While this still requires a non-trivial amount of effort from the
programmer;
it makes it harder for the programmer to make mistakes,
since their program will not compile if they forget to provide a typeclass
instance.

Regardless of how we manage lazyness in parallelism we expose,
it still introduces a runtime cost.
Every variable must be tested to determine if it is a thunk or a value,
and while this cost occurs in sequential programs, it can be higher in
parallel programs.
Furthermore, this means that it is not always clear
(even when one does use \code{pseq}, strategies or Monad.Par)
which threads will perform which computations.

\subsubsection{Parallelism in logic languages}
\label{sec:intro_par_logic}

Different logic programming languages use different evaluation strategies,
but we will restrict our attention to those that use
selective linear resolution with definite
clauses (SLD resolution) \citep{kowalski_sld}.
SLD resolution attempts to answer a query by finding a Horn clause whose
head has the same predicate name as the selected atom in the query,
performing variable substitution and then performing a query on each
conjunct in the clause's body.
If a query of a conjunct fails then the clause fails and an alternative
clause may be tried.
Multiple clauses may succeed meaning that there are several solutions,
and if no clauses succeed it means that there are zero solutions.

Prolog is the best known logic programming language based on SLD
resolution.
OR-parallel Prolog implementations may attempt to resolve a query against
multiple clauses in parallel.
In most logic programs that we have seen have very little OR-parallelism.
In cases where there is abundant OR-parallelism,
it is usually because the program uses a significant amount of
non-deterministic search.
Large search problems can be implemented more efficiently in constraint
programming languages, even sequential ones.
%Like parallel graph reduction,
%this form of implicit parallelism can create
%embarrassingly parallel workloads.
%OR-parallelism has some additional problems.
%Not all programs will provide enough OR-parallelism as many do not perform
%non-deterministic searches.
Additionally OR-parallelism is speculative:
the current goal may be executed in a context where only the first result
will be used,
therefore searching for more results in parallel may be a waste of
resources.
Systems that attempt to exploit OR-parallelism include
Aurora \citep*{lusk:1990:aurora},
Muse \citep*{ali:1990:muse}
and ACE \citep*{gupta:1991:ace}.
(ACE also supports AND-parallelism.)

AND-parallel Prolog implementations introduce parallelism by executing
conjuncts within clauses in parallel.
AND-parallelism comes in two varieties, independent and dependent.
Independent AND-parallelism is much simpler, it only parallelises goals
whose executions are independent.
%Independent AND-parallelism comes in two flavours, strict and non-strict.
%Strict independent AND-parallelism is where
%conjoined goals that may be evaluated in parallel must have no free variables in
%common.
%This includes free variables made common by aliasing.
\paul{It seems that everybody defines strict/non-strict differently!
Now I am confused about the true definitions, if there are any.
Therefore I have refactored this to just talk about some analyses;
hopefully it is also more to-the-point.}
There are various ways to prove the independence of goals with varying
degrees of completeness.
If we cannot prove that a group of goals is independent,
then we must assume the goals are dependent.
The simplest and weakest way to prove independence is to show that the
goals have no variables in common,
including aliased variables (free variables unified with one another).
This can be done statically.
However very few groups of goals are trivially independent.
We can improve this by allowing the sharing of ground variables.
Systems allowing this initially used runtime tests to check the groundness
and independence of variables.
These tests may traverse arbitrarily large data structures.
The overheads of executing these checks
(which must be executed regardless of their outcome)
can outweigh the benefit of parallel execution.
Some systems used compile time analysis to reduce the costs of the runtime
checks.
Despite this the remaining runtime tests' costs were still unbounded.
More advanced systems such as 
\paul{I checked, all three papers refer to tricky analyses.}
\citet{hermenegildo:1991:and-parallel},
\citet{DBLP:journals/tcs/GrasH09} and
\citet{Hermenegildo1995}
perform more complicated compile time analyses.
Some of these analyses require whole program analysis for completeness,
making them impractical for large programs.
Furthermore, the Prolog language allows some operations that can prevent an
analysis from being complete even if whole program information is available.

%%Non-strict independent AND-parallelism (also called restricted AND-parallelism)
%%is where some variables
%%are shared but either these variables are ground or they will be bound to
%%the same value in the goals that reference them.
%\paul{XXX Find out of these papers analyse the conjuncts to find
%unifications or if they just test groundness}
%Examples non-strict independent AND-parallel Prolog systems include \&-Prolog
%%\citep*{Hermenegildo:1991:and-parallel}
%%DBLP:journals/jlp/MuthukumarBBH99}
%%and \&ACE \citep{gupta:1991:ace}.
%First, non-strict independent parallel systems will try to prove that shared
%variables are ground,
%if they are then the goals are independent.
%This test is complete when performed at runtime, however for large terms it
%can take a sagnificant amount of time.
%Some systems combine this with compile time analysis,
%they usually generate somewhat smaller and therefore cheaper runtime tests.
%however they do not put a fixed limit on the cost of the runtime checks.
%Some systems such as
%\citet{Hermenegildo1995} describe more complicated analyses that look at the
%unifications within parallel conjuncts to determine what types of
%unifications can occur and if they can cause the same variable to be unified
%with different function symbols (causing a failure).
%As non-strict parallelism is more general it is desirable for a system to
%handle it.
%Determining which goals are independent is much more complicated and
%requires analysis;
%\citet{DBLP:journals/tcs/GrasH09} and
%suitable analyses.
%Some of the analysis can be done at compile time,
%however it is a whole program analysis which means that separate compilation
%cannot be used.

\plan{Managing logic variables and retracting their values}
Prolog supports \emph{logic variables}.
When two or more free variables are unified with one another they behave as one
variable;
any later successful unification on any of the variables implicitly modifies
the state of the other variables.
During sequential execution
part of the search space may bind variables to values.
If this branch of the search fails,
execution must backtrack and retract any bindings that were made on the
failed branch.
This is required so that alternative
branches of the search tree are free to bind the variables to other values.
With OR-parallelism a variable may be bound to distinct values in different
parts of the search tree at once.
There are many different ways to implement this,
for example Muse stores variables for different parallel parts of the search
tree in different address spaces.
While this is simple to implement it performs poorly:
extra work is required to copy the states of computations as new
computations are executed in parallel.
It is difficult to implement systems that perform well.

Also,
in independent AND-parallel systems
variable bindings are easy to manage correctly,
but difficult to manage efficiently.
%A variable can have at most a single value at a time,
%and therefore this complication has been removed.
%Also, due to the independence of the conjunction multiple bindings for the
%same variable never conflict (as they cannot occur).
Systems that combine independent AND-parallelism and OR-parallelism,
such as 
PEPSys \citep*{baron:1988:pepsys}
and
ACE \citep*{gupta:1991:ace},
combine the costs of managing variables in either parallelism type.

Unfortunately,
just as most programs do not contain much OR-parallelism,
very few programs contain much independent AND-parallelism.
For example, in the Mercury compiler itself,
there are 69 conjunctions containing two or more expensive goals,
but in only three of those conjunctions are the expensive goals independent
(Section~\ref{sec:overlap_aims}),
Supporting dependent AND-parallelism in Prolog
is much more difficult \emph{because} of the management of variable
bindings.
Any conjunct in a dependent AND-parallel conjunction may produce bindings
for any given variable.
If different conjuncts produce different values for the same variable,
then one of them must fail.
Handling this correctly and efficiently is extremely difficult.
In a system such as Ciao \citep{hermenegildo_ciao},
which attempts to infer modes for predicates (but allows them to be
specified),
the producer of a variable may be known,
greatly simplifying the management of that variable's bindings.
The next problem is how variable communication can affect other conjuncts
running in parallel.
Most systems allow parallel conjuncts to communicate variable bindings as
soon as they are produced.
This allows conjoined code running in parallel,
in particular non-deterministic search computations,
that consume those bindings to detect failure earlier and therefore execute
more efficiently.
However if the producer of a binding is non-deterministic,
and it produces the binding within the non-deterministic code,
then a failure may require it to retract variable bindings that are shared
in the parallel conjunction.
Implementing this efficiently is difficult.
Kernel Andorra Prolog \citep*{haridi:1990:andorra}
solves this problem by performing either AND-parallel execution or
OR-parallel execution.
It attempts to execute deterministic\footnote{
    Andorra uses a different definition for determinism than Mercury does.
    Section~\ref{sec:backgnd_mercury} explains Mercury's definition.}
conjoined goals in parallel.
For Andorra's purposes, deterministic means ``succeeds at most once''.
If all the goals in the resolvent have more than one matching clause,
then it uses OR-parallelism to execute the matching clauses in parallel.
This means that non-determinism cannot occur in parallel conjunctions.

Another solution to this problem involve restricting the Prolog language
by removing backtracking.
This creates committed-choice logic languages such as
Concurrent Prolog \citep*{saraswat86:concurrent_prolog_definition,
taylor:flat_concur_prolog};
Parlog \citep*{clark:84:parlog_sys_prog,clark:86:parlog}
and Guarded Horn Clauses (GHC) \citep*{ueda:ghc}.
When these systems encounter multiple clauses during execution they commit
to only one of the clauses.
Each of these system uses guarded clauses, which have the
form:
$H \leftarrow G_1 \wedge \ldots \wedge G_n : B_1 \wedge \ldots \wedge B_m$
where $G_i$ and $B_i$ are atoms in the guard and body of the
clause;
$H$ is the head of the clause.
The evaluation of the body is committed to if and only if 
all unifications in the head and the guard are true.
Declaratively the guard and body are conjoined.
All three systems have two types of unifications:
\begin{description}
    \item[\emph{ask} unifications] occur only in one direction:
    one side of the unification is a term provided by the
    environment (the resolvent),
    the other side is a term in the clause head or guard.
    An ask unification may bind variables in the second term but not in the
    first.

    \item[\emph{tell} unifications] allow binding in either direction;
    they can instantiate the resolvent's term.
\end{description}
Each system implements these differently:
Concurrent Prolog uses a read-only annotation to identify ask unifications,
and GHC uses scoping rules
(tell unifications to non-local variables cannot occur in the head or
guard).
When an ask unification is used and 
a ordinary unification in its place would normally bind a variable,
then the computation is suspended until the variable becomes instantiated
allowing the ask unification to run without binding the variable.
This mechanism ensures that enough information is available before the
program commits to a particular clause.
When a tell unification is used,
the binding is made locally and is re-unified as the clause commits.
This re-unification can add to the costs of execution,
especially if failure is detected as speculative work may have been
performed in the meantime.
Note that in GHC,
there cannot be any tell unifications in the guard and so re-unification is
not required.
However GHC suspends computations more often,
which also adds to the costs of parallel execution;
especially when most unifications can cause a task to suspend or resume.
% Originally this included unifications in clause bodies,
% as a the languages allowed predicate calls to be made from guards.
Newer \emph{flat} versions of these languages
do not allow predicate calls to be used in a guard
\citep*{taylor:flat_concur_prolog,foster:1987:flat_parlog,modedghc},
this means that unifications in clause bodies can only be tell unifications.
Whilst this reduces the expressiveness of the languages,
it allows them to use
slightly simpler unification code in more places.
However clause bodies can still include tell unifications can may provide a
variable instantiation that allows some other blocked computation to resume.
Therefore most unifications still incur these extra costs.

The Data Dependent AND-parallelism system (DDAS)
of \citet{pontelli:1996:ddas} supports explicit
dependent AND-parallelism in a non-deterministic language;
it is based on the work of \citet{gupta:1991:ace}.
Analyses determine a conservative set of shared variables in dependent
conjunctions and then \emph{guess} which conjunct produces each variable
(usually the leftmost one in which the variable appears)
and which conjuncts consume it.
The consumers of a variable are suspended if they attempt to bind it and the
producer has not yet bound it.
The compiler names apart all the potentially shared variables so
that bindings made by the producer can be distinguished
from bindings made by consumers.
As these analyses are incomplete (in particular mode analysis),
the predicted producer may not be the actual producer as execution may
follow a different path of execution than the one that was predicted.
If this happens then the runtime system will dynamically change the guess
about which conjunct is the producer.
This system still requires plenty of runtime overhead and since it is
non-deterministic it must handle backtracking over code that produces the
values of shared variables.
The backtracking mechanism used is described by \citet{shen:1996:daswam}.

Most of the systems we have discussed use implicit parallelism.
These are: Aurora, Muse, ACE, Andorra, Concurrent Prolog, Parlog, GHC and
DDAS;
while PEPSys uses explicit parallelism and Ciao can use either.
The implicitly parallel systems parallelise almost everything.
As we mentioned earlier this can lead to embarrassing parallelism.
Therefore, it is better to create fewer coarse-grained parallel tasks rather
than many fine-grained tasks.
%To avoid this many of the above systems use some type of schedule analysis
%to decide what is worth executing in parallel,
%and then attempt to only parallelise those things.
To achieve this, some systems use \emph{granularity control}
\citep{debray:1990:granularity}
to parallelise only tasks large enough such that their computational cost
makes up for the cost of spawning off the task.
While it is easy to either spawn off or not spawn off a task,
it is much more difficult to determine a task's cost.
There are multiple different ways to estimate the cost of a task.
For example \citet{king:lower_bound_time_complexity} and
\citet{lopez96:granularity} construct cost functions
at compile time which can be evaluated at runtime to estimate
whether the cost of a goal is above a threshold for parallel execution given
the sizes of certain terms.
During compile time the \citeauthor{lopez96:granularity} paper 
uses the type, mode and determinism information that
the Ciao \citep{hermenegildo_ciao} compiler makes available.
The \citeauthor{king:lower_bound_time_complexity} paper uses a convex hull
approximation to create its cost functions.
This approximation is very useful in this domain,
it makes the analysis of recursive code feasible in modest memory sizes,
and not too much information is lost during the approximation.
%The runtime may also be measured in a previous execution of the program and
%this may provide an estimate for the runtime in the future
%\citep{sarkar:1989:feedback,harris,tannier};
%our own system, including the system we implemented before commencing the
%Ph.D.\ programme, \citet*{bone:2001:honours}
\citet{shen_98_granularity-control} created a different method for
granularity control,
which measures the ``distance'' between the creation of successive parallel
tasks.
They propose two methods for increasing this distance which can create more
coarse grained parallelism and usually spawns off less parallel work
(reducing the cost of parallel execution in the program).
Their first method works at compile time by transforming a recursive
predicate so that it spawns parallel tasks less frequently.
Their second method works at runtime to achieve the same means,
it uses a counter and will not spawn off parallel work until the counter
reaches a predetermined value.
Once it reaches such a value, then the program will spawn off some parallel
work and reset the counter.
The counter can be incremented based on some indication of how much work is
being done between tasks, such as abstract machine instructions executed.
These methods can help to reduce the amount of embarrassing parallelism,
but the compiler-based method can be difficult to implement correctly for
all programs,
and the runtime-based method introduces extra runtime costs.
Finding profitable parallelism is still a difficult problem.
% This approaches the problem from the wrong direction.
% We believe that it is best to use sequential execution by default and
% introduce parallelism only where it is likely to be beneficial.

Mercury is a pure logic/functional language, with strong and static type,
mode and determinism systems.
We will discuss Mercury in more detail throughout
Chapter~\ref{chap:backgnd}.
\citet{conway:2002:par} introduced explicit independent AND-parallelism
to Mercury in deterministic code,
code with exactly one solution
(see also Section~\ref{sec:backgnd_merpar}). 
Later \citet{wang:2006:hons}; \citet{wang:2011:dep-par} 
implemented support for dependent AND-parallelism.
Mercury's mode system allows the compiler to determine the exact locations
of variable bindings.
This allowed \citeauthor{wang:2006:hons} to implement a transformation that
efficiently handles dependent AND-parallelism
(see also Section~\ref{sec:backgnd_deppar}). 
Because Mercury only supports parallelism in deterministic code,
variable bindings made by dependent conjuncts can never be retracted.
Some of our critics (reviews of the paper \citet{bone:2011:overlap}
on which Chapter~\ref{chap:overlap} is based)
are concerned that this reduces the amount of parallelism that Mercury is
able to exploit \emph{too much}.
However,
roughly 75\% of Mercury code is deterministic,
and most of the rest is semi-deterministic
(it produces either zero or one solutions).
We gathered this determinism information from the
largest open source Mercury program, the Mercury compiler;
our experience with other Mercury programs shows a similar pattern.
It may appear that we are missing out on parallelising 25\% of the
program,
but most of the parallelism in this code is speculative and therefore
more difficult to parallelise effectively.
Furthermore,
the 75\% that we can parallelise accounts for 97\% of the runtime
(Table~\ref{tab:recursion_types})
when tested on a single program (the compiler).
Parallelising the remaining 3\% would require
supporting parallelism in at least semi-deterministic code which has higher
runtime costs.
It is also unlikely that this part of the program contains much profitable
parallelism as most semi-deterministic code is in the goals of if-then-else
goals.
The Prolog systems that support parallelism in goals that can fail and
goals with more than one solution must manage variable bindings
in very complicated ways;
this adds a lot of runtime cost.
In contrast, Mercury's simpler implementation allows it to handle a
smaller number of predicates, \emph{much} more efficiently.

In Prolog, the management of variable bindings in dependent AND-parallel
programs adds extra overhead to unifications of all variables
as most Prologs cannot determine which variables are shared in parallel
conjunctions,
and none can determine this accurately.
Mercury's implementation,
which uses futures (Section~\ref{sec:backgnd_deppar}),
increases the costs of unification only for shared variables.
A related comparison, although it is not specific to parallelism,
is that unification is an order of magnitude simpler in Mercury than in
Prolog as Mercury does not support logic variables.
(The various WAM unification algorithms have roughly 15--20 steps,
whereas the most complicated Mercury unification operation has 3 steps.)
This was also a deliberate design decision.

\subsubsection{Other language paradigms}
\label{sec:intro_par_other}

Some languages allow parallelism in other forms,
or are created specifically to make parallelism easy to expose.
Parallel languages that do not fall into one of the above paradigms are
assessed here.

Sisal \citep{feo:1990:sisal-report} was a data-flow language.
Its syntax looked like that of a imperative language,
but this was only syntactic sugar for a pure functional language.
It was also one of the first single assignment languages;
using single assignment to implement its pure functional semantics.
Sisal supported implicit data-flow parallelism.
Sisal's builtin types included arrays and streams.
Streams have the same form as cons-lists (Lisp style) except that as one
computation produces results on the stream, another computation may be
reading items from the stream.
The concept of streams is not new here,
it has been seen in Scheme \citep{wizard-book} and probably other
languages.
A stream may have only one producer,
which makes them different from channels.
The other source of parallelism was the parallel execution of independent
parts of loop iterations;
as one iteration is being completed,
another iteration can be started.
It could also handle completely independent parallel loops.
Sisal's for loops included a reduction clause,
which stated how the different iterations' results were summarised into a
single overall result,
which may be a scalar, an array or a stream.
A loop's body must not have any effects or make any variable bindings
that are visible outside the loop,
so a loop's only result is the one described by the result clause.
The Sisal project is long dead,
which is unfortunate as it appeared to be good for expressing data-flow
parallel programs.
Sisal performed very well,
giving Fortran a run for its money:
in some tests Sisal outperformed Fortran and in others Fortran outperformed
Sisal \citep{feo:1990:sisal-report}.

\paul{UPC is not data-parallel, so I did not include it.}
Data-parallel languages such as
Data Parallel Haskell (DpH)
\citep{dph:2007:status_report,dph:2008:harnessing_the_multicores},
NESL
%\footnote{
%    \citet{blelloch:95:nesl} does not explain what the name ``NESL'' may
%    stand for.
%    Since the language supports nested parallelism based on sequences we
%    guess the name might be the acronym of ``NEsted Sequence Language''.}
\citep{blelloch:95:nesl} and
C* \citep{rose:cstar}
parallelise operations on members of a sequence-style data structure.
DpH calls them parallel arrays,
NESL calls them sequences, and
C* calls them domains.
DpH and NESL can handle some forms of nested parallelism.
Any operations on these structures are executed in parallel.
NESL and C* have been targeted towards SIMD hardware
and DpH has been targeted towards multicore hardware.
Data-parallelism's strength is that its computations are \emph{regular},
having a similar, if not identical, shape and operations.
This makes targeting SIMD hardware,
and controlling granularity on SMP hardware easier in these systems than
most others.

Data parallel and data-flow parallel languages do not have the same
problems (such as embarrassing parallelism)
as implicit parallel languages.
This is because programs with data-parallelism
(and to a lesser extent data-flow parallelism)
have a more regular structure,
which allows these systems to generate much more efficient parallel code.
However, very few programs have either data-parallelism or data-flow
parallelism and cannot be parallelised using these systems.
Therefore, these techniques cannot be used to parallelise most programs.

Reform Prolog \citep{bevemyr:reform} 
handles data-parallelism in a logic language.
It finds singly recursive Prolog predicates,
separating their recursive code into the code before the recursive call and
the code after.
It compiles them so that they execute in parallel the code before the
recursive call on all levels of the recursion,
then they execute the base case,
followed by parallel execution of the code after the recursive call on all
levels of the recursion.
Reform Prolog only does this for deterministic code
(which we do not think is a problem, as we do the same in Mercury).
However Reform Prolog has other limitations:
the depth of the recursion must be known before the loop is executed,
parallel execution cannot be nested,
and no other forms of parallelism are exploited.
This means that like the languages above,
Reform Prolog cannot effectively parallelise most programs.

\subsection{Feedback directed automatic parallelisation}
\label{sec:intro_auto_par}

As we mentioned above,
we believe that it is better to start with a sequential program and
introduce parallelism only where it is beneficial,
rather than parallelising almost all computations.
Programmers using an explicit parallelism system could follow this
principle.
However 
for each computation the programmer considers parallelising,
they must know and keep track of a lot of
information in order to determine if parallel execution is worthwhile.
In the context of profiling and optimisation,
it is widely reported that programmers are poor at identifying the
hotspots in their programs or
estimating the costs of their program's computations in general.
Programmers who are parallelising programs must also understand the
overheads of parallelism such as
spawning parallel tasks and
cleaning up completed parallel tasks.
Other things that can also affect parallel execution performance
including
operating system scheduling and
hardware behaviour such as cache effects.
Even if a programmer determines that parallel execution is worthwhile in
a particular case,
they must also be sure that adding parallel execution to their program
does not create embarrassing parallelism.
They must therefore understand whether there will be enough
processors free at runtime to execute each spawned off computation.
When parallel computations are dependent,
such as when futures are used in Multilisp or
dependent parallelism is used in Mercury,
programmers must also estimate how the dependencies effect parallelism.
In practice programmers will attempt to use trial and error to find the
best places to introduce parallelism,
but an incorrect estimation of the magnitude of any of the above effects can
prevent them from parallelising the program effectively.

We therefore advocate the use of feedback directed parallelism.
This technique gathers cost information from a previous typical execution of
the program,
and uses that information to estimate the costs of the program's
computations in future executions of the program,
parallelising it accordingly.
This is important because the cost of a computation often depends on the
size of its input data.
Feedback directed parallelism avoids the runtime cost of such a calculation
(as the size of the input data is not available except at runtime or through
profiling).
It also avoids the human-centric issues with explicit parallelism.

\citet*{harris_07_feedback_imp_par} developed a profiler
feedback directed automatic parallelisation approach for Haskell programs.
They reported speed ups of up to a factor of 1.8 compared to the sequential
execution of their test programs on a four core machine.
However they were not able to improve the performance of some
programs,
which they attributed to a lack of parallelism available in these programs.
Their partial success shows that automatic parallelisation is a promising
idea and requires further research.
Their system works by measuring the execution time of thunks and using
this information to inform the compiler how to parallelise the program.
Any thunk whose execution time is above a particular threshold is
spawned off for parallel evaluation.
This has two problems, both due to Haskell's lazy evaluation.
The first is that their system does not also introduce calls to \code{pseq},
which would force the evaluation of thunks in order to improve granularity.
Therefore \citeauthor{harris_07_feedback_imp_par}'s method
has the same problems with lazyness that the introduction of
\code{pseq} attempts to overcome (see Section~\ref{sec:intro_par_func}).
The second problem is that thunk execution is speculative:
when a thunk is created it is usually unknown whether the thunk's value
will actually be used;
and if it is known at compile time that the thunk will be used,
the Glasgow Haskell Compiler (GHC) will optimise it so that the computation
is not represented by a thunk at all and is evaluated eagerly.

% Jerome's work.
\citet*{tannier:2007:parallel_mercury} previously attempted to automatically
parallelise Mercury programs using profiler feedback.
His approach selected the most expensive predicates
of a program and attempted to parallelise conjunctions within them.
This is one of the few pieces of work that attempts to estimate how
dependencies affect the amount of parallelism available.
Tannier's approach counted the number of shared variables in a parallel
conjunction and used this as an analogue for how restricted the
parallelism may be.
However in practice most producers produce dependent variables late in
their execution and most consumers consume them early.
Therefore Tannier's calculation was naive:
the times that these variables are produced by one conjunct and consumed
by the other usually do not correlate with the number of dependent variables.
Tannier's algorithm was, in general, too optimistic
about the amount parallelism available in dependent conjunctions.
Tannier did made use of compile-time granularity control to reduce the
over-parallelisation that can occur in recursive code.

To improve on Tannier's methods,
my honours project
\citep*{bone:2008:hons}
aimed to calculate when the
producing conjunct is most likely to produce the dependent values and
when the consuming conjunct is likely to need them.
To do this I modified Mercury's profiler so that it could provide enough
information so that I could calculate the likely production and
consumption times.
This early work was restricted to situations with a
single dependent variable shared between two conjuncts.
It is described in more detail in Section~\ref{sec:backgnd_autopar}.
% Mercury's deep profiler \citep{conway:2001:mercury-deep} provides
% detailed and accurate profiling information.
% Among other things,
% the deep profiler records separate profiling information for separate
% uses of the same code.
% In this prior work and in this Ph.D.\ dissertation we make heavy use of the
% advanced features provided by Mercury's deep profiler.
% More information about the profiler is provided in section
% \ref{sec:backgnd_deep}.
% No equivalent profiler exists for Haskell or Clean.
% This and Mercury's purity make it the best environment for our research.

