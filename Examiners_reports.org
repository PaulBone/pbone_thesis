
Note that there may be OCR errors in this file, but I wanted it in text so I
could annotate it.

#+TAGS: clarification(c) trivial(t) bibliographic(b) diagram(p) reorder(r)
#+TAGS: discussion(d)

* TODO Examiner 1

** TODO p.28 Top.  future.wait/2                              :clarification:
   When explaining the working of future.wait/2, it would be nice to
   add a line explaining why there is no danger for deadlock.

** TODO p.30-31,  Feedback                                    :clarification:
    In your argumentation for the feedback-directed approach, you state
    yourself that a program is typically executed multiple times with
    respect to different inputs (p.31). While you use this argument in
    favour of the feedback-approach (which is imo totally justified), it
    does raise the question on how well is the feedback—approach providing a
    model for all of these runs?  Unfortunately, this question is rapidly
    put aside by your remark “Most variations in input data will not effect
    parallelisation enough to cause a significant
    difference in performance” (p.31), but this is quite strong a statement;
    is there any (experimental) proof of it?  I think the chosen approach is
    a good one.  but it should be introduced/discussed within more
    appropriate level of objectiveness. See also my remark on the discussion
    w.r.t.  Static analysis (p 92).

** TODO p 34,                                                       :trivial:
    imprecise vocabulary.  On the one hand you talk about
    “computations p and C1” but further down the test they have become
    ‘“conjuncts p and q”.

** TODO p.41 Notation                                               :trivial:
   line G_{kl},\ldots,G_{a}.  This must be G_{k+l}.

** CHCK p.50. BLKSIZE                                         :clarification:
   What is “HLKSlZE”?’ Is it some dark option to be set in the Eoehm
   runtime?  it, it should also be explained.  if care to mention you

** CHCK p.60. Choice of stack                                       :reorder:
   While l was wondering for several pages Why this structure had to
   be a stack, the second half of the page provides the explanation (to
   deal with nested conjunctions).  It would help the reader if this
   justification was moved to the spot. where the stacks are introduced.

** NOOP p.71. remark
   Just a remark, but algorithm 3.8 is basic and the idea of
    reordering independent conjunctions quite seems not far.  being pushed
    very


** TODO p. 78. XXX                                                  :trivial:
   In Figure 3.6, there is an “XXX” remaining.


** TODO p.79. Busy transitions                                :clarification:
    Figure 0 = Seems strange to characterize some transitions as
    “busier” because “you think” (p.78) they occur most often.  Is
    this relevant and, if it is, could it be better (experimentally)
    validated/justified? if it isn't, don‘t talk about it as it makes
    one wonder Whether some of the made are based on
    intuitionchoicesyou only.

** TODO p.92. Static analysis                                 :clarification:
    When (re)introducing the general approach and justifying the
    feedback-approach, the discussion on profiler-feedback versus static
    analysis could be more detailed and more objective.  You put a lot of
    emphasis on “representative input” (see also my remark concerning
    pp.30-31)that is chosen by the programmer, but i why not let the user
    decide on what is “representative input” by providing, eg. a
    specification of typical input (e.g. types and size of certain
    structures). In the latter case, an approach using static analysis might
    be more useful than a profiler—based one. Just to be clear, I 0 not
    criticising your approach, nor am I asking to change it; I am only
    stating I feel it could be somewhat more objectively (with its strong
    and weak points) introduced and discussed.

    To have this 'specification of input' you need a representative
    input, so both methods have the same requirements.  Each method
    has its own strengths and may complement the other.

** TODO p.93 (end of section 4.2). Terminology                      :trivial:
   Terminology: one often uses “monovariant/polyvariant” to refer to
   the fact that a predicate/procedure is
   analysed/transformed/compiled one versus multiple times with
   respect to a somewhat different content.

** TODO p.106 (bottom of the page):                           :clarification:
   “the recursivecalls cost at its average recursion depth is used by
   the algorithm”.  is this speaking) the best one can get or would it
   be to obtain more precise results (eg.  (theoretically possible by
   performing some finpoint computation on the predicate)?

   This is what we can do without changing the profiler further.  We
   could change the profiler but we're not convinced that doing so is
   necessary.

** TODO p.120 (bottom of the page). Typo: “perforrned perform”.     :trivial:

** TODO p. 12.4.  Typo: “that the each iteration”                   :trivial:

* TODO Examiner 2

** DONE General
   CLOSED: [2013-05-16 Thu 21:46]

*** DONE Scope outside of Mercury                                :discussion:
    CLOSED: [2013-05-16 Thu 21:46]
    I would have liked to see some discussion about how all the techniques
    proposed in this dissertation could be applied outside of Mercury
    [e.g., to Prolog? To functional languages?)

This was never within our intended scope, each of the other languages
has limitations that affect the usefulness of automatic
parallelisation.  Some of these are described in the literature
review.  For example Prolog does not have any efficient parallel
implementations and Haskell's lazyness makes it very difficult to
reason about performance.  Many other languages (including Prolog)
are not pure (which is required for auto-parallelism) and no other
language has a profiler as powerful as the Mercury deep profiler.

I've added a small note in the introduction to this affect:

diff --git a/intro.tex b/intro.tex
index 3f6ae20..01785c5 100644
--- a/intro.tex
+++ b/intro.tex
@@ -218,8 +218,14 @@ automatic parallelism.
 Our work is targeted towards Mercury.
 We choose to use Mercury because
 it already supports explicit parallelism of dependent conjunctions,
-and it provides powerful profiling tools which generate data for our profile
-feedback analyses.
+and it provides the most powerful profiling tool of any declarative language,
+which generate data for our profile feedback analyses.
+In some ways our work can be used with other programming languages,
+but most other languages have significant barriers.
+In particular automatic parallelism can only work reliably with declaratively
+pure languages,
+and the language should use a strict evaluation strategy to make it easy to
+reason about parallel performance.
 Mercury's support for parallel execution and the previous
 auto-parallelisation system \citep{bone:2008:hons} is described in
 Chapter~\ref{chap:backgnd}.

*** DONE Benchmark diversity                                     :discussion:
    CLOSED: [2013-05-16 Thu 21:27]
    Many of your considerations on two benchmarks, representing
    rely some fairly regular computations.  How would you consider
    these representatives?  Or, more in general, I would have liked to
    see a much broader pool of diverse benchmarks being used
    throughout the dissertation.

We agree, however we did not have the resources to find and construct
more benchmarks.  That said, Chapter 3 deals with these 'easy'
benchmarks deliberately, to ensure that we can handle these
computations efficiently.  Chapter 4 contains a discussion about
applying the overlap analysis to the Mercury compiler, and remarks
that fewer conjunctions are parallelised (a good thing) but that the
difference is lost in the noise.  Chapter 5 deals with a pathological
case that can be best shown using the same benchmarks as Chapter 3,
as these benchmarks exhibit the pathological behaviour without
creating extra 'noise'.

*** DONE Formal semantics                                        :discussion:
    CLOSED: [2013-05-16 Thu 20:40]
    There are no formal considerations about the fact that the
    parallel implementations respect the "theoretical" operational
    semantics of the language [e.g., same observable behavior).  Even
    though it is true, it would be a good idea to spell it out.

Each transformation individually respects the program semantics (the
_declarative_ semantics, if they respected the operational semantics
then they wouldn't have transformed anything!).  We have already said
as much as we've presented each transformation.

** DONE Chapter 1
   CLOSED: [2013-05-16 Thu 20:36]

Chapter 1 is supposed to set the contest for the whole dissertation, and it
does so in a good way. The chapter could be strengthened a bit by adding
some citations [especially in the first few pages). Additionally

*** DONE Non-SMP                                              :clarification:
    CLOSED: [2013-05-16 Thu 20:24]
    Considerations in this chapter ignore the new generations of
    architecturesbased on CUDA Numa (not SMP), etc.

These architectures aren't ignored, they're acknowledged and then we
say they're out of scope.  We did not specifically mention CUDA or
other GPGPU architectures so I will acknowledge them:

diff --git a/intro.tex b/intro.tex
index 3f6ae20..0a9678f 100644
--- a/intro.tex
+++ b/intro.tex
@@ -87,8 +87,15 @@ and slower access to the other processors' memories.
 %The benefit of NUMA is that it is easier to build large NUMA systems than
 %large SMP systems.
 %The drawback is that it is harder to program.
-SMP systems are currently vastly more common, so programmers are usually
-more interested in programming for them.
+A new type of architecture uses graphics programming units (GPUs) to
+perform general purpose computing,
+they are called GPGPU architectures.
+However they are not as general purpose as their name suggests:
+they work well for large regular data-parallel and compute-intensive
+workloads, but not work well for more general symbolic processing.
+Neither NUMA or GPGPU architectures are as general purpose as SMP systems.
+Also SMP systems are vastly more common than NUMA systems,
+so programmers are usually more interested in programming for them.
 Therefore, in this dissertation we are only concerned with SMP systems.
 Our approach will work with NUMA systems, but not optimally. 

*** DONE Pure/impure examples                                 :clarification:
    CLOSED: [2013-05-16 Thu 19:58]
    I would suggest to add examples of Pure and impure languages

diff --git a/literature_review.tex b/literature_review.tex
index b41bbbb..4f19f55 100644
--- a/literature_review.tex
+++ b/literature_review.tex
@@ -36,6 +36,8 @@ We group languages into the following two classifications.
     but we will restrict our attention to the specific benefit
     that this makes it easy for both compilers and programmers to understand
     if it is safe to parallelise any particular computation.
+    Examples of pure declarative languages are Mercury, Haskell and
+    Clean.
 
     \item[Impure] programming languages are those that allow side effects.
     This includes imperative and impure declarative languages.
@@ -45,6 +47,9 @@ We group languages into the following two classifications.
     parallelism desirable.
     Thus parallelisation of programs written in impure languages is notoriously
     difficult.
+    Examples of impure languages are C, Java, Prolog and Lisp,
+    even though some of these are declarative languages they still allow
+    side effects, and are therefore impure.
 
 \end{description}

*** DONE Is the example in page 8 correct?
    CLOSED: [2013-05-16 Thu 18:36]

The example is correct, the text was not.

diff --git a/literature_review.tex b/literature_review.tex
index b41bbbb..2c17e59 100644
--- a/literature_review.tex
+++ b/literature_review.tex
@@ -557,14 +557,14 @@ The \code{par} and \code{pseq} functions have the types:
 par :: a -> b -> b
 pseq :: a -> b -> b
 \end{verbatim}
-They both take two arguments and return their first.
+They both take two arguments and return their second.
 Their declarative semantics are identical;
 however their \emph{operational} semantics are different.
 The \code{par} function may spawn off a parallel task that evaluates its
-second argument to WHNF,
-and returns its first argument.
-The \code{pseq} function will evaluate its second argument to WHNF
-\emph{and then} return its first argument.
+first argument to WHNF,
+and returns its second argument.
+The \code{pseq} function will evaluate its first argument to WHNF
+\emph{and then} return its second argument.
 We can think of these functions as the \code{const} function
 with different evaluation strategies.

*** DONE Logic programming scope (non SLD?)                   :clarification:
    CLOSED: [2013-05-16 Thu 18:26]
    Considerations in page 9 talk about “logic programming”. but they are
    really focused on languages derived from Prolog (SLD-based, etc.).
    Logic programming is a much broader term, and the considerations in this
    page do not reach other LP languages [e.g._,ASP-based).

Say explicitly that we restrict our attention to SLD-based languages.

diff --git a/literature_review.tex b/literature_review.tex
index b41bbbb..6bef1f2 100644
--- a/literature_review.tex
+++ b/literature_review.tex
@@ -611,7 +611,9 @@ which threads will perform which computations.
 \subsubsection{Parallelism in logic languages}
 \label{sec:intro_par_logic}
 
-Logic programming languages use selective linear resolution with definite
+Different logic programming languages use different evaluation strategies,
+but we will restrict our attention to those that use
+selective linear resolution with definite
 clauses (SLD resolution) \citep{kowalski_sld}.
 SLD resolution attempts to answer a query by finding a Horn clause whose
 head has the same predicate name as the selected atom in the query,

*** WONT Dependent vs Independent                             :bibliographic:
    Hermenegildo used to stress that there is really no such thing as
    independent and dependent and-p, they are the same thing just seen at
    different levels of granularity [and I tend to agree with this).

    Try to find something about this in the literature, if I don't
    find anything then no action needs to be taken.

    I could not find any specific reference by Hermenegildo about
    this, and in any cause I strongly disagree.

*** DONE Research inheritance                                 :bibliographic:
    CLOSED: [2013-05-16 Thu 15:44]
    My memory might be wrong.  but the dependent and——p model of
    Pontelli and Gupta does not really build on [45] [they are
    completely independent).  Furthermore, DDAS was the name of the
    system developed by Kish Shen, not by Pontelli Gupta.

The reviewer is correct, I've revised the discussion in question.

diff --git a/bib.bib b/bib.bib
index 41b7cff..081c61d 100644
--- a/bib.bib
+++ b/bib.bib
@@ -918,7 +918,7 @@ Misc{shapiro:flat_concur_prolog,
 	address = {Leuven, Belgium}
 }
 
-@techreport{pontelli:1996:ddas,
+@techreport{pontelli:1996:nondet-and-par,
 	author = {Enrico Pontelli and Gopal Gupta},
     title = {Non-determinate Dependent And-Parallelism Revisited},
     institution = {Laboratory for Logic, Databases, and Advanced
diff --git a/literature_review.tex b/literature_review.tex
index b41bbbb..dbade1d 100644
--- a/literature_review.tex
+++ b/literature_review.tex
@@ -874,10 +874,8 @@ However clause bodies can still include tell unifications can may provide a
 variable instantiation that allows some other blocked computation to resume.
 Therefore most unifications still incur these extra costs.
 
-The Data Dependent AND-parallelism system (DDAS)
-of \citet{pontelli:1996:ddas} supports explicit
-dependent AND-parallelism in a non-deterministic language;
-it is based on the work of \citet{gupta:1991:ace}.
+\citet{pontelli:1996:nondet-and-par} describe a system that 
+supports explicit dependent AND-parallelism in a non-deterministic language.
 Analyses determine a conservative set of shared variables in dependent
 conjunctions and then \emph{guess} which conjunct produces each variable
 (usually the leftmost one in which the variable appears)

** TODO Chapter 2

*** TODO Detism stats                                         :clarification:
    Can you provide a source for the various statistics mentioned in page
    25?

*** CHCK TRO and and-parallelism                :clarification:bibliographic:
    How does the discussion in page 26 relate to some of the tail recursion
    optimizations developed for and=parallelism?

*** TODO Futures                                   :clarification:discussion:
    I might have missed it, but lots of what I see in page 28 resembles the
    behavior of conditional variables in POSIX threads.

*** TODO Evidence                                                :discussion:
    I found some considerations in page 30/31 a bit speculative (especially
    the last two paragraphs before 2.4.1); any evidence supporting these
    clairns?  @ particular, evidence related to how unbalanced Computations
    can become due to different inputs.

*** TODO Diagrams                                                   :diagram:
    The discussion in this Chapter could benefit from graphical
    representations of the data structures.

** TODO Chapter 3

*** TODO Proofread                                                  :trivial:
    I found several English errors and typos, please proofread

*** TODO Amdahl's law vs Gustafson-Barsis law      :bibliographic:discussion:
    Amdahl's law tend to be rather conservative \ have you considered
    using something like Gustafson-Barsis instead?

        [It's pesimistic for a reason - it works]

*** CHCK Clarification/Discussion (Page 50)        :clarification:discussion:
    Reason 2 page 50: would it be possible to test this hypothesis?  p)
    bounding/unbounding threads?

*** CHCK Prose on page 56
    I found page 56 rather poorly written and hard to follow.

** TODO Chapter 6

*** CHCK Please include more figures.                               :diagram:

** TODO Bibliography

Zoltan said he'd check these.

*** Several errors, please review your entries?

*** [46] has a spurious ‘p’

*** [45] appeared in a more complete forrn in some ICLP [perhaps 1994)

*** I believe Pontelli was an author in [47] -
 
*** also it was published in 2001, not in 1995; on the other hand 1995 saw
    the publication of Hernienegildo’s et al. paper on 8a:ACE (which
    introduces many of the independent and—pstructures and optimizations)

*** [90] was published in ICl_.P’97


