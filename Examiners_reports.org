
Note that there may be OCR errors in this file, but I wanted it in text so I
could annotate it.

#+TAGS: clarification(c) trivial(t) bibliographic(b) diagram(p) reorder(r)
#+TAGS: discussion(d)

* TODO Examiner 1

** TODO p.28 Top.  future.wait/2                              :clarification:
   When explaining the working of future.wait/2, it would be nice to
   add a line explaining why there is no danger for deadlock.

** TODO p.30-31,  Feedback                                    :clarification:
    In your argumentation for the feedback-directed approach, you state
    yourself that a program is typically executed multiple times with
    respect to different inputs (p.31). While you use this argument in
    favour of the feedback-approach (which is imo totally justified), it
    does raise the question on how well is the feedback—approach providing a
    model for all of these runs?  Unfortunately, this question is rapidly
    put aside by your remark “Most variations in input data will not effect
    parallelisation enough to cause a significant
    difference in performance” (p.31), but this is quite strong a statement;
    is there any (experimental) proof of it?  I think the chosen approach is
    a good one.  but it should be introduced/discussed within more
    appropriate level of objectiveness. See also my remark on the discussion
    w.r.t.  Static analysis (p 92).

** TODO p 34,                                                       :trivial:
    imprecise vocabulary.  On the one hand you talk about
    “computations p and C1” but further down the test they have become
    ‘“conjuncts p and q”.

** TODO p.41 Notation                                               :trivial:
   line G_{kl},\ldots,G_{a}.  This must be G_{k+l}.

** CHCK p.50. BLKSIZE                                         :clarification:
   What is “HLKSlZE”?’ Is it some dark option to be set in the Eoehm
   runtime?  it, it should also be explained.  if care to mention you

** CHCK p.60. Choice of stack                                       :reorder:
   While l was wondering for several pages Why this structure had to
   be a stack, the second half of the page provides the explanation (to
   deal with nested conjunctions).  It would help the reader if this
   justification was moved to the spot. where the stacks are introduced.

** NOOP p.71. remark
   Just a remark, but algorithm 3.8 is basic and the idea of
    reordering independent conjunctions quite seems not far.  being pushed
    very


** TODO p. 78. XXX                                                  :trivial:
   In Figure 3.6, there is an “XXX” remaining.


** TODO p.79. Busy transitions                                :clarification:
    Figure 0 = Seems strange to characterize some transitions as
    “busier” because “you think” (p.78) they occur most often.  Is
    this relevant and, if it is, could it be better (experimentally)
    validated/justified? if it isn't, don‘t talk about it as it makes
    one wonder Whether some of the made are based on
    intuitionchoicesyou only.

** TODO p.92. Static analysis                                 :clarification:
    When (re)introducing the general approach and justifying the
    feedback-approach, the discussion on profiler-feedback versus static
    analysis could be more detailed and more objective.  You put a lot of
    emphasis on “representative input” (see also my remark concerning
    pp.30-31)that is chosen by the programmer, but i why not let the user
    decide on what is “representative input” by providing, eg. a
    specification of typical input (e.g. types and size of certain
    structures). In the latter case, an approach using static analysis might
    be more useful than a profiler—based one. Just to be clear, I 0 not
    criticising your approach, nor am I asking to change it; I am only
    stating I feel it could be somewhat more objectively (with its strong
    and weak points) introduced and discussed.

    To have this 'specification of input' you need a representative
    input, so both methods have the same requirements.  Each method
    has its own strengths and may complement the other.

** TODO p.93 (end of section 4.2). Terminology                      :trivial:
   Terminology: one often uses “monovariant/polyvariant” to refer to
   the fact that a predicate/procedure is
   analysed/transformed/compiled one versus multiple times with
   respect to a somewhat different content.

** TODO p.106 (bottom of the page):                           :clarification:
   “the recursivecalls cost at its average recursion depth is used by
   the algorithm”.  is this speaking) the best one can get or would it
   be to obtain more precise results (eg.  (theoretically possible by
   performing some finpoint computation on the predicate)?

   This is what we can do without changing the profiler further.  We
   could change the profiler but we're not convinced that doing so is
   necessary.

** TODO p.120 (bottom of the page). Typo: “perforrned perform”.     :trivial:

** TODO p. 12.4.  Typo: “that the each iteration”                   :trivial:

* TODO Examiner 2

** TODO General

*** TODO Scope outside of Mercury                                :discussion:
    I would have liked to see some discussion about how all the techniques
    proposed in this dissertation could be applied outside of Mercury
    [e.g., to Prolog? To functional languages?)

*** TODO Benchmark diversity                                     :discussion:
    Many of your considerations on two benchmarks, representing
    rely some fairly regular computations.  How would you consider
    these representatives?  Or, more in general, I would have liked to
    see a much broader pool of diverse benchmarks being used
    throughout the dissertation.

*** TODO Formal semantics                                        :discussion:
    There are no formal considerations about the fact that the
    parallel implementations respect the "theoretical" operational
    semantics of the language [e.g., same observable behavior).  Even
    though it is true, it would be a good idea to spell it out.

** TODO Chapter 1

Chapter 1 is supposed to set the contest for the whole dissertation, and it
does so in a good way. The chapter could be strengthened a bit by adding
some citations [especially in the first few pages). Additionally

*** TODO Non-SMP                                              :clarification:
    Considerations in this chapter ignore the new generations of
    architecturesbased on CUDA Numa (not SMP), etc.

*** TODO Pure/impure examples                                 :clarification:
    I would suggest to add examples of Pure and impure languages

*** CHCK Is the example in page 8 correct?

*** TODO Logic programming scope (non SLD?)                   :clarification:
    Considerations in page 9 talk about “logic programming”. but they are
    really focused on languages derived from Prolog (SLD-based, etc.).
    Logic programming is a much broader term, and the considerations in this
    page do not reach other LP languages [e.g._,ASP-based).

*** CHCK Dependent vs Independent                             :bibliographic:
    Hermenegildo used to stress that there is really no such thing as
    independent and dependent and-p, they are the same thing just seen at
    different levels of granularity [and I tend to agree with this).

    Try to find something about this in the literature, if I don't
    find anything then no action needs to be taken.

*** CHCK Research inheritance                                 :bibliographic:
    My memory might be wrong.  but the dependent and——p model of
    Pontelli and Gupta does not really build on [45] [they are
    completely independent).  Furthermore, DDAS was the name of the
    system developed by Kish Shen, not by Pontelli Gupta.

** TODO Chapter 2

*** TODO Detism stats                                         :clarification:
    Can you provide a source for the various statistics mentioned in page
    25?

*** CHCK TRO and and-parallelism                :clarification:bibliographic:
    How does the discussion in page 26 relate to some of the tail recursion
    optimizations developed for and=parallelism?

*** TODO Futures                                   :clarification:discussion:
    I might have missed it, but lots of what I see in page 28 resembles the
    behavior of conditional variables in POSIX threads.

*** TODO Evidence                                                :discussion:
    I found some considerations in page 30/31 a bit speculative (especially
    the last two paragraphs before 2.4.1); any evidence supporting these
    clairns?  @ particular, evidence related to how unbalanced Computations
    can become due to different inputs.

*** TODO Diagrams                                                   :diagram:
    The discussion in this Chapter could benefit from graphical
    representations of the data structures.

** TODO Chapter 3

*** TODO Proofread                                                  :trivial:
    I found several English errors and typos, please proofread

*** TODO Amdahl's law vs Gustafson-Barsis law      :bibliographic:discussion:
    Amdahl's law tend to be rather conservative \ have you considered
    using something like Gustafson-Barsis instead?

        [It's pesimistic for a reason - it works]

*** CHCK Clarification/Discussion (Page 50)        :clarification:discussion:
    Reason 2 page 50: would it be possible to test this hypothesis?  p)
    bounding/unbounding threads?

*** DONE Prose on page 56
    CLOSED: [2013-04-14 Sun 15:46]
    I found page 56 rather poorly written and hard to follow.

I've made the prose clearer and included some extra clarifications to
resolve potential ambiguities.

diff --git a/rts_original_scheduling_performance.tex b/rts_original_scheduling_performance.tex
index 450bf3d..3b87ebd 100644
--- a/rts_original_scheduling_performance.tex
+++ b/rts_original_scheduling_performance.tex
@@ -72,42 +72,43 @@ and therefore tend to write right recursive code.
 
 \plan{Show performance figures.}
 Table~\ref{tab:right} shows average elapsed time in seconds for the
-mandelbrot\_lowalloc program from 20 test runs.
-We use the mandelbrot\_lowalloc program from Section~\ref{sec:rts_gc}.
-Using this program we can easily observe the
-speedup due to parallelism in Mercury without the effects of the garbage
-collector.
-The loop that iterates over the rows in the image uses right recursion.
-It is similar to \code{map/3}
-in Figure~\ref{fig:map_right_recursive}.
-The leftmost column shows the maximum number of contexts permitted at
-any time.
-This is the limit that was mentioned in the previous section.
+mandelbrot\_lowalloc program from Section~\ref{sec:rts_gc}.
+We this program because it is easy to observe the speedup due to parallelism
+in Mercury as garbage collection does not affect its performance very much.
+The loop that iterates over the rows in the image uses right recursion;
+it is similar to the loop in Figure~\ref{fig:map_right_recursive}.
+The leftmost column of the table shows the maximum number of contexts
+that may exist at any time.
+This is the limit that was introduced in the previous section.
 The next two columns give the elapsed execution time for a sequential
-version of the program;
-in this version of the program no parallel conjunctions were used.
+version of the program,
+the program compiled without the use of the parallel conjunction operator.
 These two columns give results without and with thread safety.
 The following four columns give the elapsed execution times
 using one to four Mercury engines.
-The numbers in parentheses are the ratio between the time and the
-sequential thread safe time.
+Each value in the table is a mean of 20 test runs.
+The numbers in parentheses are the ratio between the mean time and the
+mean sequential thread safe time.
 
 \plan{Observations}
 In general we achieve more parallelism when we use more contexts,
-up to a threshold of 601 contexts,
-as there are 600 rows in the image and a base case each one consumes a
-context.
+up to a threshold of 601 contexts.
+The threshold is at this point because
+there are 600 rows in the image meaning 600 iterations of the loop plus 1
+for the base case.
+Each iteration may consume a context.
 This is why the program does not benefit greatly from a high limit such as
 1024 or 2048 contexts.
-This program may use fewer than 600 contexts as any sequentially executed
-sparks use their parent context.
+When a spark is executed in its parent context
+(two iterations execute sequentially)
+then the program may use fewer than 600 contexts.
 This is possibly why a limit of 512 contexts also results in a good parallel
 speedup.
 When only 256 contexts are used,
 the four core version achieves a speedup of 1.30,
 compared to 3.55 for 512 or more contexts.
-Given that mandelbrot uses independent parallelism there should never be any
-need to suspend a context.
+Given that mandelbrot uses independent parallelism,
+ideally there should never be any need to suspend a context.
 Therefore the program should parallelise well enough when restricted to
 a small number of contexts (four to eight).
 Too many contexts are needed to execute this program at the level of
@@ -119,8 +120,10 @@ parallelism.
 \plan{Describe the context limit problem.}
 %Right recursion uses a context for each iteration of the loop,
 %and then suspends that context.
-To understand this problem we must consider how parallel conjunctions are
+To understand this problem, we must consider how parallel conjunctions are
 executed (see Section~\ref{sec:backgnd_merpar}).
+We will step through the execution \code{map/3} from
+Figure~\ref{fig:map_right_recursive}.
 The original context creates a spark for the second and later conjuncts and
 puts it on the global spark queue.
 It then executes the first conjunct \code{P/2}.

** TODO Chapter 6
*** CHCK Please include more figures.                               :diagram:

** TODO Bibliography

Zoltan said he'd check these.

*** Several errors, please review your entries?

*** [46] has a spurious ‘p’

*** [45] appeared in a more complete forrn in some ICLP [perhaps 1994)

*** I believe Pontelli was an author in [47] -
 
*** also it was published in 2001, not in 1995; on the other hand 1995 saw
    the publication of Hernienegildo’s et al. paper on 8a:ACE (which
    introduces many of the independent and—pstructures and optimizations)

*** [90] was published in ICl_.P’97


